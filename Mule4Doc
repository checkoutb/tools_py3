
https://www.mulesoft.org/docs/site/4.1.1/apidocs/
mule-runtime-project 4.1.0-SNAPSHOT API

https://docs.mulesoft.com/mule-runtime/4.3/build-application-from-api



https://docs.mulesoft.com/mule-runtime/4.3/whats-new-in-mule


zzzzz


。。property，variable 是不同的，，但是忘记了。。
attribute

Logger message for an attribute: message.attributes
Logger message for the variable *myVar: *vars.myVar
。。

org.mule.runtime.api.message.Message， 接口。
org.mule.runtime.api.event.Event  接口
。。大约有点数了。
Event是最顶层，Event有2个 有用的方法  getMessage， getVaribles.前者是Message借口，后者是Map<String, TypedValue<?>>
Message接口有 2个方法， getPayload, getAttributes.  2者都返回 TypedValue 类型。
。所以 varibles 有N个， 而 payload attributes 各有1个。

  /**
   * Returns the message payload for this event
   *
   * @return the message payload for this event
   */
  Message getMessage();

  /**
   * Returns the variables in the event
   *
   * @return a map of {@link TypedValue} containing the variable's names and values.
   */
  Map<String, TypedValue<?>> getVariables();

======================================

  /**
   * Gets a {@link TypedValue} with the payload of this message.
   *
   * @param <T> the type of the payload.
   * @return the message payload.
   */
  <T> TypedValue<T> getPayload();

  /**
   * Gets the attributes associated with the Message. The {@code Attributes} attributes object is specific to the connector that
   * was the source of the current message and is used for obtaining message properties or headers if applicable plus additional
   * information that provides context for the current message such as file size, file name and last modified date for FILE, and
   * origin IP address, query parameters etc. for HTTP.
   * <p>
   * If there are no attributes associated with the current message, for example if the source of the message was not a connector,
   * then the attributes with be null.
   *
   * @return attributes associated with the message, or null if none exist.
   */
  <T> TypedValue<T> getAttributes();
。。。。。这个保存的有点猛。。看来不能动这个的。。

。。transform message 中可以修改 variable。。
。。有2个core 组件， set Variable , remove variable

下面也有：
Mule Message 由2部分组成：
payload，包含了消息的body。如 文件的内容，数据库的一条记录，REST/Web服务 的response
attributes payload的metadata。
。。都忘了。



















。。在高性能的I/O设计中,有两个著名的模型:Reactor模型和Proactor模型,其中Reactor模型用于同步I/O,而Proactor模型运用于异步I/O
。。Reactor 核反应堆。。Proactor 前摄式


之前的版本，mule有且管理3个线程池。分别用于CPU_LIGHT, IO, CPU_INTENSIVE
使用Proactor设计模式来将任务task提交到这些线程池

4.3开始，合并成一个线程池。proactor模式依然使用。


https://docs.mulesoft.com/mule-runtime/4.3/execution-engine
新线程池的模型


mule4.3包含DataWeave2.3.0



uses the *map* function to iterate over the elements in the input array of objects. 
uses the new *update* operator to modify two of the key-value pairs in each object
如果是Tomo，增加一个xxx
每个age都+1

%dw 2.0
output application/json
---
payload map ((user) ->
   user update {
       case name at .name if(name == "Tomo") -> name ++ " (Christian)"
       case age at .age -> age + 1
   }
)

增加xml流式处理
DataWeave reader for the XML format now supports streaming
You need to specify the *collectionPath* and *streaming* reader properties to activate streaming.

使用流式时，xml解析器能在没有读完整个xml时就开始处理。

增强json的流式处理
现在json reader 能支持整个Json 文件的流式处理，而不仅仅是数组（instead of arrays only）


Literal Types 字面类型。。。 不，看英文。感觉更像是枚举类型。
DW增加了对字面类型的支持。

A literal type is a type that represents exactly one value, which means, for example, that DataWeave can represent the String value "hello" with the type *hello
This feature makes function overloading very dynamic and when combined with *Union types, it enables DataWeave to represent an enumeration of literal values as a type

type Weekdays = "Monday" | "Tuesday" | "Wednesday" | "Thursday" | "Friday"
type Days = Weekdays | "Saturday" | "Sunday"

fun greeting(day : "Monday") = "Best of luck in this new work week!"
fun greeting(day : "Friday") = "YAY, it's Friday! Have an awesome day!"
fun greeting(day: Weekdays) = "Have a nice day at work!"
fun greeting(day: Days) = "We hope you have a great day!"

。。这个fun。。根据后面的string，感觉更像是if。。第二个fun如果使用的时候传入Monday，会是什么？ 估计 fun的用法不是 被调用。。

The greeting function provides:
a general message ("We hope you have a great day!") on weekends (Saturdays and Sundays only).
a work- specific message ("Have a nice day at work!") on Tuesdays through Thursdays.
customized messages for the start and end of the week (Mondays and Fridays).
。。好像greeting是一个方法，不过分成4段了。。。



Period and DateTime Consistency
This feature adds an easy way to operate with periods, transform the result into different units, and decompose the period into its different parts.

shows how to deconstruct a Duration value. It defines a period variable in the header that subtracts one DateTime value from another. The script decomposes the result of the operation by selecting the hours, minutes, and secs values from the resulting period value.

%dw 2.0
output application/json
var period = (|2010-12-10T12:10:12| - |2010-12-10T10:02:10|)
---
{
   hours:  period.hours,
   minutes:  period.minutes,
   secs:  period.secs,
}

Output:
{
  "hours": 2,
  "minutes": 8,
  "secs": 2
}


Custom MIME Types
a simpler way of defining output and input formats
%dw 2.0
output json
---
{
	Hello: “there”
}





DataWeave improves the performance of memory usage, functions (like groupBy), and the internal execution engine in this release

Elimination of common subexpressions
消除相同的子表达式
DataWeave optimizes the scripts by defining internal variables to represent subexpressions that are repeated
优化脚本，通过内部变量取代重复子表达式

%dw 2.0
output json
---
{
   code: payload.message.code,
   message : payload.message.value,
   user : payload.message.users[0].name,
   contact: payload.message.users[0].email
}

变成：

%dw 2.0
output json

var fakeVariable1 = payload.message
var fakeVariable2 = fakeVariable1.users[0]
---
{
   code: fakeVariable1.code,
   message : fakeVariable1.value,
   user : fakeVariable2.name,
   contact: fakeVariable2.email
}


Memory management improvements
Memory management is now centralized. Several memory management strategies are included, such as pooling direct memory or using heap memory exclusively.
内存管理现在集中了，包含数个内存管理策略，比如。。。


DataWeave模块
DataWeave introduces the Types (dw::core::Types) module to support type introspection. This new module includes a number of functions and new types

DataWeave also introduces a number of new functions in existing modules:
Core (dw::Core) module: entriesOf, keysOf, namesOf, valuesOf.
Arrays (dw::core::Arrays) module: firstWith
Objects (dw::core::Objects) module: everyEntry, someEntry, and takeWhile; dropWhile also includes a new function that iterates over and ignores items in an array until a condition becomes true.
Strings (dw::core::Strings) module: withMaxSize enables you to specify a maximum size for a given value. A value that exceeds the size limit will be truncated. If the size is no greater than the limit, the value remains the same.


Deprecations:	反对
The entrySet, keySet, nameSet, valueSet functions in the Objects module are **deprecated in this release and replaced with the functions entriesOf, keysOf, namesOf, valuesOf in the Core module.



Intersection type 	交叉，相交
DataWeave introduces the Intersection type. The Intersection type appends Object types.
TypeA & TypeB & ...


错误显示更智能


New DataWeave Reader and Writer Properties
。。a number of 许多
Binary (application/octet-stream) format: encoding
DataWeave (application/dw) format: onlyData writer property and all the reader properties (externalResources, javaModule, and onlyData)
JSON (application/json) format: writeAttributes
XML (application/xml) format: escapeCR writer property and several reader properties (collectionPath, maxAttributeSize, optimizeFor, supportDtd, streaming)
YAML (application/yaml) format: maxEntityCount reader property



Core Components

Until Successful component
Max Retries (maxRetries) and Milliseconds Between Retries (millisBetweenRetries) now support a number or an expression that resolves to a number.

Batch Aggregator component adds the preserveMimeTypes attribute:



API Gateway now includes a disablePolicies annotation. Setting it to true enables you to block the application of policies to the requestor.

the release introduces several enhancements to resiliency, including a better backoff mechanism and improvements of the policy deployer mechanism.





https://docs.mulesoft.com/mule-runtime/4.3/
Mule Overview

Mule runtime engine (Mule) is a lightweight integration engine that runs Mule applications and supports domains and policies. Mule applications, domains, and policies share an XML DSL (domain-specific language).

Mule applications connect systems, services, APIs, and devices using API-led connectivity instead of point-to-point integrations. Mule applications provide functionality for message routing, data mapping, orchestration, reliability, security, and scalability.
。。消息路由，数据映射，组织，可靠，安全，可测量性



Mule Domains
Domains enable you to share global configurations that Mule applications need to reuse, such as default error handlers, shared properties, scheduler pools, and component configurations. You can only deploy domains when running Mule runtime engine on premises.



Policies
Policies on HTTP-based APIs can enforce security, regulate traffic through Mule applications, and adapt APIs to your business needs.
。。增强安全性，控制调度

Mule includes an embedded API Gateway, which enables you to apply security policies to an API, enrich incoming or outgoing messages, and add capabilities to an API without having to write any code.



Maven Support
Mule provides built-in Maven functionality




Mule Installation, Deployment, and Management
For a Mule app to run, it must be deployed to an environment where the Mule runtime engine is installed.

For production and pre-production deployments of Mule apps, you can use Runtime Manager to deploy Mule apps to runtimes within CloudHub and other supported platform as a service (PaaS) solutions.








https://docs.mulesoft.com/mule-runtime/4.3/mule-components
Mule Components
There are a couple types of component: Core components and the components that belong to connectors and modules.


Core Components 。。 这个是 mule palette 的 core 部分。
These are individual components that are part of the core of the functionality of Mule Runtime
。。有一些 功能各异 的 组件，这些组件是 mule运行时的 设计功能/设计目的/实用 的 核心的一部分。

。。sampling  抽样
下面是一些 可以添加到你的app中的 mule的核心组件 的 功能抽样

Asynchronous processing of parts of a flow or subflow
流程、子流程 的 异步处理部分

Batch processing of messages
消息的批处理

Initiating subflows
初始化子流程

Logging

Setting payloads

Transforming messages with the DataWeave language

Creating Try scopes in your flows so that you can catch and respond to errors
try的范围



Connectors
Connectors group together components that were created to facilitate the integration of MuleSoft applications with any external resources
。。Connectors组 组合了 那些用来 促进 mule 和 其他外部资源 整合/一体化 的 组件。

For example, the Salesforce Connector provides components that let you use the Salesforce platform APIs to perform a large number of different operations.
。。不过只有Salesforce这个标签，没有Connectors 标签啊。。估计这里需要 自己开发的。



Modules
Modules group together components that were created to add flexibility to your applications by allowing you to aggregate values compress data, use Java features, use extra features for processing JSON, and more. 
。。模块，组合了那些，通过 允许你合计值来压缩数据，使用java功能，使用额外功能 来处理json等其他数据  来增加app的灵活性的

这些 模块功能 更多地用于 传统app编码。
如XML模块


。。mule pallette, 第二行是 Add Modules，可以增加显示其他的模块。。
。。 但是core，connector 也是 module 啊。 上面是分开的。。

https://docs.mulesoft.com/connectors/
提供了很多connector  ： redis，quartz，pop3，paypal，oracle EBS，MongoDB, MicroSoft Service Bus, LDAP, JSON,JMS,JDBC,Java,HTTP,IBM MQ，Hadoop(HDFS),FTP,Email,Kafka, Azure Service Bus,Amazon ,ajax,



https://docs.mulesoft.com/mule-runtime/4.3/about-flows
Flows and Subflows
Mule apps process messages and other parts of Mule events through Mule components, connectors, and modules that are set up within the scope of Flow and Subflow components within an app.
。。mule应用，处理消息和mule事件的其他部分 通过 组件，连接器，模块，这些东西 被定义/创建在 应用的 流程、子流程 组件中。


一个应用可以由 一个单独流程，或 分散为 多个互不相关的流程 或 子流程 组成。

mule应用通常 使用多个流程/子流程，来分割 应用 到 功能模块，或者 为了 错误处理

You can connect and trigger their execution with *Flow Reference* components or by using the DataWeave *lookup* function within expressions and transformations.

You can think of a Flow Reference as a function call that accepts an event as input and then returns the modified event.


Using Flows
Flows can have Mule Sources (such as an HTTP listener receiving a request) that trigger the execution of a flow

For cases where you do not want a source to start a flow right away, you can configure your flow as initially stopped and start it later through Runtime Manager （https://docs.mulesoft.com/runtime-manager/flow-management）.

。。对于那些 并不需要一个source来立刻开始flow的，你可以设置你的flow初始时就停止的，然后在RuntimeManager中启动。
。。应该是说 怎么触发一个flow。要么http，要么RuntimeManager，， 定时器呢？，什么功能 能不通过http来启动？

If the event processing gets complicated, or must call out to other services, you might factor out that behavior into other flows.
如果事件处理是 复杂的，或必须调用 其他服务，你应该 分解 行为 到其他流程。


Using Subflows
A subflow is a scope that enables you to group event processors in a manner similar to that of a flow, but with certain differences and limitations:
。。子流程 是一个范围，在这个范围内，你可以 分类 事件处理器 ，就像在 流程 中。 除了以下的 差异 和 限制：
Subflows do not have event sources.

Subflows do not have an error handling scope.

During design, subflows work as macros that replace the Flow Reference components that call them.
When you build the application, all Flow Reference components that call a subflow are replaced by the contents of the referenced subflow.
。在设计时，子流程非常有用，用来替换flow ref 组件。
。build时，所有的 调用子流程的 flow ref 组件 都 子流程替换。
。。。 就是build时， 子流程的ref，被子流程 替换了， 等于一个 内联。。。所以性能高。。。还有根据下面的 Limitations 就是 类似 单例 变成 多例

Referencing subflows results in better performance than referencing a flow.

。。没有事件来源，没有单独的错误处理， 。。。 引用/查询/调用 子流程 比 ref 流程 有更好的 性能。


Limitations
Because the contents of a subflow replace each Flow Reference component that references that subflow, several instances of the event processors inside that subflow exist in the application at runtime. This behavior can cause issues with event processors that depend on unique IDs or instances to execute, such as batch processors.
。。event processor 单例变成多例，， 后半句什么意思？这个行为会导致事件处理器依赖于唯一的ID。。。ok，结合下面的意思就是ID得自己设置好，使用默认的ID会报错。

For example, configuring a batch job inside a subflow causes the application to fail during deployment if the subflow is referenced from more than one Flow Reference component. The application fails to deploy because multiple instances of a batch job with the same job instance ID exist, which is not allowed.
。。批处理内置一个subflow，如果subflow被多个flow ref，在发布时会报错。因为 多个批处理实例有相同的ID。

zzzzz。。。。。。 还是无法理解。。上面重复ID的是subflow的重复了？ 但是不是说 是 直接内容替换ref吗? 那么runtimr就没有 subflow 这个东西了啊。  那么就是 flow的ID重复了？但是 flow ref的subflow 被替换成 subflow的内容，这怎么会导致生成 多个 flow？。。。难道：

<bean id="flow1">
	<property ref="sub1"/>
</bean>
<bean id="sub1"></bean>

发布时是：
<bean id="flow1">
	<property>
		<bean id="sub1"></bean>
	</property>
</bean>

这样的？那不会来个匿名bean？。。可能这个subflow还能通过id来调用。 所以不能匿名，不能#1.#2后缀。
。。那怎么解决这种？batch 中不使用 subflow ？。。但是其他地方也会有这种问题啊。
。。而且 subflow 是为了复用，但是 id限制了 它不能被复用。。


Error Handling
Each flow (but not subflow) can have its own error handling. One reason for calling a flow through a Flow Reference component is to isolate the error handling to different contexts. For example, a parent flow that receives web client requests might define HTTP-related error handling. If the parent flow then calls a JMS queue for further processing, you can put the JMS event processors in a separate child flow and call that flow with a Flow Reference component. This child flow can then define its own JMS-related error handling. This practice is similar to the way you handle or propagate errors in other programming languages, like Java.

。。通过ref来调用 flow 的一个原因 就是 在不同的上下文中 独立/隔离 错误处理。
。。父flow自己是通过http source来触发的，所以父flow的错误处理处理http的错误，父flow会调用JMS 事件处理，那么将这个JMS 事件处理 抽取到 单独的flow中，父flow ref 这个flow，在单独的flow中，可以定义独立的JMS有关的错误的处理。


Like a flow or subflow, a Try scope also groups together a sequence of event processors with its own error handling. You can place a Try scope inside a flow or subflow to isolate error handling inside a flow without the need to create a separate flow or subflow. The trade-off is that error handling through a Try scope occurs inline inside the flow, which makes it harder to reuse between other flows or subflows.
。。try真的是一个 组件/模块。。而且归类为core。
。如果只是简单的错误隔离的话，可以使用try scope。  而不必抽取出 flow
。。不过 try 是无法重用的。


Branching Event Processing
Some scopes are available for a flow to branch event processing into separate threads and to allow asynchronous processing of the event at a specific point in the flow, for example, the Scatter-Gather and Async scopes. A Choice router enables you to change the downstream sequence of event processors based on some condition.





https://docs.mulesoft.com/mule-runtime/4.3/about-mule-configuration
Mule Configuration File

All Mule applications, domains, and policies are configured through an XML DSL
。。DSL，domain special language

This XML file specifies the resources that compose the artifact, including dependencies needed to run the Mule application.

能直接写xml，但是更普遍的还是用图形化工具



Overview
A Mule configuration file is a tree.
Each of the elements sets up a configuration object within Mule, for example:
。每个(xml)元素都 装配/设置/建立 一个 mule的配置对象

Mule Global Configuration
Global settings, such as the default transaction time-out, that apply to the entire Mule configuration.

Properties
Configuration Properties, message properties, and system properties.

Flows
Combine components to define a message flow.

Sources (Endpoints or Triggers)
Trigger a flow. Sources are sometimes called Endpoints in Studio and Triggers in Flow Designer.

Connectors and Modules Configurations
Declare configurations for any connectors and modules components used.

Routers
Control the flow execution.

Operations
Apply specific actions within a flow.



XML Schema

<?xml version="1.0" encoding="UTF-8"?>
<mule xmlns="http://www.mulesoft.org/schema/mule/core"
      xmlns:xsi=http://www.w3.org/2001/XMLSchema-instance
        xmlns:jms="http://www.mulesoft.org/schema/mule/jms"
        xmlns:file="http://www.mulesoft.org/schema/mule/file"
        xsi:schemaLocation="
        http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
        http://www.mulesoft.org/schema/mule/jms http://www.mulesoft.org/schema/mule/jms/current/mule-jms.xsd
        http://www.mulesoft.org/schema/mule/file http://www.mulesoft.org/schema/mule/file/current/mule-file.xsd">


Mule domains feature the *mule-domain* tag instead of *mule*.


Be sure to specify all the necessary schema files. This can be time-consuming when setting up the configuration file by hand, but importing schema files provides the following time-saving benefits:
Auto-completion and context-specific help in Anypoint Studio
Design-time configuration validation
Typed properties
。。确定所有需要schema。手工确定是非常耗时的，但导入schema文件提供了节约时间的好处：
。。这是什么？？？？


Schemas in Mule 4 are autogenerated dynamically according to the artifact dependencies. The HTTP schema, for example, will only be available if the HTTP connector is part of the artifacts dependencies. 
。。根据pom文件来确定导入的schema。
。。应该是，选择组件后，就可以知道要导入什么schema，pom里需要什么了。



Namespaces
Each Mule module or connector has its XML schema, including Mule core for its community and enterprise versions. When you import a schema, it has its namespace.
。。每个mule的模块/connector，都有它的xml schema，社区版，企业版都包含mule core。
zzzzz。。是这样翻译的吗？不过社区版，企业版，怎么选择？靠anypoint studio的是否登录？
。。感觉没有任何信息啊。。还是说对schema的理解不过？

To use the standard Mule elements, import the Mule Kernel (CE) and Mule runtime engine (EE) namespaces:
http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
http://www.mulesoft.org/schema/mule/ee/core http://www.mulesoft.org/schema/mule/ee/core/current/mule-ee.xsd


Default Namespace
Typically, you set the Mule core schema as the default namespace for your configuration file. This means that any XML element without a prefix will come from the Mule core schema. To set the default namespace schema, specify xmlns immediately followed by the URL of the Mule schema, without the colon or namespace prefix you set in the previous example (e.g., xmlns instead of xmlns:jms):

<mule xmlns="http://www.mulesoft.org/schema/mule/core"
    xsi:schemaLocation="http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd">
   ...config...
</mule>
zzzzz。。这里的意思是：这里的配置和一页前的配置一样？还是说这里是默认的，如果需要其他的(jms)功能，需要再添加。



Merging Configuration Files
If you have multiple configuration files, you can import them into one configuration file so that you only have to specify one configuration. This is useful to extract connector configurations or other global elements. For example:
zzzzz。。被导入的xml文件中不需要写schema？如果被导入的xml写了schema，那么是否是以被导入的xml中的schema为准？

<?xml version="1.0" encoding="UTF-8"?>
<mule xmlns=http://www.mulesoft.org/schema/mule/core ....>

    <import file="global-prod-configurations.xml" />
    <import file="global-error-handler.xml" />
...

These imports can also be dynamic when combined with properties:
<?xml version="1.0" encoding="UTF-8"?>
<mule xmlns=http://www.mulesoft.org/schema/mule/core ....>

    <global-property name="env" value="dev"/>
    <import file="global-${env}-configurations.xml" />
...

The *env property in the example can be set by a system property, or environment property but not by a global property.
zzzzz。。那么系统/环境变量怎么设置呢？多个同类型的，哪个为准？不同类型(系统/环境/xml)的，哪个为准？

Because of the hierarchy in which Mule runtime engine loads the properties, you cannot make imports depend on an application or a global configuration property
。。上面的例子不就是导入依赖于global xx property吗？


Declaring Multiple Configurations
You can also keep your files separate as long as you declare them as part of your application configurations. This is useful when each configuration file is relatively unrelated to the others. Configurations are declared on the application descriptor file, *mule-artifact.json, within the *configs section. For example, four configuration files are declared here:
。。你也可以分开定义。当配置文件之间无关联的时候。

{
  "configs": [
    "http-api.xml", "jms-messaging-api.xml", "monitoring-tools.xml", "core-functionality.xml"
  ],
  "redeploymentEnabled": true,
  "name": "retail-api",
  "minMuleVersion": "4.1.1",
  "requiredProduct": "MULE_EE",
  "classLoaderModelLoaderDescriptor": {
    "id": "mule",
    "attributes": {
      "exportedResources": []
    }
  },
  "bundleDescriptorLoader": {
    "id": "mule",
    "attributes": {}
  }
}
。。mule并不是读取全部的xml，而是读取 mule-artifact.json中的 configs 对应的 所有xml文件。这个.json文件 被称为 application descriptor file.


Full Application Example

Following is an example of a simple Mule configuration file for an application:

<mule xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xmlns:db="http://www.mulesoft.org/schema/mule/db"
      xmlns:email="http://www.mulesoft.org/schema/mule/email"
      xmlns:http="http://www.mulesoft.org/schema/mule/http"
      xmlns:tls="http://www.mulesoft.org/schema/mule/tls"
      xmlns="http://www.mulesoft.org/schema/mule/core"
      xsi:schemaLocation="
        http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
          http://www.mulesoft.org/schema/mule/db http://www.mulesoft.org/schema/mule/db/current/mule-db.xsd
        http://www.mulesoft.org/schema/mule/email http://www.mulesoft.org/schema/mule/email/current/mule-email.xsd
        http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
        http://www.mulesoft.org/schema/mule/tls http://www.mulesoft.org/schema/mule/tls/current/mule-tls.xsd">

    <http:listener-config name="http.listener.config" basePath="mule">
        <http:listener-connection host="0.0.0.0" port="${http.port}" protocol="HTTP"/>
    </http:listener-config>

    <http:request-config name="http.request.config" basePath="mule">
        <http:request-connection host="127.0.0.1" port="${http.port}"/>
    </http:request-config>

    <email:smtp-config name="email.config">
        <email:smtp-connection host="${email.host}" port="${email.smtp.port}"/>
    </email:smtp-config>

    <flow name="integration-routing-contentFlow">
        <http:listener config-ref="https.listener.config" path="routing/main"/>
        <logger level="INFO" message="#[attributes.headers]"/>

        <choice>
            <when expression="#[attributes.headers.'content-type' contains 'application/json']">
                <http:request config-ref="http.request.config" path="routing/http" method="POST"/>
            </when>
            <otherwise>
                <set-payload value="Error: Unexpected unmapped choice element when trying to route the request."/>
                <email:send config-ref="email.config" subject="Email routing">
                    <email:to-addresses>
                        <email:to-address value="routing@mulesoft.com"/>
                    </email:to-addresses>
                    <email:body contentType="text/plain">
                        <email:content>#[payload]</email:content>
                    </email:body>
                </email:send>
            </otherwise>
        </choice>
    </flow>

</mule>


Full Policy Example

<?xml version="1.0" encoding="UTF-8"?>
<mule xmlns="http://www.mulesoft.org/schema/mule/core"
     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
     xmlns:http="http://www.mulesoft.org/schema/mule/http"
     xmlns:http-policy="http://www.mulesoft.org/schema/mule/http-policy"
     xsi:schemaLocation="http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
              http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
              http://www.mulesoft.org/schema/mule/http-policy http://www.mulesoft.org/schema/mule/http-policy/current/mule-http-policy.xsd">

   <http-policy:proxy name="policy-example">
       <http-policy:source>
           <http-policy:execute-next/>
           <logger level="INFO" message="#[payload]"/>
       </http-policy:source>
   </http-policy:proxy>
</mule>

zzzzz。。。policy 是什么？


Full Domain Example
<?xml version="1.0" encoding="UTF-8"?>
<domain:mule-domain
        xmlns:http="http://www.mulesoft.org/schema/mule/http"
        xmlns="http://www.mulesoft.org/schema/mule/core"
        xmlns:domain="http://www.mulesoft.org/schema/mule/ee/domain"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns:doc="http://www.mulesoft.org/schema/mule/documentation" xsi:schemaLocation="
               http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
               http://www.mulesoft.org/schema/mule/ee/domain http://www.mulesoft.org/schema/mule/ee/domain/current/mule-domain-ee.xsd
http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd">

    <!-- configure here resource to be shared within the domain -->

    <http:listener-config name="HTTP_Listener_config" doc:name="HTTP Listener config" doc:id="e252ad6a-220d-4c1f-865b-d7aec30bfc30" basePath="/api" >
          <http:listener-connection host="0.0.0.0" port="8081" />
      </http:listener-config>

</domain:mule-domain>



https://docs.mulesoft.com/mule-runtime/4.3/about-mule-event
Mule Events

A Mule event contains the core information processed by the runtime. It travels through components inside your Mule app following the configured application logic.

Note that the Mule event is immutable, so every change to an instance of a Mule event results in the creation of a new instance.
。。不可变，每次修改都会生成一个新的event。

A Mule Event is composed of these objects:
A Mule Message contains a message payload and its associated attributes.
Variables are Mule event metadata that you use in your flow.
。mule event包含2样东西：
1.包含attributes和payload 2样属性 的Mule Message
2.variables（mule event metadata）


A Mule event is generated when a trigger (such as an HTTP request or a change to a database or file) reaches the Event source of a flow. This trigger could be an external event triggered by a resource that might be external to the Mule app.

mule event的 构造与析构
1 A trigger reaches the event source.
2 The event source produces a Mule event.
3 The Mule event travels sequentially through the components of a flow.
4 Each component interacts in a pre-defined manner with the Mule event.
。。触发器收到事件源，根据事件源生成mule事件，mule事件被flow中一系列的组件使用。每个组件按预定义的行为来和mule事件交互。



https://docs.mulesoft.com/mule-runtime/4.3/about-mule-variables
Variables in Mule Apps
Variables被用来保存 每个事件(per-event) 的值，这些值在mule应用的某个flow中被使用。
保存的数据可以是任何支持的数据类型，如对象，数字，string。

它也可以保存当前消息(使用message 关键字)
当前消息payload（关键字：payload）
当前消息的属性（attributes）
。。这3个就是半页前说的，mule event 包含的2样东西中的一样。 但是半页前的mule Message 和 variables 不就重复了吗？
zzzzz。。还是说这里的variables 是 mule event中 全部变量？

你甚至可以使用DW表达式做为 值。但是，vars关键字(vars.someOtherVar) 不允许。
。。结合下面vars的定义，不能自定义，这样会递归，死循环。

你能创建，更新variables 通过如下方式：
使用 Set Variable 组纪
在操作中使用目标变量，如  the Read operation to the File connector or the Store operation to the Database connector.
使用DataWeave Transform 组件（EE-Only， 企业版？）
使用Scripting 组件（在scripting 模块）

也能删除/移除
使用Remove Variable 组件

创建variable后，在你创建它的这个flow中（的后续其他位置），你能访问和使用它，包括在这个flow中ref的其他flow。

vars：访问variable的关键字。如果你的variable的名字是myVar，那么可以vars.myVar来访问。



https://docs.mulesoft.com/mule-runtime/4.3/about-mule-message
Mule Message Structure

Mule message 是mule event的一部分。
mule message 是 message内容 和 metadata 的容器。
通常来自外部源，在mule应用的flow中被处理。

mule message不可变。每次对它的修改都会生成一个新的mule message.
..一页半前 说的是 event不可变，每次都是new一个，不冲突。 ---> newEvent+oldMsg oldEvent+newMsg..

flow中每个处理器接受一个message，返回一个新的mule message。
zzzzz。。。flow 中 processor 处理的是 message， 而不是event？。。前面说的是 event啊。

Mule Message 由2部分组成：
payload，包含了消息的body。如 文件的内容，数据库的一条记录，REST/Web服务 的response
attributes payload的metadata。


A Mule message is created as part of a Mule event when a Message source in a Mule flow triggers a flow to start, as when an HTTP listener receives a response or each time the Scheduler component triggers an execution of the flow.

For example, when an HTTP listener in a Mule app receives a response, it creates a Mule event with a Mule message that contains the content of the response as its payload along with the metadata associated with that content, such as HTTP headers, as it attributes. Message processors in the flow (such as Core components, File read operations, or the HTTP request operations) can then retrieve, set, and process Mule message data that resides in the Mule event according to their configurations.
。。这里是，mule发出请求，然后对面有response了。就把response内容做为payload，http头做为attributes。



Message Payload

File Payload
{ "hello" : "world" }
payload.'hello'


Attributes

you can access attributes of an HTTP response or a file that you are reading with the following syntax:
attributes.'statusCode' to select an HTTP status code like 200.
attributes.headers.'date' to select Sun, 20 Jan 2019 18:54:54 GMT from the header of an HTTP response.
attributes.headers.'content-type' to select the HTTP content type application/json.

File Attributes
LocalFileAttributes[
  lastModifiedTime=2019-01-20T08:17:55,
  lastAccessTime=2019-01-20T10:54:55,
  creationTime=2019-01-20T08:17:55,
  size=22,
  regularFile=true,
  directory=false,
  symbolicLink=false,
  path=/Users/me/Desktop/myJson.json,
  fileName=myJson.json

attributes.'fileName' to return the file name: myJson.json.
attributes.size to return the size of the file: 22.
zzzzz。。为什么有的没有''，有的有，""能不能用？
。。下面说了。。。


Valid Identifiers for Attribute Names
You can access and declare attribute names that follow the rules described in Rules for Declaring Valid Identifiers without using any quotation marks.
。。如果是一个合法的标识符，就可以直接用，不需要任何引号。

To use an attribute name that is not a valid identifier, surround the attribute name with any of the following characters:
Single quotation marks (')
Double quotation marks (")
Backticks (`)
。。名字是一个非法的标识符就需要使用，单引号，或双引号，或反引号。

var myVar = {
              id : "1234",
              "123 abc !@#" : "some_value"
            }
myVar.'123 abc !@#'

myVar.id
myVar.'id'
myVar."id"
myVar.`id`



https://docs.mulesoft.com/mule-runtime/4.3/mule-error-concept
Mule Errors

Mule errors are grouped into error types that follow a hierarchical order and use a naming convention

To inspect and use values of Mule errors in your error handling configurations, you can also use selector expressions.

Mule errors include expression (EXPRESSION) and stream-related errors (STREAM_MAXIMIMUM_SIZE_EXCEEDED), while the operations can throw many different types of errors.


Selector Expressions for Mule Errors
Mule errors are complex data types with several fields, such as a description and type. When logging and handling Mule errors, can select the values from any number of the error fields.

Field       	Description                 	Selector Expression
Description   A description of the problem.    #[error.description] and
。。。and后本来就没有的。。

Detailed Description
	A description of the problem, which can be the same or more extensive than the description.
		#[error.detailedDescription]

Type
	A type, used to characterize the problem and allow for routing within an error handler.
		#[error.errorType]

Cause
	The underlying Java Throwable that resulted in the failure.
		#[error.cause]

Message
	An optional Mule message about the problem.
		#[error.errorMessage]

Child Errors
	An optional collection of inner errors, used by elements like Scatter-Gather to provide aggregated route errors.
		#[error.childErrors]


when an HTTP request fails with an HTTP:NOT_FOUND error (for a 404 status code), the values for each part of the Error Message are:
#[error.description] returns:
	HTTP GET on resource 'http://jsonplaceholder.typicode.com:80/mybadrequest' failed: not found (404).
#[error.detailedDescription] returns 
	HTTP GET on resource 'http://jsonplaceholder.typicode.com:80/mybadrequest' failed: not found (404).
#[error.errorType] returns 
	HTTP:NOT_FOUND
#[error.cause] returns 
	org.mule.extension.http.api.request.validator.ResponseValidatorTypedException
#[error.errorMessage] returns:
	org.mule.runtime.core.internal.message.DefaultMessageBuilder$MessageImplementation
	{
	  payload=org.mule.runtime.core.internal.streaming.bytes.ManagedCursorStreamProvider@223d8f75
	  mediaType=application/json; charset=UTF-8
	  attributes=org.mule.extension.http.api.HttpResponseAttributes
	{
	   Status Code=404
	   Reason Phrase=Not Found
	   Headers=[
	      date=Sat, 03 Aug 2019 04:28:29 GMT
	      content-type=application/json; charset=utf-8
	      ....
#[error.childErrors] returns: 
	[]

The errorMessage becomes available when a connector or component exposes the information it normally propagates upon success, when the error does not occur. For example, when an HTTP request receives a status code that Mule treats as an error, the process fails and also populates the errorMessage with information about the error. You can then gain access to error message attributes (metadata) and to the payload itself with #[error.errorMessage.payload] for the payload and #[error.errorMessage.attributes] for the metadata. In the case of an HTTP request that returns an error, you can then use #[error.errorMessage.attributes.statusCode] to select the value of the status code (such as 404). 
。。前半段感觉等于没说。


Mule Error Types
Mule errors have a namespace (such as HTTP: and FILE:) and identifier (such as NOT_FOUND), and they belong to a hierarchy of error types.

Unlike connectors, Mule runtime errors have an implicit MULE namespace, so MULE:EXPRESSION and EXPRESSION are interpreted the same way.

Error types can have a parent type. For example, the parent of HTTP:UNAUTHORIZED is MULE:CLIENT_SECURITY, and the parent of MULE:CLIENT_SECURITY is MULE:SECURITY

The hierarchies enable you to route errors in a general or more specific way

All errors belong to one of these two main types: ANY or CRITICAL
Each type under ANY is matched by its parent and can be handled, while error types under CRITICAL are so severe that cannot be handled and are only logged. 
CRITICAL errors include FATAL_JVM_ERROR and OVERLOAD.

Whenever there is no clear reason for a failure, a component can use the UNKNOWN type. 

Error Types:
ANY: Error type that matches all error types that occur in a Flow and can be handled. This type does not include errors that occur on the source.
	TRANSFORMATION: indicates an error occurred while transforming a value. This involves Mule Runtime internal transformations and not DataWeave transformations.
	。。非DW转换的错误
	EXPRESSION: indicates an error occurred while evaluating a DataWeave expression.
	。。DW转换的错误

	VALIDATION: indicates a validation error occurred.
		DUPLICATE_MESSAGE: indicates a validation error regarding a message being processed twice. For example, using the Idempotent message validator.

	REDELIVERY_EXHAUSTED: indicates that max attempts to reprocess a message from a source have been exhausted.

	CONNECTIVITY: indicates that there was a problem establishing a connection. This could occur while using a connector, for example, an HTTP requester.
		RETRY_EXHAUSTED: indicates that retries of a certain execution block have been exhausted. For example, for a given operation, or using Until Successful Scope.

	ROUTING: indicates an error occurred while routing a message. For example, using the Round Robin router.
		COMPOSITE_ROUTING: indicates that one or more errors occurred while routing a message. For example, using a Scatter Gather Router.

	SECURITY: indicates a security error occurred, like invalid credentials being received or an expired token being used.
		CLIENT_SECURITY: indicates an external entity (e.g., calling an external endpoint) produced a security error.
		SERVER_SECURITY: indicates a security error enforced by the Mule Runtime.
			NOT_PERMITTED: indicates a security restriction enforced by a filter. For example, using the Authorization Filter of the Mule Spring Module.

	STREAM_MAXIMUM_SIZE_EXCEEDED: indicates the maximum size allowed for a stream has been exceeded. For more insight, see Streaming in Mule Apps.

	TIMEOUT: indicates timeout occurred while processing a message.

	UNKNOWN: indicates an unknown or unexpected error occurred. This cannot be handled directly, only by handling ANY, to ensure backward compatibility in case more error types are added in future runtime versions.

SOURCE: indicates that an error occurred in the source of the flow.
	SOURCE_ERROR_RESPONSE_GENERATE: indicates that an error occurred in the source of the flow generating the parameters of an error response. This error cannot be handled since the source has already executed the failing path.
	SOURCE_ERROR_RESPONSE_SEND: indicates that an error occurred in the source of the flow sending an error response. This error cannot be handled since the source has already executed the failing path.

SOURCE_RESPONSE: indicates that an error occurred in the source of a flow while processing a successful response. These errors can only be propagated since the source has already executed the successful path.
	SOURCE_RESPONSE_GENERATE: indicates an error occurred in the source of the flow while generating the parameters of a successful response.
	SOURCE_RESPONSE_SEND: indicates an error occurred in the source of the flow while sending a successful response.

CRITICAL: indicates a severe error occurred. These errors cannot be handled.
	OVERLOAD: indicates a problem of overloading occurred and the execution was rejected.
		FLOW_BACK_PRESSURE: indicates a problem of overloading occurred at the source level. For example, using an HTTP listener as a source.
	FATAL_JVM_ERROR: indicates that a fatal error occurred, such as stack overflow.



Custom Error Types
自定义错误类型，需要定义它们的映射和触发(raising, 对应Raise Error Component)
这些错误需要一个唯一的自定义namespace 来和其他错误类型区分。


Error Mappings
你可以映射可能的错误类型到自定义错误类型。
你可以使用这些自定义错误类型来 区分 一个flow中错误发生的地点。

For example, if your flow has two HTTP Request operations that reach out to different REST services, a connectivity failure on either produces the same error. By mapping each error to different custom error types, you can differentiate the error handling of each operation failure and quickly identify the source of the error in the Mule app logs.
。。一个flow，2个rest访问，都报相同的错，可以通过映射每个错误到不同的自定义类型，，，这样可以很快区分到底哪个rest报错。

In the following example, you can see how mappings allow granular error handling by defining two custom error types: APP:CUSTOMER_API and APP:ORDER_API.

Example XML Configuration for Mappings:

<flow name="retrieveMatchingOrders">
  <http:request config-ref="customersConfig" path="/customer">
    <error-mapping sourceType="CONNECTIVITY" targetType="APP:CUSTOMER_API"/>
  </http:request>
  <http:request config-ref="ordersConfig" path="/order">
    <error-mapping sourceType="CONNECTIVITY" targetType="APP:ORDER_API"/>
  </http:request>
  <error-handler>
    <on-error-continue type="APP:CUSTOMER_API">
      <logger message="#['Could not retrieve customer data.']"/>
    </on-error-continue>
    <on-error-continue type="APP:ORDER_API">
      <logger message="#['Could not retrieve customer order data.']"/>
    </on-error-continue>
  </error-handler>
</flow>

。。sourceType 都是 CONNECTIVITY， targetType不同。 处理的时候根据不同的type 打印不同的日志，这样看日志就知道哪个错误了。
zzzzz。。这个自定义错误，好像也只是对 现有错误的 一种封装/转换啊。并不是 新的。






https://docs.mulesoft.com/mule-runtime/4.3/advanced-mule-concepts
Advanced Mule Concepts

you can focus on more advanced features, such as the execution engine, the classloading isolation mechanism, and distributed file polling.
。。执行引擎，类加载隔离原理，分布式文件轮询(?)

distributed locking


https://docs.mulesoft.com/mule-runtime/4.3/execution-engine
Execution Engine

Mule runtime engine implements a reactive execution engine, tuned for nonblocking and asynchronous execution.
。。反应/响应式执行引擎，协调无阻塞异步执行。

This task-oriented execution model enables you to take advantage of nonblocking I/O at high concurrency levels transparently, meaning you don’t need to account for threading or asynchronicity in your flows. 

Mule event processors indicate to Mule whether they are CPU-intensive, CPU-light, or I/O-intensive operations. These workload types help Mule tune for different workloads, so you don’t need to manage thread pools manually to achieve optimum performance.
。。但是，现在没有 3种线程池了，只有一个线程池了啊。
zzzzz。。下面Threading 也说了，看来是，一个线程池，3个执行选择，每个线程都有个类型来表明是cpu密集，执行的时候，尽量一个IO密集，一个CPU密集一起执行。。。但是只有一个线程池啊。。。搞不懂

Processing Types
CPU-Light
For quick operations (around 10 ms), or nonblocking I/O, for example, a Logger (logger) or HTTP Request operation (http:request). These tasks should not perform any blocking I/O activities. The applicable strings in the logs are CPU_LIGHT and CPU_LIGHT_ASYNC.

Blocking I/O
For I/O that blocks the calling thread, for example, a Database Select operation (db:select) or a SFTP read (sftp:read) . Applicable strings in the logs are BLOCKING and IO.

CPU Intensive
For CPU-bound computations, usually taking more than 10 ms to execute. These tasks should not perform any I/O activities. One example is the Transform Message component (ee:transform). The applicable string in the logs is CPU_INTENSIVE.

看每个组件/模块的doc，如果没有说明，则是CPU Light


Threading
Mule 4.3 contains one unique thread pool, called the UBER pool.
This thread pool is managed by Mule and shared across all apps in the same Mule instance.
zzzzz。。app和mule的范围，app难道只是一个flow？mule是指整个工程？还是说app是工程，mule是运行环境(或者说服务器)，这个环境可以运行多个工程？。。估计后者

At startup, Mule introspects the available resources (such as memory and CPU cores) in the system and tunes automatically for the environment where Mule is running. This algorithm was established through performance testing and found optimal values for most scenarios.

This single thread pool allows Mule to be efficient, requiring significantly fewer threads (and their inherent memory footprint) to process a given workload when compared to Mule 3.


Proactor Pattern
Proactor is a design pattern for asynchronous execution

According to this design pattern, tasks are classified in categories that correspond to Mule processing types, and each task is submitted for execution to the UBER pool.

Performance testing shows that applying the Proactor pattern leads to better performance, even with one unique thread pool, because it allows threads to return to the main loop more quickly, allowing the system to continue to accept new work while the I/O tasks are blocked and waiting.


Transactions
When there’s an active transaction, all thread switches are suspended. Event processors participating in the transaction are executed in the same thread.
。。。这，有事务的时候，事件处理器顺序变成串行化。


Configuration
The thread pool is automatically configured by Mule at startup, applying formulas that consider available resources such as CPU and memory.

If you run Mule runtime engine on premises, you can modify these global formulas by editing the MULE_HOME/conf/schedulers-pools.conf file in your local Mule instance.
zzzzz。。。这个文件在哪里。
。。在AnyPoint的工程中有个Mule Server 4.3.0 EE, 这里有个 conf文件夹，下面有这个文件。

Configure the org.mule.runtime.scheduler.SchedulerPoolStrategy parameter to switch between the two available strategies:
UBER
Unified scheduling strategy. Default.
DEDICATED
Separated pools strategy. Legacy.

# The strategy to be used for managing the thread pools that back the 3 types of schedulers in the Mule Runtime
# (cpu_light, cpu_intensive and I/O).
# Possible values are:
#    - UBER: All three scheduler types will be backed by one uber uber thread pool (default since 4.3.0)
#    - DEDICATED: Each scheduler type is backed by its own Thread pool (legacy mode to Mule 4.1.x and 4.2.x)
。。就是3个线程池，还是一个线程池。


UBER Scheduling Strategy
When the strategy is set to UBER, the following configuration applies:
org.mule.runtime.scheduler.uber.threadPool.coreSize=cores
org.mule.runtime.scheduler.uber.threadPool.maxSize=max(2, cores + ((mem - 245760) / 5120))
org.mule.runtime.scheduler.uber.workQueue.size=0
org.mule.runtime.scheduler.uber.threadPool.threadKeepAlive=30000


DEDICATED Scheduling Strategy
When the strategy is set to DEDICATED, the parameters from the default UBER strategy are ignored.
To enable this configuration, uncomment the following parameters in your schedulers-pools.conf file:
org.mule.runtime.scheduler.cpuLight.threadPool.size=2*cores
org.mule.runtime.scheduler.cpuLight.workQueue.size=0
org.mule.runtime.scheduler.io.threadPool.coreSize=cores
org.mule.runtime.scheduler.io.threadPool.maxSize=max(2, cores + ((mem - 245760) / 5120))
org.mule.runtime.scheduler.io.workQueue.size=0
org.mule.runtime.scheduler.io.threadPool.threadKeepAlive=30000
org.mule.runtime.scheduler.cpuIntensive.threadPool.size=2*cores
org.mule.runtime.scheduler.cpuIntensive.workQueue.size=2*cores


Considerations
MuleSoft doesn’t recommend changing the used pool strategy nor change its configuration values. 

MuleSoft recommends that you perform load and stress testing with all applications involved in real-life scenarios to validate any change in the threading configurations and to understand how the thread pools work in Mule 4.



Configuration at the Application Level

You can define the pool strategy to use in an application by adding the following to your application code:
<ee:scheduler-pools poolStrategy="UBER" gracefulShutdownTimeout="15000">
   <ee:uber
       corePoolSize="1"
       maxPoolSize="9"
       queueSize="5"
       keepAlive="5"/>
</ee:scheduler-pools>

The poolStrategy parameter exists for backward compatibility, enabling you to revert to the three pools scheme from earlier Mule versions:
<ee:scheduler-pools gracefulShutdownTimeout="15000">
   <ee:cpu-light
           poolSize="2"
           queueSize="1024"/>
   <ee:io
           corePoolSize="1"
           maxPoolSize="2"
           queueSize="0"
           keepAlive="30000"/>
   <ee:cpu-intensive
           poolSize="4"
           queueSize="2048"/>
</ee:scheduler-pools>

。。。，放哪里。


Considerations
Applying pool configurations at the application level causes Mule to create a completely new set of thread pools for the Mule app. This configuration does not change the default settings in scheduler-conf.properties, which is particularly important for on-premises deployments in which many Mule apps are deployed to the same Mule instance.
。。scheduler-conf.properties  好像得自建啊，放哪里？


Custom Thread Pools
Besides the unique UBER thread pool, some components might create additional pools for specific purposes:
NIO Selectors
	Enables nonblocking I/O. Each connector can create as many as required.
Recurring tasks pools
	Some connectors or components (expiration monitors, queue consumers, and so on) might create specific pools to perform recurring tasks.


Back-Pressure Management
。。反应式编程中的一个机制。就叫背压，，是描述 处理速度<输入速度 的情况。

Back pressure can occur when, under heavy load, Mule does not have resources available to process a specific event.
This issue might occur because all threads are busy and cannot perform the handoff of the newly arrived event, or because the current flow’s *maxConcurrency value has been exceeded already.

If Mule cannot handle an event, it logs the condition with the message Flow 'flowName' is unable to accept new events at this time.
Mule also notifies the flow source, to perform any required actions. The actions that Mule performs as a result of back-pressure are specific to each connector’s source. For example, http:listener might return a 503 error code, while a message-broker listener might provide the option to either wait for resources to be available or drop the message. In some cases, a source might disconnect from a remote system to avoid getting more data than it can process and then reconnect after the server state is normalized.


Troubleshooting
When troubleshooting a Mule app, consider thread naming conventions.
。。就是 明确线程命名规范，这样好找。

<flow name="echoFlow">
   <http:listener config-ref="HTTP_Listener_config" path="/echo"/>
   <set-payload value="#['Echo ' ++ now()]" mimeType="text/plain"/>
   <logger level="ERROR" message="request at #[now()]" />
</flow>
	After execution, the flow produces the following log:
ERROR 2020-03-12 19:13:45,292 [[MuleRuntime].uber.07: [echo].echoFlow.CPU_LITE @63e7e52c]
[event: bd56a240-64ae-11ea-8f7d-f01898419bde] org.mule.runtime.core.internal.processor.LoggerMessageProcessor:
request at 2020-03-12T19:13:45.283-03:00[America/Argentina/Buenos_Aires]

Thread names also show up on thread dumps or when using a profiler:




https://docs.mulesoft.com/mule-runtime/4.3/about-classloading-isolation
Class-loading Isolation
Mule runtime engine is developed using Java and many other third-party libraries.

The default class-loading mechanism of the JVM makes it possible to have conflicting versions of the same JAR files.
。。默认的加载机制可能导致jar包冲突。

In Mule, each artifact (the runtime, the Mule apps, and Mule extensions) is developed and released independently, but the JARs for these artifacts are together in the same location. For example, a Mule app might use Apache Commons Collections 2.1. If an extension you use requires Apache Commons Collections 3.1, the app might not work as expected because Commons Collections 2.1 and Commons Collections 3.1 have conflicting resources.
。。每个artifact 是独立的(开发，发布)，但是jar包放在一起的，

Mule 3 Solution
To solve the problem of clashes between different apps, Mule 3 relied on hierarchical organization of class loaders. Though Mule apps can see their extensions, the libraries bundled within them, and their runtime libraries, Mule apps cannot access other app libraries.
Other potential conflicts remained in Mule 3, such as clashes between libraries from different extensions, clashes between extension and app libraries, and clashes between app and runtime libraries.
。。为了解决冲突问题，mule3依靠 按等级(估计不是，估计是call hierachy的那个，就是按照调用组织类加载)组织 类加载。但，还是没说怎么解决jar冲突啊，按等级组织类加载，但不管怎么说，不同版本的jar还是会加载到jvm的，还是会报错啊。
zzzzz。。。后面也说了，把这个app能看到的东西绑在它身上。。。但。。


Mule 4 Solution
In Mule 4, the class-loading mechanism addresses all the problems depicted above for Mule 3.
。。说了什么。。


API Definition
Mule runtime engine and the artifacts (apps, domains, policies, and extensions) need to have a clear API definition that specifies a contract for each artifact and limits the exposure of the artifact content to other artifacts.

A clear API for each artifact provides a way to limit the scope of classes and libraries, which in turn makes it possible to create class loaders that do not share everything.


Mule 4 API
Mule 3 exposes all the classes that are bundled within its libraries. This exposure caused problems when upgrading from one Mule version to another because changes in the runtime could potentially affect any of your custom Java classes.

Mule 4 provides a well-defined API that makes it easier for you to extend Mule and provides clarity on the proper extension points:

Core API: Mule message.

Extensions API: Modules, message processors, transforms, and other elements to extend Mule Runtime 4.

Tooling API: All DataSense metadata and propagation is now part of Mule Runtime 4. This API is bundled with the Runtime Manager agent.



Artifact API
When you create an artifact (app, domain, policy, or extension), you need to declare the public API of that module.



Considerations
Resources and classes that are not exported are not visible from other artifacts.
Expose the minimum required set of packages and resources from your artifact.
Do not export third-party libraries unless required.
Never expose common third-party libraries like Guava, Apache common-collections, and so on. Doing so will cause clashes with other artifacts.
Ideally, expose only classes from your artifact (or the JDK).



Semver


API restrictions
All public APIs might have restrictions. These restrictions limit how you can use the API. See API annotations module for more details.



Class-loading Isolation Mechanism
Once APIs are clearly defined, you can prevent access to internal classes of the artifact and only make the public API accessible from outside. To protect the APIs, Mule 4 uses a custom class-loading mechanism.

Consider the following extension file structure:
com/example/extension/api/MyClass.class
com/example/extension/internal/Util.class
transformations/customer-to-user.dwl
license.txt
META-INF/mule-artifact/mule-artifact.json

And the following mule-artifact.json descriptor file for the extension:
{
    "name": "my-test-extension",
    "minMuleVersion": "4.0.0",
    "classLoaderModelLoaderDescriptor": {
        "id": "mule",
        "attributes": {
            "exportedResources": [
                "transformations/customer-to-user.dwl"
            ],
            "exportedPackages": [
                "com/example/extension/api"
            ]
        }
    }
}

The following diagram shows how resources exported by mule-artifact.json are applied within an app:




Adding Dependencies to Connectors
When the connectors in your application need access to a dependency in your project, configure the dependency to be visible by the connector’s class loader. There are two different ways to do this:
Configure application dependencies as shared libraries.
Configure additional plugin dependencies for the connector.


Shared Libraries
All dependencies (JAR files, for example) declared in the application’s pom.xml file are visible to the application’s class loader but not visible to the class loader of each connector used in the application. 
The Mule runtime engine class-loading mechanism isolates each connector to prevent it from accessing classes from other connectors. If a connector’s class loader needs access to an application dependency, declare this dependency as a shared library.


Additional Plugin Dependencies
Instead of configuring shared libraries to make application dependencies visible to all connectors in the application, you can configure additional plugin dependencies to enrich a specific connector’s classpath. This configuration enables you to effectively add new dependencies to a connector, as if the dependencies were declared in the connector’s pom.xml file.




https://docs.mulesoft.com/mule-runtime/4.3/distributed-file-polling
Distributed File Polling

Enterprise Edition

Some connectors, such as the File connector or FTP connector, poll directories and read certain files as they are created in the directories polled. These files can reside on a remote file system, including file systems of nodes belonging to a Mule High Availability (HA) Cluster.

In Mule 4, distributed file polling makes it possible to poll files in all cluster nodes. Enabled by default, this feature is used by the following connectors:
File Connector
FTP Connector
SFTP Connector

You can configure connectors to only poll from the primary node, @PrimaryNodeOnly, which ignores the default setting set by the Mule runtime engine. This feature is only available in Mule 4.x



https://docs.mulesoft.com/mule-runtime/4.3/distributed-locking
Distributed Locking
Mule runtime engine provides the ability to create locks for synchronizing access to resources within Mule components.
To manage concurrent access to resources, Mule provides a lock factory that you can access programmatically by scripts or in custom extensions built with the Java SDK.

Any locks you create with the Mule lock factory work seamlessly on deployment models that use either a single server or a cluster of servers.
。。seamlessly  无缝地

LockFactory creates a Lock that implements java.util.concurrent.Lock but does not support the method newCondition(). Using newCondition() causes Mule to throw an UnsupportedOperationException. Mule does support other methods of interfacing with a lock, such as lockInterruptibly() and tryLock().



Create a Lock
To create a lock, perform these steps:
Gain access to a LockFactory through injection.
	If you are building an extension with the Java SDK and need locking for one of your operations, you inject LockFactory into your class.

Use LockFactory to start creating locks.
	See the following sample code that declares an extension operation that manages code access through Mule locks and thus prevents synchronization issues when the operation is executing in parallel (like in a scatter gather scope or in different cluster nodes).

Use an identifier for the lock you create.
	An identifier allows you to access one lock from different threads without having to explicitly share the same lock instance between them.

。。通过注入，获得LockFactory，通过LockFactory创建锁，为锁设置一个标识符

This lock implements the java.util.concurrent.Lock interface, so calling the method lock() makes the thread wait until it acquires the lock.

lock后必须unlock，一般在finally中unlock
public class ClusterOperations {

  @Inject
  private LockFactory lockFactory;

  public void sharedResourceOperation() {
      Lock lock = lockFactory.createLock("sharedResourceId");
      lock.lock();
      try {
          // Lock acquired, execute critical code.
      } finally {
          lock.unlock();
      }
  }
}


Create a Lock Using Scripting
If you are using the scripting extension, you must first access the registry to obtain the LockFactory and then create locks through scripting code.
The following example uses Groovy to achieve the same result as the previous example:

lockFactory = registry.lookupByName("_muleLockFactory").get()
lock = lockFactory.createLock("sharedResourceId")
lock.lock()
try {
	// Lock acquired, execute critical code.
} finally {
	lock.unlock()
}

。。registry是什么？全局变量？mule提供的？



Share a Lock Between Operations
also can be used for different operations that may be running in parallel
Using the same lock ID to create a lock in different operations allows those operations to obtain access to the same lock. 
The following code sample illustrates two custom operations that create locks using the same lock ID:

public class ClusterOperations {

  @Inject
  private LockFactory lockFactory;

  public void sharedResourceOperationA() {
      Lock lock = lockFactory.createLock("sharedResourceId");
      lock.lock();
      try {
          // Lock acquired, execute critical code.
      } finally {
          lock.unlock();
      }
  }

  public void sharedResourceOperationB() {
      Lock lock = lockFactory.createLock("sharedResourceId");
      lock.lock();
      try {
          // Lock acquired, execute critical code.
      } finally {
          lock.unlock();
      }
  }
}

。。createLock 是单例锁。



Mule Javadocs
这里就一个网址，就是最上面的apidoc


Third-Party Software in Mule
列表，展现了mule使用到的框架。









https://docs.mulesoft.com/mule-runtime/4.3/mule-app-dev
Mule Application Development
A request to a Mule application triggers Mule to encode the request and data in a Mule event and to pass it to either single or multiple threads.


Getting Started with Mule Application Development

Building Blocks of a Mule Application

DataWeave Language
DataWeave is the primary language used for formulating expressions in Mule. Connectors, modules, and components support the use of DataWeave to access, manipulate, and transform data structures and output formats, and to extract data that is processed within the Mule application.
At runtime, Mule evaluates DataWeave expressions while executing a flow to:
Extract data needed to process the current message
Set or manipulate a value in the message

Mule Flows

Sources
A source component (or trigger) is the first component in a flow. It receives a triggering event, creates a corresponding Mule event, and forwards that event for processing by the next component in the flow.
Examples of listeners and connector operations that can trigger a flow include:
HTTP, JMS, and VM listeners in their associated connectors
On Table Row operation in the Database connector
On New or Updated File operation in the File and FTP connectors
Scheduler

Processors
After a flow is triggered through the source component, subsequent components process the data as it travels through the flow. 
By default, each processor that receives a Mule event returns a new Mule message


Security
You can provide security to your Mule applications by encrypting properties, configuring secure communications over TLS, setting up authentication over OAuth 2.0, and providing cryptographic and other capabilities, such as FIPS compliance.


Development Environments

Mule Versioning
。。分类，core，connector等， DW1，DW2.




https://docs.mulesoft.com/mule-runtime/4.3/mule-app-dev-hellomule
Hello Mule Tutorial

This flow accepts an HTTP request, sets a static payload on the message, and then returns a response to you.


When you deselect fx, the field contains only the string "Hello Mule" and does not contain a hash (#) or square brackets ([]).
。。fx是什么意思，感觉是：是否把后面的string做为DW表达式来执行？。是不是DW? 的表达式？

。。照网上默认的0.0.0.0不行，配置用localhost就可以。。
。。配置了0.0.0.0，用localhost可以访问，用ip无法访问。

输出的日志
INFO  2020-08-25 16:57:19,162 [[MuleRuntime].uber.01: [hellomule].hellomuleFlow.CPU_LITE @720914e0] [processor: hellomuleFlow/processors/1; event: fb374f40-e6b0-11ea-8b29-001e64fb1477] org.mule.runtime.core.internal.processor.LoggerMessageProcessor: /hellomule

线程是uber.01， cpu_lite,,,lite 清淡的(light的一种拼写方法)
logger里Message选择fx，输出 attributes.requestPath， 所以输出了 /hellomule



Configuration XML for the Hello Mule Example

和网站上有一些差别
网站
<set-payload value="Hello Mule!"
实际
<set-payload value='"hi,mule."'
。。估计是编写的问题。网站的xml对应的是在set payload中直接helloMule, 没有""的

还有就是，实际的xml中，每个标签都有一个doc:id。
	<http:listener-config name="HTTP_Listener_config" doc:name="HTTP Listener config" doc:id="e26e5d2b-2b7f-492a-86a0-55bcf42b27c0" >
		<http:listener-connection host="0.0.0.0" port="8081" />
	</http:listener-config>
	<flow name="hellomuleFlow" doc:id="b4913fbd-db26-4a91-b22f-7dba017014bc" >
		<http:listener doc:name="Listener" doc:id="f3a524dc-7533-4f23-982d-c2b6389c7bd1" config-ref="HTTP_Listener_config" path="/hellomule"/>
		<set-payload value='"hi,mule."' doc:name="Set Payload" doc:id="f847f5cd-6b64-48df-bbd2-f7d94ca1604e" />
		<logger level="INFO" doc:name="Logger" doc:id="3d19db6c-006e-4e98-a4a3-47ecc68e7903" message="#[attributes.requestPath]"/>
	</flow>



https://docs.mulesoft.com/mule-runtime/4.3/mule-app-tutorial
Mule App Development Tutorial

。。md，DataBase Config里 required libraries，选maven下载，不行，估计是maven的配置问题，没有从maven库里找，选择用local file...可以找到jar，但是测试下，连不到数据库。。
mudb.learn.mulesoft.com 这个网址，浏览器直连也不行。。

zzzzz。。太坑了，mysql的连不上，db2说没有合适的driver。
。。kao,我就给了IP端口数据库名，前面的jdbc:db2://没有给。。。
jdbc:db2://10.145.9.85:50000/cms0331:currentSchema=DB2INST2;retrieveMessagesFromServerOnGetMessage=true;
jdbc:db2://10.145.9.85:50000/cms0331
jdbc:db2://10.145.9.85:50000/cms0331:currentSchema=DB2INST2;
这3个是可以的，2个最后有;;;

select USERNAME from sec_user where oid=1769144432;
会报错，去掉最后的; 就可以了。
org.mule.runtime.api.metadata.MetadataResolvingException: An unexpected token "" was found following "".  Expected tokens may include:  "where oid=1769144432".. SQLCODE=-104, SQLSTATE=42601, DRIVER=3.67.27
Caused by: com.ibm.db2.jcc.am.SqlSyntaxErrorException: An unexpected token "" was found following "".  Expected tokens may include:  "where oid=1769144432".. SQLCODE=-104, SQLSTATE=42601, DRIVER=3.67.27

。。但是没有""啊。。
。。由于上面配置了mule的mysql，和db2，所以导致了 global elements里有2个 DataBase Config， 但用的是db2的， 启动后，mule似乎2个连接都会尝试。等于 启动cms的时候，启动所有数据库连接，，而不是lazy。


open your REST API client.
。。md，要这个的，直接用浏览器就报错：Attempted to send invalid data through http response.
。。还是不行，postman，post/get都不行，都是上面这个错。
而且我还不知道这个到底是什么数据。。
不过docs里也说了，这里会有错，但是它说是500错，我这里不是。。。。不，我这里就是500错误，。。。看来没有问题，  确实需要一个 转换器。。
不过错误的描述不一样。docs里是使用默认的ObjectToByteArray，这个转换器，它不支持ManagedCursorIterator。。


。加了Transform Message后， payload里就直接有了 数据库的列。。。

%dw 2.0
output application/java
---
{
}

上面是默认的改成

%dw 2.0
output application/json
---
payload


可以了。。。postman返回：
[
    {
        "GENDER": null,
        "EMPLOYEENUMBER": "0000004709 jennifer_jiang ",
        "GPINTERNALORGANIZATIONID": 330530816,
        "DESCRIPTION": "",
        "SIMPLENAME": "jennifer_jiang",
        "OID": 1769144432,
        "PRIMARYROLEID": null,
        "ENABLED": 0,
        "ACCOUNTNONEXPIRED": 1,
        "PASSWORD": "ed52bccc9aa85e3bf9f111ddf3fe292a",
        "ACCOUNTNONLOCKED": 1,
        "USERPREFERENCEID": 1771601920,
        "LASTLOGON": "2016-04-28T16:01:13.739",
        "CREDENTIALSNONEXPIRED": 1,
        "USERNAME": "jennifer_jiang"
    }
]


.json.example
[
    {
        "OID": 1769144432,
        "GENDER": null,
        "SIMPLENAME": "jennifer_jiang",
        "LASTLOGON": "2016-04-28T16:01:13.739",
        "CREDENTIALSNONEXPIRED": 1,
        "user":{
            "USERNAME": "jennifer_jiang"    
        }
    }
]

..docs说拖拉，嗯，从input拖到output，来完成映射。 不是payload的那个output。
。。gender连不动。。放弃gender了。。用oid可以，因为oid是Number类型，其他的类型都带有?，

%dw 2.0
output application/json
---
payload map ( payload01 , indexOfPayload01 ) -> {
	OID: payload01.OID,
	SIMPLENAME: payload01.SIMPLENAME default "",
	LASTLOGON: payload01.LASTLOGON as String default "",
	CREDENTIALSNONEXPIRED: payload01.CREDENTIALSNONEXPIRED default 0,
	user: {
		USERNAME: payload01.USERNAME default ""
	}
}

输出
[
    {
        "OID": 1769144432,
        "SIMPLENAME": "jennifer_jiang",
        "LASTLOGON": "2016-04-28T16:01:13.739",
        "CREDENTIALSNONEXPIRED": 1,
        "user": {
            "USERNAME": "jennifer_jiang"
        }
    }
]



https://docs.mulesoft.com/mule-runtime/4.3/about-components
Core Components

分为以下几类：
Batch
包含：
Batch Aggregator
Batch Job
Batch Step

Components
Custom Business Events: For collecting information about flows and message processors that handle your business transactions. See also Business Events.
Dynamic Evaluate: For dynamically selecting a script, instead of forcing you to hardcode it through the Transform Message Component.
Flow Reference: For routing the Mule event to another flow or subflow (and back) within a Mule app.
Logger: For logging important information about your Mule app, such as error messages and status notifications.
Parse Template: For processing a template and obtaining a result.
Transform Message: For converting input data to a new output structure or format.
。。不过mule4.3的anypoint7.6 多了好几个


Endpoints
Endpoints (sometimes called Sources in Studio or Triggers in Design Center) include components that initiate (or trigger) processing in a Mule flow. The *Scheduler is an endpoint. It triggers a flow to start at a configurable interval.
。。就一个 Scheduler

Error Handling
Error Handler
On Error Continue
On Error Propagate

Flow Control (Routers)
Choice
First Successful
Round Robin
Scatter-Gather

Scopes
Async
Cache
Flow
For Each
Try
Until Successful

Transformers
Remove Variable
Set Payload
Set Variable




https://docs.mulesoft.com/mule-runtime/4.3/async-scope-reference
Async Scope
The Async scope is a branch processing block that executes simultaneously with the main flow
。。simultaneously 同时地

The flow does not have to pause until the last message processor embedded in the asynchronous flow has completed its task.
。。flow可能早于 同步块 结束。

Async can be useful for executing time-consuming operations that do not require you to send a response back to the initiating flow (such as printing a file or connecting to a mail server).
。。执行耗时的，且不需要返回数据给主flow的。
。。不耗时的话，没准起个线程的消耗都大于 执行的消耗。。


To facilitate simultaneous branch processing, the Async scope sends one copy of the message it has received to the first embedded message processor in its own processing block. At the same time, it sends another copy of the message to the next message processor in the main flow.
。。facilitate 促进
。复制2份 从上个processor传来的eevent，一份是 async里的用，一份是 主flow的下个processor用。
。。IDE中就是一个框，得记得 Async这个框是和 它的后续 并发执行的。
。。Async应该就是 新建一个线程，然后执行 框里的东西。
。。没有 回调，所以Async中的 结果 不可能 被主flow 获得。


Async Configuration
Display Name (name)
Name for the Async scope.

Max Concurrency (maxConcurrency)
Optional. Sets the maximum number of concurrent messages that the scope can process. By default, the container thread pool determines the maximum number of threads to use to optimize the performance when processing messages. When the scope is processing the maximum number of concurrent messages, it cannot receive additional requests.
Set maxConcurrency to 1 to cause the scope to process requests one at a time.
See Back-Pressure Management for details about Mule behavior after reaching the maximum concurrency value.
zzzzz。。这个线程池 是 被 主flow用到， 所以 只可能是 调用多次主flow，才会 触发多次async吧，，，，或者 写在for-each里。。也会被调用多次。。
。。这个线程池是 flow独占的，还是 所有的 async公用一个线程池，，这不可能，因为 每个async 都可以设置不同的 Max Concurrency， 所以说 是 这个 Async独占的，，那么如果 flow 是多例模式的，这个async 是不是多例，是的话，每个例 一个线程池？还是多例 就一个线程池？
zzzzz。。而且 觉得有点垃圾啊，正常来说 应该是 超过max了就 等待啊，或者newThread，但是不start，，，它直接不接受了，，这数据不丢失啊。。
。。Executors里的线程池 都有一个 无线长的 LinkedBlockingQueue 来存超过Max的线程。


Async Scopes versus Subflows

Unlike a subflow, an Async scope:
Does not inherit the exception strategy of the main flow.
To handle errors in an Async scope, you should use the Try scope.
Processes messages asynchronously.
Does not pass data back to the main flow.
Exists inline with the main flow thread.		，，，？和主flow线程紧密结合？
Is not called by a Flow Reference component.
Is not reusable
。。不能 继承/使用/共用 主flow的 异常处理。
，在async中使用try scope， 异步处理， 不能返回数据给主flow。， 不能被flow ref到，不可重用。

<async doc:name="Async">
  <!-- One or more processors here -->
</async>

Note that even though the Async scope receives a copy of the Mule message, the payload is not copied. The same payload objects are referenced by both Mule messages: One that continues down the original flow, and the one processed by the Async scope.
。。那这样的话，就意味着 async 不能写，至少不能写 和 主flow有关的 那些 属性。 也不能保存。。
。。最后的one... one.. 不就是说。。不，这里的 one 是指message。。



https://docs.mulesoft.com/mule-runtime/4.3/batch-processing-concept
Batch Processing
Batch processing is exclusive to Mule Enterprise runtimes.
。。独有

Overview
batch processing provides a construct for asynchronously processing larger-than-memory data sets that are split into individual records.

。batch job是最外层，包含了至少1个batch step, step里前半部分是processor，后半个是Aggregator(聚合器)。。job最后还有一个on Complete部分。


Basic Anatomy
The heart of Mule’s batch processing lies within the batch job. A batch job is a scope that splits large messages into records that Mule processes asynchronously. In the same way flows process messages, batch jobs process records.
。。processes asynchronously ？ 异步的？ 真是，batch job有个 max concurrency的属性。。默认是cpu核心数量*2。。

<flow name="flowOne">
	<batch:job jobName="batchJob">
		<batch:process-records>

			<batch:step name="batchStep1">
				<event processor/>
				<event processor/>
			</batch:step>

			<batch:step name="batchStep2">
				<event processor/>
				<event processor/>
			</batch:step>
		</batch:process-records>
	</batch:job>
</flow>


A batch job executes when the flow reaches the process-records section of the batch job. When triggered, Mule creates a new batch job instance.
。。话说 flow 是单例的吗？还是 每个 请求/Listener 一个实例，，，应该是单例的， 毕竟 http Listener啊，有了listener才能 听到请求。。 不可能做出 多例的。

When the job instance becomes executable, the batch engine submits a task for each record block to the I/O pool to process each record.

When the batch job starts executing, Mule splits the incoming message into records, stores them in a persistent queue, and queries and schedules those records in blocks of records to process. By default, the runtime stores 100 records in each batch step. 

After all the records have passed through all batch steps, the runtime ends the batch job instance and reports the batch job result indicating which records succeeded and which failed during processing.


Error Handling
Batch jobs can handle any record-level failure that might occur in processing to prevent the failure of a complete batch job.
。。try 每条记录 catch 异常，，，防止整个job 直接失败。


Batch Job vs. Batch Job Instance

A batch job is the scope element in an application in which Mule processes a message payload as a batch of records. The term batch job is inclusive of all three phases of processing: Load and Dispatch, Process, and On Complete.

A batch job instance is an occurrence in a Mule application whenever a Mule flow executes a batch job. Mule creates the batch job instance in the Load and Dispatch phase. Every batch job instance is identified internally using a unique String known as batch job instance id.
。。using a unique String known as batch job instance id.


Batch Job Processing Phases
Load and Dispatch.
Process.
On Complete.


Load and Dispatch

Mule splits the message using Dataweave. This first step creates a new batch job instance.
Mule exposes the batch job instance ID through the *batchJobInstanceId variable. This variable is available in every step and the on-complete phase.

Mule creates a persistent queue and associates it with the new batch job instance.

For each item generated by the splitter, Mule creates a record and stores it in the queue. This activity is "all or nothing" – Mule either successfully generates and queues a record for every item, or the whole message fails during this phase.

Mule presents the batch job instance, with all its queued-up records, to the first batch step for processing.


Process
the runtime begins processing the records in the batch asynchronously
Each record moves through the processors in the first batch step, then is sent back to the original queue while it waits to be processed by the second batch step and so on until every record has passed through every batch step.
Only one queue exists, and records are picked out of it for each batch step, processed, and then sent back to it; each record keeps track of what stages it has been processed through while it sits on this queue.
。。是一个step处理完，就放回到queue，然后等下一个step的线程有空闲了，再处理。
。。始终只有一个queue。
。。感觉 step1-2-3 直接处理完不是更好？ 
zzzzz。。难道它是这样的，线程池空，读取一个record，判断执行哪个step，new一个step processor，处理。。。这样 不能复用processor啊， 这个看processor里的 int 能不能递增。
。。感觉step1-2-3，这种更快，更好啊。。

Note that a batch job instance does not wait for all its queued records to finish processing in one batch step before pushing any of them to the next batch step. Queues are persistent.
。。有点懂了，好像有点道理，但是速度快不了吧。
zzzzz。。用的是queue，所以最开始的时候全部new step1 processor,遇到第一个需要step2的 record后，后续就new step2的processor。。。遇到处理完的record，pop掉，添加新的record到尾巴，然后遇到新的record后，就new step1 processor。
。。估计这些processor 有对象池缓存者吧。
。。还是觉得step1-2-3，更好。

。。除非 它的 聚合器(Batch Aggregator) 有 一些特殊的作用。..话说 聚合器有什么用。。


Beyond simple processing of records, there are several things you can do with records within a batch step.
You can apply filters by adding acceptExpressions within each batch step to prevent the step from processing certain records. For example, you can set a filter to prevent a step from processing any records which failed to process in the preceding step.
You can use a batch aggregator processor to aggregate records in groups, sending them as bulk upserts to external sources or services. For example, rather than upserting each contact (that is, a record) in a batch to Google Contacts, you can configure a batch aggregator to accumulate, say, 100 records, then upsert all of them to Google Contacts in one chunk.
。。处理一半，就分组，然后发给其他应用。。


On Complete
During this phase, you can optionally configure the runtime to create a report or summary of the records it processed for the particular batch job instance. 

After Mule executes the entire batch job, the output becomes a batch job result object (BatchJobResult). Because Mule processes a batch job as an asynchronous, one-way flow, the results of batch processing do not feed back into the flow which may have triggered it, nor do the results return as a response to a caller. Any event source that feeds data into a batch job must be one-way, not request-response.
。。异步执行batch job的？？。。async也是(从图上看不出异步)，从IDE的流程图上看不出异步的

You have two options for working with the output:
Create a report in the On Complete phase, using DataWeave using information such as the number of failed records and successfully processed records, and in which step any errors might have occurred.
Reference the batch job result object elsewhere in the Mule application to capture and use batch metadata, such as the number of records which failed to process in a particular batch job instance.


Scheduling Strategy
一个应用有多个batch，每个 batch定义 可以有它自己的调度策略。(就是 这个batch有多个 instance时，哪个实例先执行)
。。只是 相同batch定义 的 多个实例 之间的调度策略。

ORDERED_SQUENTIAL (the default): If several job instances are in an executable state at the same time, the instances execute one at a time based on their creation timestamp.
。。FIFO。。单个job执行

ROUND_ROBIN: This setting attempts to execute all available instances of a batch job using a round-robin algorithm to assign the available resources.
zzzzz。。轮询方式来全部执行。。 等于就是全部执行吧。。怎么轮询的呢？具体实现是什么？

The ROUND_ROBIN option is useful when you can guarantee that no job execution can have a side effect on another job execution. Therefore, this option is not a good choice for data synchronization jobs, which can update the same record in two concurrent jobs. Because the order of the job execution is not guaranteed, your result might be a prior version of the data. However, you can safely use this strategy to parallelize the job’s execution if your batch job retrieves only new records from a database or lifts individual files from an SFTP server (and you are certain that all records are completely independent).

Note that none of these strategies guarantee that records will be executed in order. The strategies do not control the execution order of records in the batch job, nor do they depend on the number of records each instance contains.



https://docs.mulesoft.com/mule-runtime/4.3/batch-filters-and-batch-aggregator
Refining Batch Steps Processing
。。refine 改进

You can set filters upon batch steps to only accept some records for processing.
You can aggregate records in groups, sending them as bulk upserts to external sources or services.

This document outlines how and when to use batch filters and the batch commit.


Batch Filters
You can apply one or more filters as attributes to any number of batch steps.

一个批处理，第一步检查是否存在salesforce contact, 第二步更新已存在的salesforce contact. 在第二步之前加一个filter，来确保第二步只处理 成功执行第一步的record。

streamline  高效。

A batch step uses two attributes to filter records:
acceptExpression
acceptPolicy

Each batch step can accept one acceptExpression and one acceptPolicy attributes to filter records.

Use the acceptExpression attribute to process only records that evaluate to true; if the record evaluates to false, the batch step skips the record and sends it to the next one.
In other words, the records with an accept expression that resolves to false are the ones that Mule filters out.
zzzzz。。失败的record会被下一个step继续process？还是直接就不会回到queue中？
。下面也是，不通过的record还会被处理吗？应该会。

The example below filters out all records where the age is less than 21; the batch step does not process those records.
<batch:job jobName="batchJob">
	<batch:process-records >
		<batch:step name="adultsOnlyStep" acceptExpression="#[payload.age > 21]">
			...
		</batch:step>
	</batch:process-records>
 </batch:job>


Use the acceptPolicy attribute from batch step to process only the records which, relative to the value of the accept policy attribute, evaluate to true. Refer to the table below for a list of the available values for the accept policy.
NO_FAILURES		Default
	Batch step processes only those records that succeeded to process in all preceding steps.
ONLY_FAILURES
	Batch step processes only those records that failed to process in a preceding batch step.
ALL
	Batch step processes all records, regardless of whether they failed to process in a preceding batch step.


Each batch job has a *maxFailedRecords attribute that controls how many failed records you are willing to accept for a batch job.




Filter Characteristics
。特征
Batch filters only apply to batch steps which, in turn, are only usable within the batch process phase of a batch job. You cannot apply filters with the Input or On Complete phases.
。。只能用在processor，不能用在input，on complete上。

If you apply no filters to a batch step, the batch processes only those records which succeeded to process in all preceding steps. In other words, the default Accept Policy applied to all batch steps is NO_FAILURES.
。默认是处理那些 在之前的processor中全部处理成功的 record。

When a batch job instance exceeds its max-failed-records value, regardless of the filter set on the batch step, the step does not process any records and pushes the failed batch job instance to the On Complete phase.
。当一个job处理 失败的record达到 max-failed-records时，step不会再处理任何record，并且job直接跳到 on complete。
。max failed records是 job的属性，所以所有step共用的，默认0，-1是无限制。

Where you apply both types of filters, Mule evaluates them in the following order:
Accept Policy
Accept Expression
。。先计算policy，然后计算expression。。



Batch Aggregator
You can use the batch aggregator scope to accumulate a subset of records from a batch step, and bulk upsert them to an external source or service.
。。upsert == update or insert.... 批量更新/插入

For example, rather than upserting each lead (i.e., record) in a batch to Salesforce, you can configure a Batch Commit to accumulate, say, 200 records and then upsert all of them to Salesforce in one chunk.
<batch:step name="Step2">
	<batch:aggregator size="200">
     <salesforce:create type="Lead" .../>
	</batch:aggregator>
</batch:step>

。。aggregator 只是一个框，具体做什么需要自己拖bulk insert 或 update 等组件过来。

。。aggregator的 streaming 和 Aggregator Size 这2个选项是 互斥的

The batch aggregator is mutable, meaning that you can access the payloads and variables of the records grouped on your batch aggregator.

Keep in mind that, when aggregating a fixed amount of records, you can access each record sequentially, or you can specify a random record to modify.
。有size的时候，是顺序访问，没有size的时候随机处理record
zzzzz。。这个配合下面的，还有IDE(size，streaming 必须 二选一)， 有size 的时候 必然 顺序访问，没有size的时候，就必须streaming，还是 顺序访问啊。。。 随机访问怎么触发？
。。ok，结合下一节，，最后的 or 是和 sequentially 并且的。。
。。当你使用size的时候，既可以顺序访问，也可以随机访问。

However, if you configured your batch aggregator to stream its content, you can only access those records sequentially.



Aggregating Records using a Fixed Size

When using a fixed-size aggregator, you can replace, change, or store the payload and variable data of each record.
。。

As stated above, since the batch aggregator is mutable, by adding a foreach scope you can iterate through a fixed-size aggregator block, you can sequentially go over each record’s data and persistently store each record’s payload and variables.
for example, use the Groovy scripting module to modify the payload and create a variable for each collected record.
<batch:job jobName="batchJob">
	<batch:process-records>
		<batch:step name="batchStep">
			<batch:aggregator doc:name="batchAggregator" size="10">
				<foreach doc:name="For Each">
					<script:execute engine="groovy">
			    	<script:code>
			        		vars['marco'] = 'polo'
							    vars['record'].payload = 'foo'
			    	</script:code>
					</script:execute>
				</foreach>
			</batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:job>

The sequential access method assumes that:
The aggregator size matches the amount of aggregated records.
There is a direct correlation between the aggregated records and the items in the list.
。。第一个，那总有一个尾巴的啊，而且filter，policy没有通过的话，不会走processor，也不会进入aggregator。。

You can also access random records by specifying the iteration number of the foreach, saving you the need to iterate through all records.
The foreach scope exposes a *records* variable. This variable is an immutable list used by foreach to keep track of the iteration and provides a random access list that is accessible across the batch aggregator.

You can carry out the same result as the example above by specifying an arbitrary index number for the records list instead of sequentially accessing each record. You can, for example, create a variable and modify the payload of the first record as shown below.

<batch:job jobName="batchJob">
	<batch:process-records>
		<batch:step name="batchStep">
			<batch:aggregator doc:name="batchAggregator" size="10">
				<foreach doc:name="For Each">
					<script:execute engine="groovy">
			    	<script:code>
			        	records[0].vars['marco'] = 'polo'
						    records[0].vars['record'].payload = 'foo'
			    	</script:code>
					</script:execute>
				</foreach>
			</batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:job>
。。只处理第一个record。。。这个语句会被执行10遍？



Considerations for Defining a Block Size

In a traditional online processing model, each request is usually mapped to a worker thread.
。传统在线处理模型，每个请求对应一个单独的工作线程。


Regardless of the processing type (either synchronous, asynchronous, one-way, request-response or even if the requests are temporarily buffered before being processed), servers usually end up in a 1:1 relationship between a request and a running thread.
。。无论处理类型(同步，异步，one-way(单行，估计是不需要返回？还是序列化执行)，请求-应答，缓存请求-过会儿执行)，服务器通常都最终变成一个请求，一个处理线程，这种1:1的关系


To improve performance, the runtime queues and schedules batch records in blocks of up to 100 records per thread.
。为了提高性能，运行时队列和调度分割记录，变成最多100个记录每个线程。
。。。最多100个请求一处理？响应会慢啊。
。。100个record/线程，指的是 batch 组件的 倒数第二个设置：Max concurrency(默认为cpu核心数2倍)。

This behavior reduces the number of I/O requests and improves an operation’s load. 
。降低了I/O请求数，改善操作的负载。。？
。。估计说的是record，现在一次读100个，之前是每次读一条处理一条。批处理能重用processor，所以不需要每条record，搜索/创建一个processor，所以能降低系统压力？

Batch jobs use the Mule runtime engine’s thread pools, so there is no default for the job.
。batch使用了mule运行引擎的线程池。

Each thread iterates through that block to process each record, and then each block is queued back, and the process continues.
。每个线程处理一个记录块，然后把记录块放到队列尾。
zzzzz。。估计之前上面的是线程内的模型，就是 也是queue，每个step处理一遍，然后下一个step再处理一遍。。估计是，，，，得看源码。
。现在这里是多线程之间的处理模式，每个块一个线程。
。。但是为什么不用blockingQueue(这个是接口，有N多个实现，如LinkedBlockingDeque,LinkedTransferQueue)? 这个确实没有意义，想的是：应该是一个ConcurrentLinkedQueue，不是blockingQueue，这个queue包含record，然后多个job线程都读这一个queue，而不是读取 块组成的queue，，，读块的话，线程的冲突会很小很小，因为，第一请求次数少了，直接/100次，而且线程处理时间长了，这样冲突的几率也变低了。
。。但是aggregator怎么搞？不同线程，不知道有没有全部处理完，那就只能这个线程里的100个做一个 聚合。


Consider having 1 million records to place in a queue for a 3-step batch job. At least three million I/O operations occur as the Mule runtime engine takes and requests each record as they move through the job’s phases.
。。1百万条记录，3个step，需要3百万次I/O。

Performance requires having enough available memory to process the threads in parallel, which means moving the records from persistent storage into RAM. The larger your records and their quantity, the more available memory you need for batch processing.
。。加载到内存，

Although the standard model of up to 100 records per thread in the batch job works for most use cases, consider three use cases where you need to increase or decrease the block size:
Assume you have 200 records to process through a batch job. With the default 100-record block size, Mule can only process two records in parallel at a time. If you request fewer than 101 records, then your processing becomes sequential. If you need to process heavy payloads, then queueing a hundred records demands a large amount of working memory.
。。一共2百条，一个block100，那么最多也只能双线程(因为一共就2个block)。如果小于101条，那么只有一个块，只能单线程。 如果处理一些重量级的payload，那么100个record会需要大量的内存。

Consider a batch job that needs to process images, and an average image size of 3 MB. In this case, Mule processes 100-record blocks with payloads of 3 MB in each thread. Hence, your default threading-profile setting would require a large amount of working memory just to keep the blocks in the queue. In this case, set a lower block size to distribute each payload through more jobs and lessen the load on your available memory.
考虑，一个job处理图片，图片平均3mb，一个块100个记录，需要300mb，4个线程，那么就需要1.2g。。此时，需要即将block size 设置的小一点。

Suppose you have 5 million records with payloads so small that you can fit blocks of 500 records in your memory without problems. Setting a larger block size improves your batch job time without sacrificing working memory load.
假设你有5百万记录，但是记录中的payload都很小，你能每个block500个记录，而不会有问题，那么设置一个大的block size 来改善job运行时间而不必牺牲工作内存(就是说不会由于占用内存过大导致内存换页。)。

To take full advantage of this feature, you must understand how the block sizes affect your batch job. Running comparative tests with different values and testing performance helps you find an optimum block size before moving this change into production.

Remember that modifying the batch block size is optional. If you apply no changes, the default value is 100 records per block.

You set the size through the Batch Aggregator component, for example:
<batch:aggregator size="100">
 ...
</batch:aggregator>
。。batch job有一个 size， aggregator也有一个size。。


Streaming Records in a Batch Aggregator
Setting your batch aggregator to stream the records enables you to aggregate all the records in the job instance, no matter how many or how large they are.
。。流式是 聚合 job里所有的record

Instead of a list of elements that you receive with a fixed-size batch aggregator, the streaming functionality ensures that you receive all the records in the job instance without running out of memory.
。stream能让你接受所有的记录，而不会超过内存限制。。估计stream是process一个，aggregate一个，而固定的size是，处理完(所有/size个？)，加载size个到内存，然后aggregate。

Remember that since this batch aggregator is streaming, you can only access its content sequentially

Due to memory restrictions, random access is not supported for streaming aggregators.
The record payloads for random access are exposed as an immutable List, and since streaming aggregators implies having access to the entire set of records, without a fixed commit size, the runtime can’t guarantee that all records will fit in memory.

Tips
Streaming from SaaS providers: In general, you likely wouldn’t use batch streaming when sending data through an Anypoint Connector to a SaaS provider like Salesforce, because SaaS providers often have restrictions on accepting streaming input. Use streaming batch processing when writing to a file such as CSV, JSON, or XML.

Batch streaming and performance: Batch processing streaming data does affect the performance of your application, slowing the pace at which it processes transactions. Though performance slows, the trade-off to be able to batch process streaming data may warrant using it in your implementation.

Batch streaming and access to items: The biggest drawback to using batch streaming is that you have limited access to the items in the output. In other words, with a fixed-size commit, you get an unmodifiable list, thus allowing you to access and iteratively process its items; with streaming commit, you get a one-read, forward-only iterator.


Batch Aggregator Characteristics
The batch aggregator scope can only exist in batch steps which, in turn, are only usable within the batch process phase of a batch job. You cannot use batch aggregators during the On Complete phase of a batch job.
。aggregator只能存在于batch的step中，即只能在batch的process中能用，不能在batch的on complete中用。

An aggregator can only wrap the final element within the batch step in which it resides.
。aggregator只能保存它所在的step的最终元素。

Several Anypoint Connectors can handle record-level errors without failing a whole batch aggregation (i.e., upsert).
。一些连接器能处理 记录级别的错误，而不必 让整个aggregator失败(?) (比如，更新)
At runtime, these connectors keep track of which records were successfully accepted by the target resource, and which failed to upsert. Thus, rather than failing a complete group of records, the connector upserts as many records as it can, and tracks any failures for notification. Some of these connectors are:
Salesforce
NetSuite
Database
To make sure that the connector you are using supports record-level errors, check the connector’s documentation.

The batch aggregator scope does not support job-instance-wide transactions. You can define a transaction inside a batch step that processes each record in a separate transaction. Think of it as a step within a step.
Such a transaction must start and end within the step’s boundaries.
。。aggregator不支持 一个job一个事务，你可以定义一个事务，在step中(应该是指step的aggregator部分)，这会导致每个 记录都在一个 独立的事务中。

You cannot share a transaction between a batch step and a batch aggregator that exists within the step. Any transaction that the batch step starts, ends before the batch aggregator begins processing. In other words, a transaction cannot cross the barrier between a batch step and the batch aggregator scope it contains.
。step和aggregator不能共用一个事务，任何事务都会在step开始时开始，在aggregator开始前结束。
zzzzz。。那就是说前面理解错了，流式不是process完一个record就aggregate的，而是处理完全部，然后才流式aggregate。


Preserving the MIME types of the Aggregated Records
Supported by Mule 4.3
。保留 聚合记录的 MIME类型。

Aggregated records are passed into the aggregator as an array containing each record’s payload. However, the MIME types associated with those payloads are not preserved by default. You can preserve record’s MIME types by specifying the *preserveMimeTypes* attribute in a batch aggregator.


[
	{
		"name": "Tony Stark",
		"alias": "Iron Man",
		"status": "DEAD"
	},
	{
		"name": "Steve Rodgers",
		"alias": "Captain America",
		"status": "RETIRED"
	},
	{
		"name": "Peter Parker",
		"alias": "SpiderMan",
		"status": "FUGITIVE"
	}
]

<batch:job name="avengersLogger">
	<batch:process-records>
		<batch:step name="log">
			<batch:aggregator size="10">
				<foreach>
					<logger message="Agent #[payload.alias] is #[payload.status]" />
				</foreach>
			</batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:name>

However, when the logger element attempts to evaluate the #[payload.alias] expression, it results in an error similar to the following:
********************************************************************************
Message               : "You called the function 'Value Selector' with these arguments:
  1: Binary ("ewogICJmaXJzdE5hbWUiOiAiUmFtIiwKICAibGFzdE5hbWUiOiAiUmFtMSIsCiAgImFkZHJlc3Mi...)
  2: Name ("alias")

But it expects one of these combinations:
  (Array, Name)
  (Array, String)
  (Date, Name)
  (DateTime, Name)
  (LocalDateTime, Name)
  (LocalTime, Name)
  (Object, Name)
  (Object, String)
  (Period, Name)
  (Time, Name)

5|                                         name: payload.alias,


The previous error occurs because MIME types are not preserved by default, and therefore Mule doesn’t know that this record is actually a JSON. You can fix this by specifying the preserveMimeTypes attribute in the batch aggregator:

<batch:aggregator size="10" preserveMimeTypes="true">
	<foreach>
	   <logger message="Agent #[payload.alias] is #[payload.status]" />
	</foreach>
</batch:aggregator>

。。这个就是Batch Aggregator 组件的最下面一个 勾选框。
。。。感觉这个是 必须的啊。。为什么不是默认呢？。。加这个功能需要怎么样的代码。




https://docs.mulesoft.com/mule-runtime/4.3/batch-job-instance-id
Batch Job Instance ID

Users of batch processing frequently need the ability to determine a Batch job’s instance ID during the execution phases of a Batch job.

The Batch job instance ID is useful to:
Pass the local job instance ID to an external system for referencing and managing data
Improve the job’s custom logging
Send email or SMS notifications for meaningful events


Example
Mule exposes the batch job instance ID through a flow variable of key *batchJobInstanceId. That flow variable is available at the beginning of the input phase. The flow variable is also available in every step and in the on-complete phase.
。。在step，on-complete中 都能获得 batchJobInstanceId 


The log output produces the following:
com.mulesoft.mule.runtime.module.batch.internal.engine.DefaultBatchEngine: Created instance '7dc5cad0-ab09-11e8-a790-9801a7b055d3' for batch job 'CreateLeadsBatch'


Custom Batch Job Instance ID
You can use a DataWeave expression to specify a unique identifier for each batch job by passing a Job Instance Id attribute that takes a DW expression.

Note that constant values are not allowed for batch jobs and if the DW expression returns a previously seen value, Mule throws an exception and does not create the job instance. If you don’t set a job instance ID, Mule auto generates a UUID to assign to your instance.
。。不允许job instance id 相同，如果有，则抛出异常。
。。那就只能用DW了啊， 直接stirng，基本都不行，毕竟多线程的多数， 除非 Max concurreny，设置为1，，， 但是也不清楚 这个job实例几时销毁？销毁后再new一个，算不算id重复，估计不会销毁，应该不算重复(感觉是bean id的样子)


In order to guarantee uniqueness, you can set the Job Id to take server date through the MEL expression:
#['Job From ' ++ now() as String {format: 'dd/MM/yy hh:mm:ss'}].

This names the execution instance Job From 15/01/16 05:23:12.
。。不如直接毫秒数，1秒太长了。。
zzzzz。。。如果 实例Id相同，抛出异常，这个异常会中断job吗？，比如现在这种id，1秒里可能会重复的，但是下一秒就不重复了，此时，肯定希望 Id相同时，异常不会有任何影响，，，感觉就是这种while(jobId 重复) {jobId = 新Id;}。。
。。上面这种不可能的， 系统不知道这个id 会不会变，就算探测到会变，也不可能知道 多少个小时一变， while循环 会死循环，很浪费CPU。




https://docs.mulesoft.com/mule-runtime/4.3/batch-error-handling-faq
Handling Errors During Batch Job

Logs of Failing Records Inside a Batch Step

When this occurs – perhaps because of corrupted or incomplete record data – Mule logs the stack traces following this logic:
	Mule gets the exception’s full stack trace.
。记录方法调用栈

	Mule strips that stack trace from any messages.
	Even if all records raise the same error, the messages being processed would probably contain specific information related to those records. For example, if I’m pushing leads to my Salesforce account and one record fails because the lead was already uploaded, another repeated lead would have different record information, but the error is the same.
。记录message的栈调用

	Mule verifies if the stack trace was already logged in the current step.
	If this was the first time the runtime encountered this error, it logs the error and showing a message like this:
		com.mulesoft.mule.runtime.module.batch.internal.DefaultBatchStep: Found exception processing record on step 'batchStep1' for job instance 'Batch Job Example' of job 'CreateLeadsBatch'.
		This is the first record to show this exception on this step for this job instance. Subsequent records with the same failures will not be logged for performance and log readability reasons:
	Mule logs on a "by step" basis. If another step also raises the same error, the runtime logs it again for that step.
。。判断是不是本step中第一次出现，是的话，再加点日志提示，

	When the batch job reaches the On Complete phase, Mule displays an error summary with every error type, and how many times it happened in each batch step.
	The error summary for a batch job with two batch steps that raised a batch.exception type:
。到达on-complete时，mule展现 错误总结。

Mule logs batch errors using this behavior as its default configuration, which only logs INFO level messages to get a balanced trade-off between logging efficiency for large chunks of data.
。mule的INFO等级 是比较平衡的一个级别。


log4j.logger.com.mulesoft.module.batch=DEBUG
设置为debug。
生产不建议设置为debug。除非你确定job不会处理大量数据。


DataWeave Functions for Error Handling
Mule 4.x includes a set of DataWeave functions that you can use in the context of a batch step.

#[Batch::isSuccessfulRecord()]
A boolean function that returns true if the current record has not thrown exceptions in any prior step.
。在任何之前的step中，当前记录，是否 没有 抛出过任何异常。

#[Batch::isFailedRecord()]
A boolean function that returns true if the current record has thrown exceptions in any prior step.
。在任何之前的step中，当前记录，是否 抛出过异常。

#[Batch::failureExceptionForStep(String)]
Receives the name of a step as a String argument. If the current record threw exception on that step, then it returns the actual Exception object. Otherwise it returns null
。。形参是step的名字，当前记录，在那个step中是否抛出过异常，抛出过则返回真实的异常，否则返回null。

#[Batch::getStepExceptions()]
Returns a java Map<String, Exception> in which the keys are the name of a batch step in which the current record has failed, and the value is the exception itself.
If the record hasn’t failed in any step, this Map will be empty but will never be null. Also, the Map contains no entries for steps in which the record hasn’t failed.
。返回Map<String, Exception>，key是当前记录执行失败的step的名字，value是异常。。如果没有失败过，则返回emptyMap

#[Batch::getFirstException()]
Returns the Exception for the very first step in which the current record has failed. If the record hasn’t failed in any step, then it returns null.
。当前记录，所有异常中的，第一个异常，没有则null。

#[Batch::getLastException()]
Returns the Exception for the last step in which the current record has failed. If the record hasn’t failed in any step, then it returns null.
。当前记录的 最后一个异常，没有则null


Example



Batch Processing Strategies for Error Handling

Mule has three options for handling a record-level error:
Finish processing Stop the execution of the current job instance. Finish the execution of the records currently in-flight, but do not pull any more records from the queues and set the job instance into a *FAILURE state. The On Complete phase is invoked.
。完成当前的record后，不再执行job，设置job为失败，执行on-complete。

Continue processing the batch regardless of any failed records, using the *acceptExpression and *acceptPolicy attributes to instruct subsequent batch steps how to handle failed records.
。继续执行，不管失败的record，使用*,* 属性来指明 其他step处理 失败的record

Continue processing the batch regardless of any failed records (using the *acceptExpression and *acceptPolicy attributes to instruct subsequent batch steps how to handle failed records), until the batch job accumulates a maximum number of failed records at which point the execution will halt just like in option 1.
。继续执行，不管失败的记录，直到 失败数 到达 batch组件的 max failed records 属性。
zzzzz。。这里的不管失败的记录，是 不管有没有失败，都会继续处理，还是不处理失败的记录。。。不，是否处理失败的任务，是通过accept policy来管理的，不是 错误处理策略 来管理的，所以应该是前者，不管发生多少错误，继续执行下去。，，，

By default, Mule’s batch jobs follow the first error handling strategy which halts the batch instance execution. The above behavior is controlled through the maxFailedRecords attributes.
默认情况下，mule在第一个错误处理后，中断batch的执行， 上面的行为通过 maxFailedRecords属性来控制。

当错误发生时，停止处理			0
继续处理，不管有多少错			-1
继续处理，直到上限，然后停止	N+

<batch:job jobName="Batch1" maxFailedRecords="0">


..Threshold 门槛，阈，开端， cross 超过
Crossing the Max Failed Threshold
When a batch job accumulates enough failed records to cross the maxFailedRecords threshold, Mule aborts processing for any remaining batch steps, skipping directly to the On Complete phase.
。错误个数>界限，不会再处理任何step，直接去on-complete.

For example, if you set the value of maxFailedRecords to "10" and a batch job accumulates ten failed records in the first of three batch steps, Mule does not attempt to process the batch through the remaining two batch steps. Instead, it aborts further processing and skips directly to On Complete to report on the batch job failure.

If a batch job does not accumulate enough failed records to cross the maxFailedRecords threshold, all records – successes and failures – continue to flow from batch step to batch step; use filters to control which records each batch step processes.



https://docs.mulesoft.com/mule-runtime/4.3/cache-scope
Cache Scope
The Cache scope is for storing and reusing frequently called data. You can use a Cache scope to reduce the processing load on the Mule instance and to increase the speed of message processing within a flow. It is particularly effective for these tasks:
Processing repeated requests for the same information.
Processing requests for information that involve large repeatable streams.

Note that you can put any number of message processors (such as connectors or components) into a Cache scope and configure the Caching Strategy to store the responses (which contain the payload of the response message) produced by the processing that occurs within the scope.


Caching Process
The Cache scope caches repeatable streams. It does not cache nonrepeatable streams, which can be read only once before they are lost. By default, all streams are repeatable in Mule unless a component’s streaming strategy is configured to be nonrepeatable.

In general, the caching process follows this sequence:
	A message enters the Cache scope.
。。进入范围

	The Cache scope determines whether the message payload is nonrepeatable.
。。确定message payload 是否 不可重复

	The Cache scope generates a key to identify the message’s payload.
	By default, Mule uses an SHA256KeyGenerator and a SHA256 digest to generate a unique key for the message payload. However, you can set up your own key through a Custom Caching Strategy.
。。对payload 生成一个 标识符
。。默认使用 SHA256KeyGenerator，你能设置你自己的key通过一个 自定义的缓存策略。

	The Cache scope compares the newly generated key to cached responses that it has previously processed and saves it in the ObjectStore you set up (recommended) or in the default InMemoryObjectStore.
		If there is no cached response event (a cache miss), the Cache scope processes the new message and produces a response.
			It also saves the resulting response in the object store (if the response is repeatable).
		If there is a cached response event (a cache hit), the Caching Strategy generates a response that combines data from both the new request and the cached response.
		Note that if the generated response is a nonrepeatable stream, the scope does not cache the response.
。。cache比较新生成的标识符 和 已经缓存的之前处理的response，保存新标识符到 你设置的 ObjectStore,或者默认的 InMemoryObjectStore。
。。如果没有已缓存的process event，则处理信息，并且生成一个响应。保存响应结果到object store
。如果存在已缓存的response，则生成一个response，这个应答里组合了新的请求和已缓存的应答的 数据。
。如果生成的响应是 nonrepeatable 流，则不会缓存。

	The Cache scope pushes the response out into the parent flow for continued processing.
。返回给父flow。


Caching Strategy
A Caching Strategy defines the actions a Cache scope takes when a message enters its sub-flow. By default, the Cache scope uses a Caching Strategy that stores data in an InMemoryObjectStore instead of an ObjectStore. MuleSoft recommends that you only use the default for nonproduction purposes, such as testing. For production environments, you should set up a Caching Strategy that uses the ObjectStore.
。默认是保存数据到InMemoryObjectStore，这个策略只建议在非生产环境使用，生产环境建议ObjectStore

Your Caching Strategy can also reference these customizations:
	Event Key for the ObjectStore, which is a DataWeave expression or Java object used to create the key to use for storing the payload in the ObjectStore.
	Response Generator for your strategy, which is a reference to a Java object that is used to create the responses returned by the Caching Strategy.
	Event Copy Strategy for mutable or immutable data.
。一些自定义项。
。事件的标识符，可以是DW表达式，用来生成标识符的java对象。
。自定义应答生成器，
。事件中可变数据，不可变数据的 复制规则。


Filters
Instead of processing all message payloads that it receives, the Cache scope can exclude specific payloads from the Cache scope flow based on an DataWeave expression.


Example
In Anypoint Studio, you can download and open the example project *Cache Scope with Salesforce Contacts* from Anypoint Exchange to learn more about how to use the Cache scope.
。。。


https://docs.mulesoft.com/mule-runtime/4.3/cache-scope-to-configure
Configure the Cache Scope
In your Mule app, add a Cache scope to a flow.
Click to open the General tab for the Cache scope.
Configure the Cache scope:
	Provide a Display Name.
	Select a Caching Strategy.
	Examples:
		From the UI: My_Caching_Strategy
		In the Studio XML: cachingStrategy-ref="My_Caching_Strategy"
			MuleSoft recommends that you reference a Caching Strategy that uses an ObjectStore instead of using the default in a production environment.
			If you need to create a Caching Strategy, see the procedure in See Also.
Opt to set up a filter for specific payloads if you need one.
	Example that uses a DataWeave expression: filterExpression="#[user.isPremium()]"
	If the message matches the expression(s), Mule executes the Caching Strategy.
	If the message does not match expression(s), Mule processes the message through all message processors within the Cache scope but never saves or produces cached responses.


Example Cache Scope Configuration
The following example shows the configuration of a Caching Strategy, which is then referenced by a Cache Scope that contains a Database Select operation and a Transform component:

<!-- Caching Strategy definition-->
<ee:object-store-caching-strategy name="Caching_Strategy" doc:name="Caching Strategy" />

<!-- The Database Connector config is necessary in this example because there is a Database Select operation-->
<db:config name="Database_Config" doc:name="Database Config" >
    <!-- Database Connector Configuration -->
</db:config>

<!-- Cache Scope configuration referencing the Caching Strategy-->
<ee:cache doc:name="Cache" cachingStrategy-ref="Caching_Strategy">
  <db:select doc:name="Select" config-ref="Database_Config">
    <db:sql >
      <!-- An SQL query-->
    </db:sql>
  </db:select>
  <ee:transform doc:name="Transform Message" >
    <ee:message >
      <ee:set-payload >
        <!-- A DataWeave transformation for the query results -->
      </ee:set-payload>
    </ee:message>
  </ee:transform>
</ee:cache>



https://docs.mulesoft.com/mule-runtime/4.3/cache-scope-strategy
Set Up a Caching Strategy

You can create a caching strategy from a Cache scope properties panel or a Global Elements configuration in Anypoint Studio. After you create a strategy, a Cache component in your flow can reference it.

Follow these steps to configure a caching strategy:
	Open the caching strategy configuration window.
		You can open it from the Cache scope properties panel or the Caching Strategies option in the Global Elements tab within Studio:
	Define the Name of the caching strategy.
	Define the ObjectStore to use by selecting any of the following options:
		Edit Inline
		Use this option to create an Object Store configuration specific for this caching strategy.

		Global Reference
		Use this option either to select an existing Object Store, or to create a new global Object Store that can be referenced by this caching strategy and any other component in your application.
zzzzz。。上面这步不行。需要导入module，哪有啊
	Select a mechanism for generating a key used for storing events within the caching strategy:
		Default
		Key Expression
			A DataWeave expression (for example, keyGenerationExpression="#[vars.requestId]").
			Note that for two requests that are the same ("equal"), you need to generate the same key. Otherwise, you can get wrong results.
		Key Generator
			Requires you to implement the com.mulesoft.mule.runtime.cache.api.key.MuleEventKeyGenerator interface.
	(Optional) Open the Advanced tab in the properties window to configure advanced settings:
		Select or create a Response Generator.
			Note that this step requires that you implement the com.mulesoft.mule.runtime.cache.api.response.ResponseGenerator interface.
		Select the Event Copy Strategy:
			Simple event copy strategy
				Data is immutable. This is the default value.
			Serializable event copy strategy
				Data is mutable.


Example Caching Strategy
The following XML snippet shows the configuration of a caching strategy that defines a persistent Object Store to store the cached the responses. The caching strategy is then referenced by a Cache Scope:

<!-- Caching strategy definition -->
<ee:object-store-caching-strategy name="Caching_Strategy" doc:name="Caching Strategy" >
  <!-- Object Store defined for the caching strategy-->
  <os:private-object-store
    alias="CachingStrategy_ObjectStore"
    maxEntries="100"
    entryTtl="10"
    expirationInterval="5"
    config-ref="ObjectStore_Config" />
</ee:object-store-caching-strategy>

<!-- Cache scope referencing the strategy-->
<ee:cache doc:name="Cache" cachingStrategy-ref="Caching_Strategy">
      <!-- Some processing logic to cache-->
</ee:cache>



https://docs.mulesoft.com/mule-runtime/4.3/choice-router-concept
Choice Router

Only one of the routes in the Choice router executes, meaning that the first expression that evaluates to true triggers that route’s execution and the others are not checked. If none of the expressions are true, then the default route executes.

To configure the Choice router in Studio, follow these steps:
Drag the Choice component to any point of the flow.
Click When, inside the Choice router, and configure the Expression value in the properties window to specify the condition to evaluate.
Drag message processors inside When to specify the processors to execute when the condition is met.
Drag message processors inside Default to specify the processors to execute when none of the defined conditions are met.



Following is the basic XML structure for a Choice router:
<choice doc:name="Choice">
  <when expression=${expressionToEvaluate}> 
  	<!-- Message processors --> 
  </when>
  <otherwise>
    <!-- Message processors--> 
  </otherwise>
</choice>


Adding Routes to the Choice Router
By default, the Choice router has one route option that executes when the configured DataWeave expression evaluates to true, and a default route that executes when none of the expressions in the existing routes are true. Add more routes when you need to evaluate more than one condition and then execute different operations depending on which condition is met.
。就是直接拖 其他组件 到 choice 中，会自动加一个when。 当然不能直接拖到某个when，或者default中。
。。还有，当第一个when为空的时候，第一个拖进去的logger会进入第一个when。拖第二个logger的时候，才会新建一个when，并把logger放入when中。



Add Routes by Editing the XML
In Studio, right-click the Choice router and select Go to XML…​.
Inside the <choice> element, add a <when> element:

<!--Content based routing example flow -->
<flow name="content-based-routingFlow">
  <http:listener config-ref="HTTP_Listener_config" path="/" doc:name="Listener"/>
  <set-variable variableName="language" value="#[attributes.queryParams.language]" doc:name="Set Variable" />
  <!-- Choice router block-->
  <choice doc:name="Choice" >
    <when expression="#[vars.language == 'Spanish']" >
      <set-payload value="Hola!" doc:name="Reply in Spanish" />
    </when>
    <when expression="#[vars.language == 'French']" >
      <set-payload value="Bonjour!" doc:name="Reply in French" />
    </when>
    <!-- This is the new route option added in this step -->
    <when>
    </when>
    <otherwise>
      <flow-ref name="reply-in-default-languageSub_Flow" doc:name="reply-in-default-languageSub_Flow" />
    </otherwise>
  </choice>
  <logger level="INFO" doc:name="Log the reply" message='#["The reply $(payload) means hello in $(vars.language)" ]'/>
</flow>



A Choice flow control component contains these elements:
A single root element <choice>
A <when> child element for each routing option, each with an expression to match
Optionally, an <otherwise> child element of Choice that handles all cases where none of the expressions on the <when> elements evaluate to true
Components that are conditionally executed under each of the choices are defined as child elements of the when and otherwise elements


Properties of <choice>
Business Events
	For activating the Business Events feature:
	<choice doc:name="Choice" tracking:enable-default-events="true" >


Properties of <when>
Expression (expression)
	Expression in DataWeave language to evaluate input.
	If the expression evaluates to true, this routing option is used:
	<when expression="#[vars.language == 'Spanish']" >



https://docs.mulesoft.com/mule-runtime/4.3/business-events-custom
Custom Business Event Component

Use the Custom Business Event component (tracking:custom-event element in the XML view) to add metadata and Key Performance Indicators (KPIs) to your flow. Configure the component as follows:
Click the Mule Palette tab in Anypoint Studio.
Drag Custom Business Event to any point of your flow.
Open the component properties view and specify values for Display Name and Event Name:

You can also configure Key Performance Indicators (KPIs) to capture information from the message payload:
In the UI:
	Click the plus button (+号) inside Key Performance Indicators.
	Configure Name and Expression / Value:
In the XML:
	Add a child tracking:meta-data element to the tracking:custom-event.
	Configure the key and value attributes inside tracking:meta-data:
	For example:
	<tracking:custom-event doc:name="Custom Business Event" event-name="New order">
			<tracking:meta-data key="order-id" value="#[payload.order.header.orderID]" />
	</tracking:custom-event>


For your KPIs, use names that are easy to search for in the Anypoint Runtime Manager interface, and use a representative value, which can be any Mule expression:

Name	Expression / Value
employee-id
	#[payload['ID']]
employee-email
	#[payload['Email']]
employee-git-id
	#[payload['GITHUB_ID']]
price
	700
price-after-discount
	#[vars.price]
order-id
	#[payload.order.header.orderID]

zzzzz。。。what happen.....感觉是 增加 payload里的信息，并且，这些信息 可以通过payload里已有的数据计算出来，或者直接是固定值。
。。感觉是一个 映射，就是 后续的 代码可能使用 oid 这个key来搜索，而不是 id 这个key。所以要让 id的 值 被 oid映射。
。。简配的 transform message ？，恩，transform 可以把 一个input 映射到 2个output上。



https://docs.mulesoft.com/mule-runtime/4.3/dynamic-evaluate-component-reference
Dynamic Evaluate Component
Available since 4.1

This Core component evaluates an expression that should result in another script, then evaluates that script for a final result. Such behavior allows you to dynamically select the script, instead of hardcoding it through the Transform Message Component.
。。先执行一遍，得到一个脚本，然后执行脚本，得到最终的结果。。。动态生成脚本，而不是硬编码到Transform Message组件。
。。不是上面的，结合下面的例子，，是执行一个脚本，然后对数据进行处理。

The script can use any of the usual context variables, such as message, payload,vars, or attributes, but you can also add custom ones by providing a set of key-value pairs.

The following example selects a script from a database through a userId query parameter and stores that script in a userScript variable. The dynamic-evaluate component then accesses the variable to invoke the script so it can add a name variable from a userName query parameter.

<flow name="raise-error-example-flow">
  <http:listener config-ref="HTTP_Listener_Configuration" path="/"/>
  <db:select config-ref="dbConfig" target="userScript">
    <db:sql>#["SELECT script FROM SCRIPTS WHERE ID = $(attributes.queryParams.userId)"]</db:sql>
  </db:select>
  <ee:dynamic-evaluate expression="#[vars.userScript]">
    <ee:parameters>#[{name: attributes.queryParams.userName}]</ee:parameters>
  </ee:dynamic-evaluate>
</flow>



Sending lsalander as the userId would result in a JSON response, while sending mblomkvist would result in a x-www-form-urlencoded response, essentially allowing the response type to be parameterized per user. In fact, the entire response could be parameterized to each users needs.


Dynamic Evaluate Configuration
Field	Value	Description	Example
Expression
	DataWeave expression
	Specifies the expression to be evaluated into the final script.
	expression="#[vars.generateOrderScript]"
Parameters
	DataWeave expression
	Specifies key-value pairs that should be bound to evaluate the final script.
	#[{joiner: ' and ', id: payload.user.id}]
。zzzzz



https://docs.mulesoft.com/mule-runtime/4.3/first-successful
First Successful Router

The First Successful router iterates through a list of configured processing routes until one of the routes executes successfully. If any processing route fails execution (throws an error), the router executes the next configured route.

If none of the configured routes execute successfully, the First Successful router throws an error.
。。按顺序执行，直到一个分支执行成功，如果执行失败(抛出异常)，执行下一个分支。
。如果所有的分支都失败，则抛出一个异常。



<mule xmlns:file="http://www.mulesoft.org/schema/mule/file" xmlns:sockets="http://www.mulesoft.org/schema/mule/sockets"
  xmlns:http="http://www.mulesoft.org/schema/mule/http" xmlns="http://www.mulesoft.org/schema/mule/core"
  xmlns:db="http://www.mulesoft.org/schema/mule/db"  xmlns:doc="http://www.mulesoft.org/schema/mule/documentation" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
                            http://www.mulesoft.org/schema/mule/db http://www.mulesoft.org/schema/mule/db/current/mule-db.xsd
                            http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
                            http://www.mulesoft.org/schema/mule/sockets http://www.mulesoft.org/schema/mule/sockets/current/mule-sockets.xsd
                            http://www.mulesoft.org/schema/mule/file http://www.mulesoft.org/schema/mule/file/current/mule-file.xsd">
  <http:request-config name="HTTP_Request_configuration" doc:name="HTTP Request configuration" doc:id="b026579b-5a59-444f-8f91-ff209bed8342" >
    <http:request-connection >
      <http:client-socket-properties >
        <sockets:tcp-client-socket-properties connectionTimeout="500" clientTimeout="500" />
      </http:client-socket-properties>
    </http:request-connection>
  </http:request-config>
  <http:listener-config name="HTTP_Listener_config" doc:name="HTTP Listener config" doc:id="4631e4fd-571f-41c8-831d-d908b1763ef2" >
    <http:listener-connection host="localhost" port="8081" />
  </http:listener-config>
  <flow name="testFlow" doc:id="79cd7fcd-d355-418b-898b-5d7e3a1cbcab" >
    <scheduler doc:name="Scheduler" doc:id="6dc5dcc7-ce33-4d18-9de9-2c665f713508" >
      <scheduling-strategy >
        <fixed-frequency />
      </scheduling-strategy>
    </scheduler>
    <first-successful doc:name="First Successful" doc:id="6ae009e7-ebe5-47cf-b860-db6d51a31251" >
      <route>
        <file:read doc:name="Read non existent file" doc:id="199cdb01-cb43-404e-acfd-211fe5a9167e" path="nonExistentFile"/>
        <set-variable value="1" doc:name="Set successfulRoute var to route 1" doc:id="c740b39e-a1c4-41d6-8a28-0766ca815ec6" variableName="successfulRoute"/>
      </route>
      <route>
        <set-payload value="#[vars.nonExistentVar!]" doc:name="Set Payload with non existent variable" doc:id="0cc9ac4d-5622-4e10-971c-99073cb58df0" />
        <set-variable value="2" doc:name="Set successfulRoute var to route 2" doc:id="88f15c26-d242-4b11-af49-492c35625b84" variableName="successfulRoute" />
      </route>
      <route>
        <set-variable value="3" doc:name="Set successfulRoute var to route 3" doc:id="446afb25-0181-45e5-b04a-68ecb98b57b7" variableName="successfulRoute" />
      </route>
      <route >
        <logger level="INFO" doc:name="Logger" doc:id="b94b905a-3a68-4c88-b753-464bc3d0cfeb" message="This route is never going to be executed"/>
      </route>
    </first-successful>
    <logger level="ERROR" doc:name="Logger" doc:id="9ffe328d-2595-4f28-81e8-ae731fc6cb89" message="#['Successful route was $(vars.successfulRoute)']"/>
  </flow>
</mule>



The first route executes and fails because it tries to read a file that does not exist.

The second route fails because it tries to access a variable that does not exist.

The third route executes successfully and sets variable successfulRoute with value 3.

The fourth route is not executed because the First Successful router stops executing routes after one of them completes successfully.






https://docs.mulesoft.com/mule-runtime/4.3/flow-component
Flow and Subflow Scopes

Flow and Subflow scopes are components for grouping together a sequence of other Core components and operations (provided by connectors and modules) to help automate integration processes. The Flow component is fundamental to a Mule app. Because all Mule apps must contain at least one flow, Anypoint Studio and Flow Designer automatically provide the first Flow component in your Mule app.

。把其他核心组件/操作，按顺序，包含到flow中，来帮助自动集成处理。
。flow 对于 mule应用 是非常重要的。所有的应用都至少包含一个flow。
。IDE在应用中会自动创建一个flow

A Mule app can contain additional flows and subflows

Note that flows always function synchronously. If you need to achieve asynchronous patterns, such as Fire-and-Forget(发射后不管), you can use the Async Scope (<async/>).


Flow Configuration
Flows are configurable. For example, you can set them to start or remained stopped when the Mule app starts, place limits on the allowed concurrency, or set up business events. You can also set up error handling for a component (see Error Handling).
。可配置，你能设置应用启动时 启动它们(默认)，或者不启动它们。，放置并发的限制，创建business events，，也能设置错误处理。

Field	Description
Name (name)
	Name for the flow. Flows automatically receive an editable name that matches (or partially matches) the project name.

Initial State (initialState)
	Values: Started, Stopped. These are Mule runtime settings for the flow. The default, Started (initialState="started", also called Empty in Studio), indicates that the flow is active when you start the app. So it can be triggered by an internal or external event source within the flow (such as an HTTP listener or Scheduler), by a Flow Reference from another flow or subflow, or through a call to the lookup function.
	If you set the initial state to Stopped (initialState="stopped"), you need to use Runtime Manager to activate the flow, or you can simply reset the Flow configuration to Started. Note that the console output for this state looks something like this: Flow flow_component_ex has not been started (initial state = 'stopped')

Max Concurrency (maxConcurrency)
	Optional. Sets the maximum number of concurrent messages that a flow can process. If not set, the container thread pool determines the maximum number of threads the flow can use to optimize the performance when processing messages. While the flow is processing the maximum number of concurrent messages, it cannot receive additional requests.
	Set maxConcurrency to 1 to cause the flow to process requests one at a time.
	See Back-pressure for details about Mule’s behavior after the the maximum concurrency value is reached.

Business Events
	Optional: Defaults to false. For Mule apps that you deploy to CloudHub, you can enable business events (XML example: tracking:enable-default-event="true") and add a Transaction ID (XML example: <tracking:transaction id=12345/>). See Business Events.
zzzzz。。这个在IDE中，感觉是event的跟踪功能。就是从哪里来，经过了什么处理，之类的吧，IDE是个勾选框，名字是Enable default events tracking，上面有一行描述：enabling this option will activate event tracking feature for all elements within the flow.
。。和business events，感觉没有任何关系。。

Metadata
	As with many other components, you can set metadata for this component. For more on this topic, see the Studio document Metadata Editor.



Subflow Configuration
Subflow scopes provide a way to edit the name of the subflow and to add metadata. Unlike flows, subflows lack a built-in mechanism for setting up error handling, though you can use error handling provided by the Try Scope within a subflow if you need to perform error handling.
。。subflows提供了一种发放来编辑subflow的名字，增加元数据。
。subflows缺少内置的错误处理机制

Name (name)
	Name for the subflow. Subflows automatically receive an editable name that matches (or partially matches) the project name.

Metadata
	As with many other components, you can set up metadata for this component. For more on this topic, see the Studio document Metadata Editor.

。所有组件都有metadata，都可以 add metadata..估计是作用范围吧。



XML for Flows and Subflows
This example shows the XML for a simple flow (<flow/>) that uses a Scheduler as a source to trigger execution of the flow every 10 seconds, and it connects to a subflow (<sub-flow/>) through a Flow Ref (<flow-ref/>) component. That subflow then connects to another subflow, also using a Flow Ref component. Note that when you configure components through the Studio UI, Studio automatically adds the XML to the Configuration XML for your Mule app.
。。flow-ref 是一个组件 flow reference。



Error Handling
You can add error handlers to the Flow components (but not to subflows).

。error开头的组件，一共4个(error handling, on error continue, on error propagate, raise error)，只有2个(on开头的2个)可以拖入 flow的 error handling.
。。on error xxx 有单独一章的，在后面

Near the end of this XML example, the flow shows an On Error Continue configuration (<on-error-continue/>):
  <error-handler >
    <on-error-continue enableNotifications="true" logException="true" doc:name="On Error Continue" type="EXPRESSION" when="#[isEmpty(payload.prices)]">
      <logger level="ERROR" doc:name="Logger" message='"An Error Occurred"'/>
    </on-error-continue>
  </error-handler>



https://docs.mulesoft.com/mule-runtime/4.3/flowref-about
Flow Reference Component

Flow Reference routes the Mule event to another flow or subflow, executes all processors in the referenced flow, and then routes the event back within the same Mule application.

When the main flow is triggered, the Mule event travels through and executes the flow until the event reaches the Flow Reference. The event then travels through and executes the referenced flow from beginning to end, and then returns to the main flow.

This process enables you to treat the entire referenced flow like a single component in your current flow.


Configure a Flow Reference
	Drag the Flow Reference component from Mule Palette to the point in your flow where you want to create the call to the referenced flow:
	Open the Flow Reference properties and specify the flow you want to reference in Flow name


Enrich Content with a Flow Reference Target Variable
Sometimes you need to execute a flow that uses the current payload and variables, but you want the Mule message to remain unchanged after such process. In this case, you can use a target variable to store the results of the referenced flow processing without changing the original message.
You can store the result of a Flow Reference call in a target variable by configuring the following fields in your Flow Reference properties:
	Target: Name of the variable that the Flow Reference creates to store the result of the processed message.
	Target Value: An expression that evaluates against the operation’s output. The result of this evaluation is stored in the defined target variable.


Considerations when Using a Target Variable
Setting the *target variable in a Flow Reference component causes the original message to remain unchanged. This means that any modification to the payload or to the variables that occur in the referenced flow revert after the referenced flow finishes executing its processors, returning the payload and variables to their original values.

Alternatively, if you do not define a target variable, any modification that occurs to the payload or to the variables in the referenced flow persist after the referenced flow finishes its execution, changing the original value of the payload or variables.
。。。设置了target，那么payload里的值，在进入和退出时的 值不会变。
。。如果没有设置target，则payload里的值，会被ref的flow 修改。
zzzzz。这个target有没有什么讲究？直接一个a？后续有可能通过这个a来获得什么东西吗？
。下面有，可以的。


XML Example of Flow Reference
The following XML code represents the project created in Configure a Flow Reference, adding the target variable flowReferenceVar, which stores the result of the secondary flow called by the Flow Reference component. This variable is then used in the main flow by logging vars.flowReferenceVar:
<!-- Main flow -->
<flow name="mainFlow" >
  <http:listener doc:name="Listener" config-ref="HTTP_Listener_config" path="test"/>
  <set-payload value="Original payload" doc:name="Set Payload"  />
  <set-variable value="1" doc:name="Set myVar=1" variableName="myVar"/>
  <!-- Flow Reference component with target variable flowReferenceVar -->
  <flow-ref doc:name="secondaryFlow" name="secondaryFlow" target="flowReferenceVar"/>
  <logger level="INFO" doc:name="Log payload" message='#["Payload = " ++ payload]'/>
  <logger level="INFO" doc:name="Log myVar" message='#["myVar = " ++ vars.myVar]'/>
  <logger level="INFO" doc:name="Log flowReferenceVar" message='#["flowReferenceVar = " ++ vars.flowReferenceVar]'/>
</flow>
<!-- Secondary flow that is referenced from Main flow-->
<flow name="secondaryFlow" doc:id="044ece19-aa71-4fc9-ad34-8df655dd59a8" >
  <set-payload value="Modified payload" doc:name="Update Payload" />
  <set-variable value="2" doc:name="Update myVar=2" variableName="myVar"/>
</flow>

The following log represents the output of the example application after its execution:
INFO  LoggerMessageProcessor: Payload = Original payload
INFO  LoggerMessageProcessor: myVar =  1
INFO  LoggerMessageProcessor: flowReferenceVar =  Modified payload

If you remove target="flowReferenceVar" from the Flow Reference component, the application returns the following output:
INFO  LoggerMessageProcessor: Payload = Modified payload
INFO  LoggerMessageProcessor: myVar = 2
INFO  LoggerMessageProcessor: flowReferenceVar = null

The logs are shortened to improve readability.

。。恩，很明显，，但是 vars 又是什么。。




https://docs.mulesoft.com/mule-runtime/4.3/for-each-scope-concept
For Each Scope

The For Each scope splits a payload into elements and processes them one by one through the components that you place in the scope. It is similar to a for-each/for loop code block in most programming languages and can process any collection, including lists and arrays. The collection can be any supported content type, such as application/json, application/java, or application/xml

General considerations about the For Each scope:
。一般的深思。。

	By default, For Each tries to split the payload. If the payload is a simple Java collection, the For Each scope can split it without any configuration. The payload inside the For Each scope is each of the split elements. Attributes within the original message are ignored because they are related to the entire message.
。。forEach尝试分割payload，如果是一个java集合，那么不需要任何配置就可以切分。foeRach范围内的payload是切分后的每个元素。。原始message中的属性被无视了，因为这些属性关联的是整个message。
	For Each does not modify the current payload. The output payload is the same as the input.
。。forEach不会修改payload，返回的payload和输入的payload一样。
	For non-Java collections, such as XML or JSON, use a DateWeave expression to split data. Use the Collection field for this purpose.
。。对那些非java集合的，如xml，json，使用DW表达式来切分数据。使用forEach的Collection属性来完成这个目标。


The For Each scope stores each item of the collection in payload during each iteration.
。forEach范围内，payload保存的是元素。

You can also split an array into batches to enable quicker processing. Each batch is treated as a separate Mule message. For example, if a collection has 200 elements and you set Batch Size to 50, the For Each scope iteratively processes 4 batches of 50 elements, each as a separate Mule message.
zzzzz。。也能切分成batch的块。每个块被当做一个独立的mule message。。那这样，forEach里面的payload，保存的是 块(mule message) 吧。


Example XML
  <foreach doc:name="For Each" collection="#[payload.topics]" batchSize="1" rootMessageVariableName="rootMessage" counterVariableName="counter">
      <file:write ... >
      <!--Any other module that you want to include in the For Each scope -->
  </foreach>



Variable Propagation
Every execution of the For Each scope starts with the variables and values from the previous execution of the block. New variables or modifications to existing variables that take place when processing one element are visible during the processing of another element. These changes to variables continue to be available outside the For Each scope.
。。每次forEach的执行，开始于 上次执行后返回的 变量和值。        第一次呢？？
zzzzz。new变量 或 修改变量 发生在  当处理一个元素 这个过程 对其他 元素的处理过程 中可见时。
。这些 变量的修改 在forEach外 也可见。

<set-variable variableName="var1" value="var1"/>
<set-variable variableName="var2" value="var2"/>
<foreach collection="#[['apple', 'banana', 'orange']]">
    <choice>
        <when expression="#[payload == 'apple']">
            <set-variable variableName="var2" value="newValue"/>
            <set-variable variableName="var3" value="appleVal"/>
        </when>
        <when expression="#[payload == 'banana']">
            <set-variable variableName="var3" value="#[vars.var3 ++ ' bananaVal']"/>
            <!-- var3 will now have value 'appleVal bananaVal'-->
        </when>
        <otherwise>
            <set-variable variableName="var3" value="otherVal"/>
            <set-variable variableName="var4" value="val4"/>
        </otherwise>
    </choice>
</foreach>

After aggregation, the variables are:
{var1: "var1", var2: "newValue", var3: "otherVal", var4: "val4"}



Error Handling
If one of the elements in a collection throws an exception, the For Each scope stops processing that collection and invokes the error handler.
。。一个元素处理时抛出异常，整个forEach停止，然后调用错误处理。
zzzzz。。这个怎么设置的，，我拖error handling 拖不进去。。 forEach本身也没有错误处理啊。


XML Reference
For Each scopes open and close with a <foreach> tag. Components that are affected by this scope are defined as child elements of the <foreach> tag.


Configurable Properties
属性   默认值   描述
collection
	payload
		An expression that returns a Java collection, object array, map, or DOM nodes.

counterVariableName
	counter
		Name of the property that stores the number of messages over which it iterates.

batchSize
	1
		Partitions the collection into sub-collections of the specified size. For example, if a collection has 200 elements and you set the batch size to 50, it processes 4 batches of 50 elements each.

rootMessageVariableName
	rootMessage
		Name of the property that stores the parent message. The parent is the complete, non-split message.


Differences between For Each and Parallel For Each Scopes
Both For Each and Parallel For Each split the defined collection, and the components within the scope process each element in the collection. Also, in both cases, each route runs with the same initial context. The difference between these two scopes are:

	For Each works sequentially, while the Parallel For Each processes in parallel. This difference affects error handling:
		Because of the processing differences, the execution of For Each execution is interrupted when an error is raised (and the Error Handler is invoked), while Parallel For Each processes every route before invoking the Error Handler with a MULE:COMPOSITE_ROUTE error type.

	For Each does not modify the payload, while the Parallel For Each outputs a collection of the output messages from each iteration.

。forEach线性执行，parallel 并发执行，这个差异影响了错误处理：forEach当错误发生时，直接中断处理，，，parallel处理每个 路径 在 通过MULE:COMPOSITE_ROUTE错误类型 调用 错误处理之前。。。。zzzzz
。。forEach不修改payload，parallel输出一个处理后的集合。




https://docs.mulesoft.com/mule-runtime/4.3/idempotent-message-validator
Idempotent Message Validator
。。幂等的

The Idempotent Message Validator ensures that only unique messages continue through a flow’s execution by checking the unique ID of the incoming message. You can use any incoming attribute of the Mule message as an ID, or you can compute the ID by configuring a DataWeave expression. You can also use the DataWeave *Crypto functions to compute hashes (SHA, MD5) from the data.


Message Hashing

DataWeave enables you to import the *Crypto library to execute hashing functions. Calculating a hash can be useful when you want to compute a unique ID for the Mule message. For example, if you want to get an MD5 hash from the payload and use it as the ID, open the Idempotent Message Validator properties in Anypoint Studio and set ID Expression (idExpression in XML) to the following DataWeave expression:
#[
    %dw 2.0
    output text/plain
    import dw::Crypto
    ---
    Crypto::hashWith(payload,'MD5')
]

zzzzz。。这个怎么，在哪里 设置。。。。靠，Idempotent Message Validator 是一个组件
。。这个组件的Id Expression, 点下fx， 然后复制上面的，这个框会变大的。。
。。但是这个是在listener 之后的吧，连续来2个请求呢，，，hashCode一般不变的，所以能剔除一个。


Examples
The following example shows an Idempotent Message Validator configured to extract the URL parameter id from the HTTP request and use that value as a unique identifier to filter the message:
<flow name="myFlow">
  <http:listener doc:name="Listener" config-ref="HTTP_Listener_config" path="/"/>
  ...
  <idempotent-message-validator doc:name="Idempotent Message Validator" idExpression="#[attributes.queryParams.id]"/>
  ...
</flow>

The next example shows an Idempotent Message Validator configured to hash the payload using a DataWeave expression, store the result in a persistent object store, and then use the processed message IDs as the unique identifier to filter the message:
<flow name="myFlow">
  <http:listener doc:name="Listener" config-ref="HTTP_Listener_config" path="/"/>
  ...
  <idempotent-message-validator doc:name="Idempotent Message Validator" idExpression="
    #[%dw 2.0
      output text/plain
      import dw::Crypto
      ---
      Crypto::hashWith(payload,'MD5')]">
    <os:private-object-store alias="privateObjectStore"
       entryTtl="1"
       entryTtlUnit="MINUTES"
       maxEntries="10" />
  </idempotent-message-validator>
  ...
</flow>

zzzzz。。这个怎么起效，只要listener后配一个，有重复的就抛出异常？。。怎么配置，


Parameter Reference
Name	Type	Description	Default Value	Required
ID Expression
	Expression
		The expression Mule uses to generate the ID. You can use a DataWeave function to calculate the ID, or you can extract the ID from any existing value of the Mule message.
			#[correlationId]
				No

Value Expression
	Expression
		This parameter is not used. Setting a value has no effect.
			#[correlationId]
				No

Object Store
	Object Store
		Either a name to reference a global object store or a definition of a private object store where the component stores the processed message IDs.
			Object Store created by the Object Store Manager. Not persistent, with an entry TTL of 5 MINUTES and an expiration interval of 6 SECONDS.
				No

Store Prefix
	String
		Defines the prefix of the object store names. This value is used only for the internally built object store.
			configFileName.flowName.IdempotentMessageValidator
				No
..


Throws
MULE:DUPLICATE_MESSAGE



https://docs.mulesoft.com/mule-runtime/4.3/logger-component-reference
Logger Component

This Core component helps you monitor and debug your Mule application by logging important information such as error messages, status notifications, payloads, and so on. You can add a Logger anywhere in a flow, and you can configure it to log a string that you specify, the output of a DataWeave expression you write, or any combination of strings and expressions.

Keep in mind that the Logger is one of the only components that supports mixing Strings and expressions within a value. DataWeave String interpolation or concatenation within a single expression should be used elsewhere.

The configured messages are logged to the app’s log file, which is located in MULE_HOME/logs/<app-name>.log if no custom log file path is specified in the log4j2.xml file.


Logger Configuration
Field	Value	Description	Example
Message
	String or DataWeave expression
		Specifies the Mule log message. By default, messages are logged to the application’s log file.
			message="Current payload is #[payload]"

Level
	Available options:
	DEBUG
	ERROR
	INFO(Default)
	TRACE
	WARN
		Specifies the log level.
			level="ERROR"

Category
	String
		Optional setting that specifies a category name that it adds to log4j2.xml file. For example, you might use a category to route your log messages based on your category, or you might set log levels based on category.
			category="MyCustomCategory"


Examples
This Logger is set to monitor message processing status, mixing Strings and expressions:
	<logger category="monitoring" message="Message #[payload.id] processed successfully" level="INFO"/>

This Logger set to record the processing time of a flow, using a single expression and DataWeave’s String concatenation:
	 <logger category="performance" message="#['Message ' ++ payload.id ++ ' took ' ++ vars.processingTime ++ ' milliseconds to process']" level="INFO"/>



https://docs.mulesoft.com/mule-runtime/4.3/on-error-scope-concept
On-Error Error Handlers

When an error occurs in a Mule app, an Error Handler component routes the error to the first On-Error component (On Error Continue or On Error Propagate) configuration that matches the Mule error, such as HTTP:NOT_FOUND, DB:CONNECTIVITY, or ANY (the default for all Mule errors). If no error handling is configured for the error, the app follows a default error handling process.

On-Error components differ in the way they affect their owner, that is, the Flow or Try scope where they are defined.
。。flow，try 中 on-error还会有不同表现。。好像说的是 continue，和 propagate 有不同?？

On Error Propagate
Executes but propagates the error to a higher level, such as a containing scope (for example, to a Flow that contains a Try scope where the error occurs) or external flow containing a Flow Reference to the flow in which an error occurs. The error breaks the owner’s execution and propagates to that higher level, causing a failure at that level. In addition, any transaction the owner handles is rolled back. However, note that the transaction is not rolled back if another component (one that does not own On Error Propagate) created the transaction.
。。catch到，处理，然后再throw出去。事务回滚
。 However, note that the transaction is not rolled back if another component (one that does not own On Error Propagate) created the transaction.
zzzzz。。another component是什么？在哪里的？是指try里？还是说 调用者？还是说try ref的那个？

On Error Continue
Executes and uses the result of the execution as the result of its owner, as if the owner completed the execution successfully. Any transaction the owner handles is committed. However, note that the transaction is not committed if another component (one that does not own On Error Propagate) created the transaction.
。。catch到，处理，不throw出去。事务commit
。。However, note that the transaction is not committed if another component (one that does not own On Error Propagate) created the transaction.
zzzzz。another，own on error propagate。。。这里也需要propagate。。

Error Matching
To help you identify and handle potential errors when designing a Mule app, On-Error components list the errors that the runtime engine and operations configured in the app can throw. You can select the errors you want to handle.

Matching: On-Error components can perform matching based on error types you select. The list of selectable error types depends on the module and connector operations within the flow or scope to which the On-Error component applies. It also lists EXPRESSION and STREAM_MAXIMUM_SIZE_EXCEEDED errors, which the runtime can throw.
。。on-error 有一个 type 属性，这个能选 哪些错误，不过这里展现的错误 是 flow 里可能抛出的错误(估计这个就是方法上的throw xxxException)，和 运行时可能抛出的错误.
zzzzz。。那 那些没有在 方法声明时 列举的错误 怎么 catch？

Matching expression (for more advanced use cases): You can perform error matching based on a when condition that you define in the *When* field of an On-Error component. For example, you might map the component to fatal errors, which have error messages that contain the word "fatal":

When field configuration in the UI: error.cause.message contains "fatal"
XML configuration example:
<on-error-continue
  enableNotifications="true" logException="true"
  type="ANY"
  when='error.cause.message contains "fatal"'/>

In the example, every error that matches the when expression is handled by on-error-continue. You can also add a restrictive type such as type="HTTP:CONNECTIVITY" so that the error handler handles only the errors that match the specified type and condition.
。。how。。这里说的是 when 里加 type="HTTP:CONNECTIVITY"吧？ 但是怎么加，和上面的fatal 是 &&，and，;，连接？如果 是或的关系呢？。。"或" 可以 多写几个on error。。。

Note that matching conditions are evaluated sequentially, in the order in which the On-Error components reside in the error handler. For example, if you want to handle VALIDATION:NOT_NULL in one way but handle all other errors a different way, provide an On-Error component configuration for that error before a component that captures the remaining errors (identified as ANY by the second On-Error component). Remember that default error handling takes place for errors you do not explicitly match or use ANY to capture.
。从上往下，有一个能进入了，就不会继续往下走了。


Configuration and Use
Because an Error Handler component is built into Flow and Try components, you can use On-Error components for errors that occur within the scope of a flow or within a Try component. For examples, see Using On-Error Components to Handle Messaging Errors.
。flow，try 内置了 error handling

You can also use On-Error components in a separate Error Handler configuration that is referenced from a Flow component or Try scope (see Referencing a Global Error Handler).
。。error handling 能做为一个独立的错误处理。。。。这个会处理所有flow的报错？

Drag the Error Handler component from the Mule Palette to the Studio canvas, and name it allErrorHandler.
Notice that the component is not part of any flow.

Drag the On Error Continue component into the Error Handler component.

Drag a Set Payload component into the On Error Continue component.
Note that the Set Payload component is an example. You can use other components, such as a Logger, and you can write your own error message inside that component. The Set Payload example uses a Mule variable for the error description (see the error variable in Predefined Variables for details).

Create a global error-handling reference in Studio:

Click Global Elements to open Global Configuration Elements.
Global Elements is located below the Studio canvas.
In Global Configuration Elements, click Create to open the Choose Global Type dialog.

From the dialog, select Global Configuration -→ Configuration, and then click OK to open the Configuration dialog.

From the select Configuration dialog, select allErrorHandler for the Default Error Handler field, and click OK.

Set up the HTTP listener and HTTP request components using the values described in the XML.

Check that the XML configuration looks correct by clicking Configuration XML (located below the Studio canvas).

zzzzz。。IDE只拉一个全局error handling, 是不会 catch异常的，需要 global elements里 create - global configuration - configuration，里配置， 配置完以后， xml里才会有：
	<configuration doc:name="Configuration" doc:id="35a41eb8-31ed-4fdb-9f12-3ebffb2af25e" defaultErrorHandler-ref="muledoc2Error_Handler" >
		<cluster:cluster-config >
			<cluster:performance-store-profile />
		</cluster:cluster-config>
	</configuration>


Referencing an Error-Handling Configuration from a Flow
The configuration of some elements in these examples is XML-only.

A flow can reference a global error handler that resides outside the flow. The flow logs all its errors through a reference.
。。flow可以引用 flow之外的 全局error handler.

To reference a global error handler configuration from a flow in Studio:
	From Studio, drag an Error Handler component from the Mule Palette to the canvas, and configure it components through the UI.

	In the Mule app, click Configuration XML (located below the Studio canvas).

	(XML-only configuration) Within your <flow/> element, type the XML for the reference, for example, <error-handler ref="loggingErrorHandler"/>.
。。最后一步可以 global element 设置啊。。不，这里是新建一个全局error handler..。。。。。。。。但是这里应该是 ref 一个全局错误处理啊， 第三步的 error-handler ref...ok,, 确实是 ref 全局错误处理。 第三步不是新建。。
下面是2个配置，
第一个是 global elements 新建一个配置，来设置 全局错误处理。
第二个是 本次的，直接在xml里写。
	<configuration doc:name="Configuration" doc:id="1b60425a-25e7-4f85-9c24-674595749bf0" defaultErrorHandler-ref="muledoc2Error_Handler" >
		<cluster:cluster-config >
			<cluster:performance-store-profile />
		</cluster:cluster-config>
	</configuration>

	<error-handler ref="loggingErrorHandler"/>
zzzzz。这2个都不报错。。有什么区别？。。第二个可以ref其他xml里的？ 而且第二个修改名字不会报错，第一个 修改ref后的名字 就会报错。
。。上面错了，，它说了 要 flow 里 写 error-handler ref。。。
。。在flow外面的话，会变成 一个 error handling 组件。。flow里面的话， IDE的GUI里看不出来的。


The following example references a global On Error Continue element (<on-error-continue/>) using <on-error ref="loggingErrorHandler"/> within an <error-handler/> element. Both require manual configuration through the Configuration XML, rather than the UI.

<http:listener-config name="HTTP_Listener_config" doc:name="HTTP Listener config" >
	<http:listener-connection host="0.0.0.0" port="8081" />
</http:listener-config>
<on-error-continue name="loggingErrorHandler">
    <logger message="#['Error: ' ++ error.description]" level="INFO"/>
</on-error-continue>

<flow name="withSharedHandler">
  <http:listener doc:name="Listener" config-ref="HTTP_Listener_config" path="/"/>
  <http:request url="http://jsonplaceholder.typicode.com/badrequestbad" method="GET"/>
  <error-handler >
    <on-error ref="loggingErrorHandler"/>
  </error-handler>
</flow>

。。on-error-continue 是无法 放在 组件外的，，就是 它不是一个 独立的组件，必须在 error-handling中， 所以 GUI无法拖出 上面这种效果的。
。。靠，上面是 https://docs.mulesoft.com/mule-runtime/4.3/error-handling#global_error_handler  的 东西。。


Error Handling within a Flow


Reuse of On-Error Scopes
Just as you can share error handlers by exposing them globally and referencing them in flows and try scopes, you can also reuse On-Error components. You can define and name an On-Error component globally and then reference from an Error Handler component.

Example: XML Configuration for Referenced On-Error
<on-error-propagate name="sharedPropagateHandler">
  <logger message="An unhandled error has occurred."/>
</on-error-propagate>

<on-error-continue type="EXPRESSION" name="sharedContinueHandler">
  <logger message="An expression failed to evaluate."/>
</on-error-continue>

<error-handler name="reusingHandlers">
  <on-error ref="sharedContinueHandler"/>
  <on-error-continue type="ROUTING">
    <logger message="An expression failed to evaluate."/>
  </on-error-continue>
  <on-error ref="sharedPropagateHandler"/>
</error-handler>






https://docs.mulesoft.com/mule-runtime/4.3/parallel-foreach-scope
Parallel For Each Scope

The Parallel For Each scope enables you to process a collection of messages by splitting the collection into parts that are simultaneously processed in separate routes within the scope of any limitation configured for concurrent-processing. After all messages are processed, the results are aggregated following the same order they were in before the split, and then the flow continues.
。。分割集合，同时处理各个部分(会有各种限制)。处理完，结果聚集，且元素顺序不变。


Configuration
The Parallel For Each scope can be configured through the following fields:

Child element	Description
Collection (collection)
	Specifies the expression that defines the collection of parts to be processed in parallel. By default, it uses the incoming payload.
。。用这个来 划分 集合？。。。不，看例子， 这个是  定义 并行forEach要处理的集合。。基本没用。因为默认payload。。

Attribute	Description
Collection Expression (collection)
	An expression that returns a collection. By default, the payload is taken as the collection to split.
。。一个返回集合的表达式，默认是 payload。

Timeout (timeout)
	Specifies the timeout for each parallel route. By default, there is no timeout.
。。定义了每步处理 的超时。

Max Concurrency (maxConcurrency)
	Specifies the maximum level of parallelism for the router to use. By default, all routes run in parallel.
。。多少线程，默认，全并行(有多少个part，就有多少个线程)。。

Target Variable (target)
	Specifies a variable to use for storing the processed payload. By default, the output is saved in the flow’s payload.
。。处理后的payload 的 key？ 。。 保存在 哪个变量 上。

Target Value (targetValue)
	Specifies an expression to evaluate against the operation’s output value. The outcome of this expression is stored in the target variable. By default, this is the same as the operation’s output value.
。。evaluate against：根据。。
。根据计算结果，来进行再计算，这个输出保存在 上面的target variable。 默认情况下，输出 就是 计算的结果。
。。。
zzzzz。。它怎么划分的？


Example
This XML example adds to every element in the collection the string "-result":
<flow name="myFlow">
  <parallel-foreach collection="#[['apple', 'banana', 'orange']]">
      <set-payload value="#[payload ++ '-result']"/>
  </parallel-foreach>
</flow>

Every execution of the Parallel For Each scope starts with the same variables and values as before the execution of the block.
。。每次执行的变量和值，和整个块执行前一样。。。简单点：所有part全部并行，那么它们的变量和值，肯定都是最初的。

New variables or modifications of already existing variables while processing one element are not visible while processing another element.
。。处理一个元素时的 新建变量，修改变量，对另一个元素的执行 并不可见。

All of those variable changes are not available outside the Parallel For Each scope, the set of variables (and their values) after the execution of the Parallel For Each Scope remains the same as before the execution.
。这些变量的 改变(修改+new)，在parallel forEach 外不可见。，变量的集合和它们的值，在执行parallel前 和 后 是一样的。

Consider the following example:

<flow name="myFlow">
  <set-variable variableName="var1" value="var1"/>
  <set-variable variableName="var2" value="var2"/>
  <parallel-foreach collection="#[['apple', 'banana', 'orange']]">
      <choice>
          <when expression="#[payload == 'apple']">
              <set-variable variableName="var2" value="newValue"/>
              <set-variable variableName="var3" value="appleVal"/>
          </when>
          <when expression="#[payload == 'banana']">
              <set-variable variableName="var3" value="bananaVal"/>
          </when>
          <otherwise>
              <set-variable variableName="var3" value="otherVal"/>
              <set-variable variableName="var4" value="val4"/>
          </otherwise>
      </choice>
  </parallel-foreach>
</flow>

After aggregation, the variables are:
{var1: "var1", var2: "var2"}
。。不会修改，不会new


Error Handling

Because every route is processed in parallel, if an error is thrown in one route, processing continues in all of the other routes until all finish processing. After that, all results (and any errors) are aggregated and then processed by the Error Handler, as shown here:
。。多路并行。，如果其中一路报错，依然会等其他路都执行完，然后所有结果/异常 聚集，然后被 错误处理 处理。

<flow name="myFlow">
  <parallel-foreach collection="#['banana', 'apple']">
      <choice>
          <when expression="#[payload == 'banana']">
              <!-- Processor that throws error -->
          </when>
          <otherwise>
              <set-payload value="#[payload ++ '-result']"/>
          </otherwise>
      </choice>
  </parallel-foreach>
  <error-handler>
      <on-error-continue type="COMPOSITE_ROUTING">
          <!-- This will have the error thrown by the above processor -->
          <logger message="#[error.errorMessage.payload.failures['0']]"/>
          <!-- This will be a null value -->
          <logger message="#[error.errorMessage.payload.failures['1']]"/>
          <!-- This will be a null value -->
          <logger message="#[error.errorMessage.payload.results['0']]"/>
          <!-- This will have the result of this (correctly executed) route -->
          <logger message="#[error.errorMessage.payload.results['1']]"/>
      </on-error-continue>
  </error-handler>
</flow>

zzzzz。。不知道，如果是一共10个part，2线程的线程池，执行，某个线程报错后，是依然会执行完所有的10个part，还是说 现有线程执行完 就不再继续执行(线程池不会再添加线程进去)..下面有，processes every route



Throws
MULE:COMPOSITE_ROUTING


Differences between For Each and Parallel For Each Scopes
Both For Each and Parallel For Each split the defined collection, and the components within the scope process each element in the collection. Also, in both cases, each route runs with the same initial context. The difference between these two scopes are:
	For Each works sequentially, while the Parallel For Each processes in parallel. This difference affects error handling:
		Because of the processing differences, the execution of For Each execution is interrupted when an error is raised (and the Error Handler is invoked), while Parallel For Each processes every route before invoking the Error Handler with a MULE:COMPOSITE_ROUTE error type.

	For Each does not modify the payload, while the Parallel For Each outputs a collection of the output messages from each iteration.
。forEach不修改payload，，parallel输出一个集合。(。到哪里？覆盖payload？)




https://docs.mulesoft.com/mule-runtime/4.3/parse-template-reference
Parse Template Reference

Parse Template is the Mule component to use for processing a template and obtaining a result. A template is defined as text with embedded Mule expressions that are evaluated and replaced with their result.
。。Parse Template 是mule组件，用来处理 一个模板，然后得到一个结果。
zzzzz。。。模板是 内置mule表达式(这个表达式已经被评估和替换为它们的值)的 文本，


Configuring a Parse Template
In Studio, drag a Parse Template message processor from the palette onto the canvas.

Configure the fields described in this table:
Field	Value	Description

Content
	Template String
		A string to use as a template. Instead of defining the template in an external file, you can use this field to write it inline. It can contain embedded expressions to be evaluated and replaced with their results.
。。string，用作 模板，代替 定义在外部文件 中的 模板。

Display Name
	Parse Template
		Customize to display a unique name for the transformer in your application.
。自定义 这个组件 在应用中的 唯一命名。

Location
	filepath
		Define the location of the file that Mule uses as a template into which to insert values extracted from the message properties or variables.
。外部文件的地址，这个文件被用作一个模板。。。后面的是什么意思。。而且这个文件 是什么类型？txt/string？json？xml？

Target Variable
	Variable Name
		The name of a variable that stores the result of evaluating the expression defined in Target Value.
。变量名，保存结果。

Target Value
	Expression
		Mule Expression to be evaluated after executing Parse Template. The result of this expression is stored in a variable with the name defined in the Target Variable field.
。mule表达式，用来评估，，，但是这里又说 after executing Parse template。。。
zzzzz。。感觉 template 翻错了，  好像是 数据源的意思。。感觉好像是，文件保存了 一些数据(包含占位符)，然后parse 读取到内存，顺便把占位符解码，然后执行mule表达式，得到结果。。


If you are using the XML editor in Studio or a Standalone Mule instance:
Add a parse-template element to your flow, then configure it according to the tables below.
	<parse-template location="users/administrator/desktop/hrweb/confirmation.html" doc:name="Parse Template"/>

Attributes of the parse-template element:
Element Attributes	Value
content
	A string representing the template to be used where the embedded expressions will be evaluated and replaced by their results.

location
	Filepath which defines the location of the file that Mule uses as a template into which to insert values extracted from the message properties or variables.

doc:name
	Customize to display a unique name for the transformer in your application. (Note: Not needed in Mule standalone.)

target
	The name of a variable where the result of the expression defined in targetValue will be stored after the Parse Template is executed.

targetValue
	A Mule Expression that will be evaluated after the Parse Template is executed and which result will be stored in a variable with name as defined in the target attribute.


Code Example
The following example demonstrates the use of a Parse Template in an application that accepts queries by employeeID to acquire data about an employee.

In this case, Parse Template uses a file external to the flow as a template into which it inserts values for fields (name, department, job title, start date, and employee type) drawn from the message payload. The flow then returns the template-built payload to the caller.
。。文件里是模板，然后用payload里的数据来 填充模板，然后返回。
。。渲染。。。ojbk，view 和 model 的关系。


<html>
	<body>
		<table>
		<tr>
			<th>First Name</th>
			<th>Last Name</th>
			<th>Department</th>
			<th>Job Title</th>
			<th>Start Date</th>
			<th>Employee Type</th>
		</tr>
		<tr>
			<td>#[payload[0]['first_name']]</td>
			<td>#[payload[0]['last_name']]</td>
			<td>#[payload[0]['department']]</td>
			<td>#[payload[0]['job_title']]</td>
			<td>#[payload[0]['start_date']]</td>
			<td>#[payload[0]['employee_type']]</td>
		</tr>
		</table>
	</body>
</html>


<?xml version="1.0" encoding="UTF-8"?>
<mule xmlns:ee="http://www.mulesoft.org/schema/mule/ee/core" xmlns:http="http://www.mulesoft.org/schema/mule/http"
      xmlns:db="http://www.mulesoft.org/schema/mule/db"
      xmlns="http://www.mulesoft.org/schema/mule/core" xmlns:doc="http://www.mulesoft.org/schema/mule/documentation" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
http://www.mulesoft.org/schema/mule/db http://www.mulesoft.org/schema/mule/db/current/mule-db.xsd
http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
http://www.mulesoft.org/schema/mule/ee/core http://www.mulesoft.org/schema/mule/ee/core/current/mule-ee.xsd">
    <db:config name="Database_Config" doc:name="Database Config" doc:id="e4df87d8-ec1e-49ec-a528-43e93baa5bda" >
        <db:my-sql-connection host="localhost" port="3306" user="user" password="pw" database="MySQL_Data_Source" />
    </db:config>
    <http:listener-config name="HTTP_Listener_config" doc:name="HTTP Listener config" doc:id="724690b1-cc2f-451a-86eb-66f7750feba1">
        <http:listener-connection host="localhost" port="8081" />
    </http:listener-config>
    <flow name="exampleTemplateFlow1" doc:id="b8cc9464-5991-4977-978f-9f51eb252ec8" >
        <http:listener doc:name="Listener" doc:id="f334ba27-ae1f-4da5-a194-26e57a1aa637" config-ref="HTTP_Listener_config" path="/getEmployee"/>
        <db:select doc:name="Select" doc:id="c3ab7830-9920-4637-9329-954a8a94e54c" config-ref="Database_Config">
            <ee:repeatable-file-store-iterable />
            <db:sql >SELECT * FROM Employees WHERE id=#[attributes.queryParams['id']]</db:sql>
        </db:select>
        <parse-template doc:name="Parse Template" doc:id="53211517-f943-40b2-86e3-20a0e1ca4eb6" location="src/main/resources/responseHtml.template"/>
    </flow>
</mule>

To use the application in this example, you must send an HTTP request with a URL that includes an ID query parameter, such as http://localhost:8081/getEmployee?id=12345
。。本地哪有这个数据库。。



Special Characters
Parse Template supports the use of expressions within a template and the use of literals within these expressions. The template recognizes certain special characters, such as the sequence \#[ and quotations marks. You can escape such characters to treat them as regular characters.

#[an-expression]:
	Within a template, use the characters #[] to designate an expression. For example, a template can contain expressions within HTML elements, such as <td>#[upper("mulesoft")]</td>, which resolves to <td>MULESOFT</td>. Empty expressions (#[]) return an error, but empty strings, such as #[''] or #[""] are valid expressions.
。说明里面是一个表达式，估计是 js 语言？

Sub-expressions:
	To resolve a sub-expression that is surrounded by quotation marks, surround the sub-expression with another #[]. For example, the four elements in the template snippet below contain the same sub-expression (upper("world")), either inside or outside of quotation marks. The first two return the same results. The last two do not process upper("world") because the sub-expression is inside the quotation marks but not surrounded by another #[].
。嵌套的规则。
下面的1.在stirng中，但是直接被#[]包围了，所以 被执行了
2.不在string中，在#[]中(但没有被直接包围)，执行
3.在string中，没有#[]，不执行
4.在string中，，不执行。
。。感觉，在#[]中，但不在string中，会直接执行。
在#[]中，在stirng中，。。 不
。就是，，看最里面的 外面一层，是 引号，还是#[]，(这里的外面一层不需要紧贴。)，如果外面一层是#[]，就认为是一段代码。如果是 引号，则认为是string。

	Parse Template Snippet:
	<td>#['hello #[upper("world")]']</td>
	<td>#['hello ' ++ upper("world")]</td>
	<td>#['hello upper("world")']</td>
	<td>#['hello ++ upper("world")']</td>
	Output Values:
	<td>hello WORLD</td>
	<td>hello WORLD</td>
	<td>hello upper("world")</td>
	<td>hello ++ upper("world")</td>

。。escape character， 转义字符。。。。
Escape character (\):
	Parse Template uses the character sequence #[ to identify where an expression begins. To avoid this behavior and treat that sequence as literal characters, escape it with \. For example, *<td>#[</td>* returns *<td>#[</td>*.

	In addition, expressions can contain strings with special characters that you want to treat as regular characters. To escape any special characters within a string that is embedded inside an expression, append \ to the character. Examples of special characters include the sequence #[, quotations marks (' or "), apostrophes ('), and $. It is not necessary to escape \# or [ unless they are adjacent to one another in the string, with the \# preceding the [.
。\#,\[,没有必要，只有\#[   \'   \"     \$
..$ 是什么 作用？，好像是 js的 取 对应的值？

	Parse Template Snippet:
	<td>\#[</td>
	<td>#['abcd\#[-1234' ++ now() as String ++ '.log']</td>
	<td>'abc'def'</td>
	<td>#['abc\'def']</td>
	<td>"xyz"xyz"</td>
	<td>#["xyz\"xyz"]</td>
	<td>#["abc\$DEF\#ghi\[JKL]"]</td>

	Output Values:
	<td>#[</td>
	<td>abcd#[-12342020-07-06T17:20:10.683-07:00.log</td>
	<td>'abc'def'</td>
	<td>abc'def</td>
	<td>"xyz"xyz"</td>
	<td>xyz"xyz</td>
	<td>abc$DEF#ghi[JKL]</td>
	For more information, see Escape Special Characters.





https://docs.mulesoft.com/mule-runtime/4.3/raise-error-component-reference
Raise Error Component

This core component generates a Mule error, as if a failure had occurred, which allows you to customize its description and type.

Use this component to only raise:
Core runtime errors, such as MULE:SECURITY, MULE:CONNECTIVITY, etc.
Custom error types.

You cannot raise an error from another connector or module like FILE:FILE_NOT_FOUND, JSON:INVALID_SCHEMA. You cannot use a connector’s existing namespace.



Raise Error Configuration

Field	Value	Description	Example
Description
	String or DataWeave expression
		Specifies the error description.
			description="Error description message."

Type
	String
		Specifies the type of the error.
		type="CUSTOM:CUSTOM_ERROR"


Raise Core Runtime Error Types
For core runtime error types, you must use the implicit namespace and identifier, you can only customize the error’s description message. For example:
	<raise-error type="MULE:CONNECTIVITY" description="Error description message"/>
。implicit  无疑问的，，清楚地


Raise Custom Error Types
For custom error types, declare a new namespace. The namespace of an error type should help you identify the origin of an error. For example:
	<raise-error type="ORDER:INVALID_DATA" description="Email is invalid. Cannot complete transaction"/>

You cannot use existing namespaces from connectors or modules, since these have their defined namespaces. Once you declare a custom namespace by using it in a raise-error, you can use it in other raise-error components and custom types.
zzzzz。。这种错误类型有什么特别的意义。。前面是，命名空间，然后:,然后是 错误名称。。 命名空间，和错误名称 是可以 随便写的？还是 需要 真实存在的 java类？


Examples
The following example of a driving lessons platform checks that the user is over 16 years old to drive. The flow raises a PRECONDITIONS:INCORRECT_AGE error when the user enters an age lower than 16 years.

	The Choice router components adds conditional processing to the flow.

	If the age parameter sent in the request is lower than 16 http://localhost:8081/test?age=15, the Raise Error component raises the error type PRECONDITIONS:INCORRECT_AGE and description Minimum age of 16 required to drive.

	Then, an On-Error Continue component matches the type of the error PRECONDITIONS:INCORRECT_AGE and executes the Logger component inside its scope. The Logger component logs the error message Minimum age of 16 required to drive.

	If the age parameter is greater than 16 in the request http://localhost:8081/test?age=17, a Logger component returns the default message User age above 16 years. Allowed to drive.

	If no age parameter is passed in the request http://localhost:8081/test the choice component will automatically raise a MULE:EXPRESSION error when trying to evaluate the expression #[message.attributes.queryParams.age < 16] because no age parameter was provided in the request, so its value is null.

	Then, an On-Error Continue component matches the type of the error MULE:EXPRESSION and executes the Logger component inside its scope. The Logger component returns the message The parameter age is missing or invalid.

XML Configuration of the Flow:
	<http:listener-config name="HTTP_Listener_config">
	  <http:listener-connection host="0.0.0.0" port="8081" />
	</http:listener-config>
	<flow name="raise-error-example-flow">
	  <http:listener config-ref="HTTP_Listener_config" path="/test"/>
	  <choice>
	    <when expression="#[message.attributes.queryParams.age &lt; 16]">
	      <raise-error type="PRECONDITIONS:INCORRECT_AGE" description="Minimum age of 16 required to drive" />
	    </when>
	    <otherwise >
	      <logger level="INFO" message="User age above 16 years. Allowed to drive"/>
	    </otherwise>
	  </choice>
	  <error-handler >
	    <on-error-continue enableNotifications="true" logException="true" type="PRECONDITIONS:INCORRECT_AGE">
	      <logger level="INFO" message="Minimum age to drive is 16 years old"/>
	    </on-error-continue>
	    <on-error-continue enableNotifications="true" logException="true"  type="MULE:EXPRESSION">
	      <logger level="INFO" message="The parameter age is missing or invalid"/>
	    </on-error-continue>
	  </error-handler>
	</flow>



Consider an API that returns an account balance when making a request to https://unsecurebank.com/balance.
The following example, makes a request to this API and stores the value in the balance variable, then it compares it with payload.amount to determine if the operation is possible.
The Choice router produces an ACCOUNT:INSUFFICIENT_FUNDS error when a transfer amount surpasses the available balance, preventing the transfer from taking place. Additionally, it generates a dynamic description by providing an expression with failure details.

XML Configuration of the Flow:
	<flow name="raise-error-example-flow">
	  <http:request url="https://unsecurebank.com/balance" target="balance"/>
	  <choice>
	    <when expression="#[payload.amount > vars.balance]">
	      <raise-error type="ACCOUNT:INSUFFICIENT_FUNDS" description="#['Cannot transfer $(payload.amount) since only $(vars.balance) are available.']"/>
	    </when>
	  </choice>
	  <http:request url="https://unsecurebank.com/transfer"/>
	</flow>




https://docs.mulesoft.com/mule-runtime/4.3/round-robin
Round Robin Router

The Round Robin router iterates through a list of two or more routes in order, but it only routes to one of the routes each time it is executed. It keeps track of the previously selected route and never selects the same route consecutively. For example, the first time Round Robin executes, it selects the first route. The next time, it selects the second route. If the previously selected route is the last route in the list, Round Robin jumps to the first route.

。。还真是轮询，，一轮 全都得询问到。。第一次是随机选一个，做为开始，然后开始转圈处理。
。。不过这里还真没说，是不是要全部处理一遍。。
。。不过这种轮询有什么用？降低 处理器 的压力？毕竟 全部从第一个开始的话，第一个满载，后面几个全部空载的。。
。。这个要配合 其他东西使用吧？不然它的数据来源是什么？就一个事件的话，轮询没有意义，，那就只能forEach，或者batch了。。。但是 IDE里 它可以放在很多地方。。
。。结合下面的例子与描述，好像是，单例的轮询器，一个事件选一条路径，然后i++，下个事件就选下一个路径了。
zzzzz。。但是有什么用？除非大压力，但是为什么不用batch？。。可能这个轮询在 batch之前， batch也扛不住的时候，就需要这个了。。。，，这里可以调用不同的端点吗？其他的端点里是batch。。。但是 这个轮询太简陋了吧。。

<mule xmlns="http://www.mulesoft.org/schema/mule/core" xmlns:doc="http://www.mulesoft.org/schema/mule/documentation"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd">
	<flow name="round-robinFlow" doc:id="95d79691-4142-43b1-9719-7d6dc19c40bd" >
		<scheduler doc:name="Scheduler" doc:id="77a281f1-7643-422d-be72-25456b767d68" >
			<scheduling-strategy >
				<fixed-frequency />
			</scheduling-strategy>
		</scheduler>
		<round-robin doc:name="Round Robin" doc:id="6fe1f667-05b2-47d3-81d2-e9ef27f7c9b6" >
			<route>
				<logger level="ERROR" doc:name="Logger" doc:id="e38d4396-d4ea-4eec-88dc-2bf60f579d5d" message="Route1"/>
			</route>
			<route>
				<logger level="ERROR" doc:name="Logger" doc:id="6127fe32-0bc0-48e9-bb18-a9f3bfc45d0b" message="Route2"/>
			</route>
		</round-robin>
	</flow>
</mule>

In the example above, the first time the Round Robin is executed it prints Route1. The next time, it prints Route2. The third time, since there are only two routes, Round Robin starts again with the first route and prints Route1.
。。ok。感觉是：轮询器 是 单例的，所以 一个请求 后移一个processor。


Throws
MULE:ROUTING




https://docs.mulesoft.com/mule-runtime/4.3/remove-variable
Remove Variable Transformer

The Remove Variable component takes a variable name and removes the variable from the Mule event.
。。map.remove(xx)

Field	Description
Display Name (doc:name)
	Customize to display a unique name for this component in your flow. Defaults to Remove Variable.

Name (variableName)
	Name of the variable to remove.




https://docs.mulesoft.com/mule-runtime/4.3/scheduler-concept
Scheduler Endpoint (Trigger)
。。这是一个事件的起点，所以必然是fow的第一步。

The Scheduler component enables you to trigger a flow when a time-based condition is met. You can configure it to be triggered at a regular interval or give it a more flexible cron expression. For example, a Scheduler might trigger an event to start a flow every 5 seconds.
。。定时(每隔x分钟)触发，或 几时几点几分 触发。

Consider the following when you implement a Scheduler in your Mule application:
	Schedulers use the same timezone as the machine on which Mule is running. However, if an application is running in CloudHub, the Scheduler conforms to the UTC timezone, regardless of the geographic region in which the application is running.
在本地机器上跑，就根据本地机器的时区来做为 它的时区，如果在cloudhub上，就使用UTC
。。。为了方便，在不需要精确到秒的情况下，通常将GMT 和UTC 视作等同。但UTC 更加科学更加精确，它是以原子时为基础，在时刻上尽量接近世界时的一种时间计量系统。它的出现是现代社会对于精确计时的需要。GMT（Greenwich Mean Time）——格林尼治标准时间

	The scheduler component can trigger only one flow at any given time, and if a triggered flow is executing, it won’t be triggered again until it finishes processing.
。不能重复触发，只能等待上次触发的完成后，才触发。，，应该有配置能concurrent吧。

	If back-pressure is triggered because no resources are available at the time of the trigger, that trigger is skipped.
。如果由于没有资源(cpu，内存) 而触发了 back-pressure(背压),那么这个触发器会被跳过。。
。。是跳过一次，还是下次重启前都不会再触发。。。估计前者。
。。背压，是 由于trigger触发的背压，还是 其他地方触发了背压？

	In a Mule runtime engine cluster or multi-worker CloudHub deployment, the Scheduler runs (or triggers the flow) only on the primary node (that is, only in one Mule instance).
。集群，多工作者，，调度器 只在 最主要的 那个节点上 工作。。但是 集群 哪有 最主要的节点呢。。


Example Projects
There are several example projects in Anypoint Exchange that you can open in Anypoint Studio to learn more about how to use the Scheduler component:
Data Sync Using Watermarking and Batch
Import Contacts into Microsoft Dynamics CRM
Import Leads into Salesforce
Importing a CSV File into mongoDB
Importing an Email Attachment Using the IMAP Connector
Salesforce to MySQL Database Using Batch Processing
Sending a CSV File Through Email Using SMTP



Fixed Frequency
。固定频率。。这是一个 下拉框(scheduling strategy)选择的一项。

You can use the following parameters to set a Scheduler to trigger a flow at a regular intervals.

Attribute	Description	Default
Frequency
	The frequency at which the Scheduler triggers the flow. You select the unit of time in the Time unit field. Setting the value to 0 or a negative value causes the default value to be used.
		1000
。。多久触发一次。

Start Delay
	The amount of time to wait before triggering the flow for the first time after the application is started. This value is expressed in the same unit of time as the frequency.
		0
。。启动延迟。时间单位 和 frequency 一致。
。。就是 上面2个 int的 单位都是下面。

Time Unit
	The time unit for the values of Frequency and Start Delay. The possible values are:
	MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS
		MILLISECONDS
。。时间单位，毫秒，秒，分，时，天。



Cron Expressions
。。下拉框的另一个选项。

Cron is a widely used standard for describing time and date information.

The Scheduler keeps track of every second and creates a Mule event when the Quartz Cron expression matches your time-date setting. You can trigger the event just once or at regular intervals.
。。保持追踪每一秒。

A date-time expression consists of six required settings and can include the optional year setting. You specify the settings in the following order:

Seconds (0-59)
Minutes (0-59)
Hours (0-23)
Day of month (1-31)
Month (1-12 or JAN-DEC)
Day of the week (1-7 or SUN-SAT)
Year (empty or a 4-digit year between 1970-2099, for example, 2019)
。。2099，，总觉得短了点？


Expression	Behavior
1/2 * * * * ?
	Run every 2 seconds of the day, every day.
。。 0/2 呢？似乎是这种规则， x/y, 说明是  ??分x秒触发第一次，之后每隔y秒触发一次。。
。。0/2的话， 在下一分钟的时候会不会触发2次，，不，世界上不存在??分60秒 这个概念。
。。* ? 区别
。。为什么有些有 年，有些没有？
。 '?' can only be specfied for Day-of-Month -OR- Day-of-Week.
- *为字段指明特定值，代表所有可能的值
- ？指明该字段没有特定的值
？只用在日期或星期字段，日期和星期字段必须有且只能有一个字段是特定值（*或数字等）另一个字段设为没有特定值（？）

0 15 10 ? * *
	Run at 10:15 a.m., every day. 0 15 10 * * ? * and 0 15 10 * * ? produce the same effect.
。。早上10.15，每天， **?*,**? 也提供了 相同的作用。

0 15 10 * * ? 2019
	Run at 10:15 a.m., every day during the year 2019.
。。2019年 每天10.15

0 * 14 * * ?
	Run every minute starting at 2pm and ending at 2:59pm, every day.
。。14点到14.59 中的 每分钟

0 0/5 14 * * ?
	Run every 5 minutes starting at 2pm and ending at 2:55pm, every day
。。14到14.55 中的 每5分钟

1 1 1 1,7 * ?
	Run the first second of the first minute of the first hour, on the first and seventh day, every month.
。。每月，每周日，的1.1.1 。


Note that the Scheduler component also supports Quartz Scheduler special characters:
*: All values.
?: No specific value.
-: Range of values, for example, 1-3.
,: Additional values, for example, 1,7.
/: Incremental values, for example, 1/7.
L: Last day of the week or month, or last specific day of the month (such as 6L for the last Saturday of the month).
W: Weekday, which is valid in the month and day-of-the-week fields.
#: Nth day of the month. For example, #3 is the third day of the month.



Time Zones (timeZone)
For Cron configurations, Java timeZone values are supported. You should avoid the Java abbreviations, such as PST and AGT, and instead use the full-name Java equivalents, such as America/Los_Angeles and America/Argentina/Buenos_Aires.
。避免缩略语。


Debugging
In the Studio Debug Mode view, the Mule app disables the triggering of events. You can instead execute the Scheduler component when debugging by clicking the green arrow icon that appears at the bottom of the component.



https://docs.mulesoft.com/mule-runtime/4.3/scheduler-xml-reference
Scheduler XML Reference

A XML for the Scheduler has these elements:
	scheduler* parent element that encloses the rest of the elements.
	scheduling-strategy* block inside scheduler.
	fixed-frequency* or cron* element inside the scheduling-strategy block.

。。xml元素。

<scheduler> Properties
disallowConcurrentExecution
	Skips the next scheduled flow execution if the last one has not ended. Next attempt to execute will be after another frequency period. Default value is true.
		disallowConcurrentExecution="false"
。不允许多线程执行，默认false(就是允许多线程)。


<fixed-frequency> Properties

frequency
	Triggering frequency, relative to time unit. When set to 0 or a negative value, it switches to the default (1 minute).
		frequency="1000"
。。0或负数，就1分钟一次。。估计GUI里也是这样的。

timeUnit
	The time unit for the frequency and startDelay values:
	MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS
		timeUnit="MILLISECONDS"

startDelay
	Upon triggering the flow for the first time, Mule delays the first execution of the scheduler for a specific amount of time. This time period is expressed in the same time units as the frequency.
		startDelay="0"


<cron> Properties

Parameter	Default Value	Description	Example
expression
	-
		Use a cron expression for when to do a trigger.
			expression="1 1 1 1,7 * ?"

timeZone
	Time zone passed as system property, or in machine’s operating system.
		Timezone in Java time zone format
			timeZone="America/Argentina/Buenos_Aires"



https://docs.mulesoft.com/mule-runtime/4.3/scatter-gather-concept
Scatter-Gather Router
。。分散-聚集 路由器

The Scatter-Gather component is a routing event processor that processes a Mule event through different parallel processing routes that contain different event processors. Each route receives a reference to the Mule event and executes a sequence of one or more event processors. Each of these routes uses a separate thread to execute the event processors, and the resulting Mule event can be either the same Mule event without modifications or a new Mule event with its own payload, attributes, and variables. The Scatter-Gather component then combines the Mule events returned by each processing route into a new Mule event that is passed to the next event processor only after every route completes successfully.
。。是一个 分发事件处理器，来 通过 不同的 并行处理路径 来处理事件。。。。。。这里的通过 是指 全部处理一遍。。而不是通过某条路径。
。每个路径接受一个mule事件的 ref引用，然后执行一系列的processors。
。每个路径 使用一个单独的线程 来执行processors，，产生的mule事件 可以是一个 没有改变的mule事件， 也可以是一个 新的mule事件(有它自己的payload，属性，变量)。
。在所有路径 成功 处理完后， scatter-gather组件 组合 这些 来自各个路径 的 mule事件 到一个 新的 mule 事件， 然后传递给 下一个 组件。
zzzzz。。怎么分发？这里说了 不同的事件处理啊，难道说的是 多例？ 而不是说 不同的处理逻辑？。。 是 人为地 将 一个完整的处理逻辑，分割为 多个 可以独立执行的 部分。
。怎么合并？ 难道 event套娃？。。
。。总不可能是 轮询处理全部吧。。。不是轮询，是全部。。这个需要人把 一个完整处理逻辑 分割成 多个独立的部分，然后这个 组件 就把 一个事件 分别给 所有的独立的处理部分，然后再 把结果合并。
。。合并的话，应该 需要 和 原始event对比，来看 修改了哪个属性，，，但是如果 2个 processor 都修改了 某个属性呢？，不过这种不太可能， 属性只有一个，中间变量不应该赋给 这个属性，就是说，set属性，只应该被调用一次。

The Scatter-Gather component receives a Mule event and sends a reference of this Mule event to each processing route.
。。这里才说了，发送一个引用 给 每个 处理路径。
。。。感觉不是 引用，应该是 副本。

Each of the processing routes starts executing in parallel. After all processors inside a route finish processing, the route returns a Mule event, which can be either the same Mule event without modifications or a new Mule event created by the processors in the route as a result of the modifications applied.
。。并发，

After all processing routes have finished execution, the Scatter-Gather component creates a new Mule event that combines all resulting Mule events from each route, and then passes the new Mule event to the next component in the flow.
。。

Configure your Scatter-Gather component to have at least two routes; otherwise, your Mule application throws an exception and does not start.
。。必须有2条路。。


Variable Propagation
Every route starts with the same initial variable values. Modifications to a variable within a specific route do not affect other routes. So, if a variable is added or modified in one route, then, after aggregation, the value is defined by that route. If a variable is added or modified by more than one route, the value is added to a list of all the values defined for that variable within all the routes, for example:
。。副本，不然 会相互影响的。
。。route里设置的 variable，会 带出route。  如果重复了，那么就是 一个list。

<set-variable variableName="var1" value="var1"/>
<set-variable variableName="var2" value="var2"/>
<scatter-gather doc:name="Scatter-Gather" doc:id="abc665e0-6119-4ecb-9f8b-52dbcbb1d488" >
	<route >
		<set-variable variableName="var2" value="newValue"/>
        <set-variable variableName="var3" value="appleVal"/>
	</route>
	<route >
		<set-variable variableName="var3" value="bananaVal"/>
	</route>
	<route >
		<set-variable variableName="var3" value="otherVal"/>
        <set-variable variableName="var4" value="val4"/>
	</route>
</scatter-gather>


After aggregation, the variables are:

{var1: "var1", var2: "newValue", var3: ["appleVal, bananaVal, otherVal"], var4: "val4"}
。。如果 多个route 修改了 var2呢？ 也会变成 var2:[xx1,xx2] ?
。。multihashMap。



Error Handling Inside Scatter-Gather Routes

You can use a Try scope in each route of a Scatter-Gather component to handle any errors that might be generated by a route’s event processors. After every route executes, if any has failed with an error, then the Scatter-Gather component throws an error of type MULE:COMPOSITE_ERROR, and event processing does not proceed past the Scatter-Gather component in the flow. Instead, the flow branches to your error-handling event processors.
。可以，每条路径使用try 包裹。来处理 错误。
。。所有路径 执行完后，如果某条路径 因为 错误而 失败 ，scatter-gather就抛出异常，scatter-gather之后的 组件不在 执行，而是执行 错误处理组件。
。。try住了，不抛出，应该不算 执行失败吧。
。。或者 我执行失败，但不抛出异常呢，，估计不行，，执行成功/失败，应该是mule控制的，人写代码时 无法控制吧。


Because the MULE:COMPOSITE_ERROR error object gathers not only errors from routes that failed but also Mule events from successfully completed routes, your application can use the error-handling event processors to process Mule events from the routes that completed
。。MULE:COMPOSITE_ERROR 这个错误对象，不仅包含失败路径的错误，也包含 成功路径 的 mule event， 应用可以使用 错误处理组件 来处理 mule event.

To illustrate how this works, consider the following two cases:
	The routes in a Scatter-Gather component each contain a Try scope.
	One of the routes generates an error that is successfully handled by that route’s Try scope through an on-error-continue error handler, so the route is completed successfully. The Scatter-Gather component consolidates the Mule events from all routes into a new Mule event and passes the consolidated event to the next event processor.
。。每个路径都有一个try。
。一条路径 产生一个错误，被这条路径的try 处理了，那么这条路径 是执行成功的。
。scatter-gather组件 组合 所有路径返回的事件 到一个新的事件，传递给下一个组件。

	One of the routes in a Scatter-Gather component does not contain a Try scope or contains a Try scope with an error handler that cannot handle the error type, or the error handler is an on-error-propagate type.
	An error occurs in this route, causing the route to fail, which in turn causes the Scatter-Gather component to throw a MULE:COMPOSITE_ROUTING error. The flow branches to your error-handling event processors, which are able to process the Mule events from the completed routes.
。。一条路径没有try，或者try 无法处理 这个错误类型，或者 这个错误处理器 是一个 on-error-propagate 。
。。一个错误发生，导致这条路径失败，继而导致scatter-gather抛出一个错误，
zzzzz。。一条路径失败，scatter-gather 到底抛出  MULE:COMPOSITE_ROUTING  还是  MULE:COMPOSITE_ERROR ？


Example of handling these errors:
<flow name="errorHandler">
    <scatter-gather>
        <route>
            <raise-error type="APP:MYERROR"/>
        </route>
        <route>
            <set-payload value="apple"/>
        </route>
    </scatter-gather>
    <error-handler>
        <on-error-continue type="COMPOSITE_ROUTING">
            <!-- This will have the error thrown by the first route -->
            <logger level="WARN" message="#[error.errorMessage.payload.failures['0']]"/>
            <!-- This will be a null value -->
            <logger level="WARN" message="#[error.errorMessage.payload.failures['1']]"/>

            <!-- This will be a null value -->
            <logger level="WARN" message="#[error.errorMessage.payload.results['0']]"/>
            <!-- This will have the result of the second (correctly executed) route -->
            <logger level="WARN" message="#[error.errorMessage.payload.results['1']]"/>
        </on-error-continue>
    </error-handler>
</flow>


Handle Timeout Errors in a Scatter-Gather
If you configure a timeout for a Scatter-Gather component and a route does not complete processing before the timeout expires, the route throws a MULE:TIMEOUT error. This error is then handled the same way as any other error generated from a route: after each route completes (either by processing success or by throwing a MULE:TIMEOUT error), the successful results and errors are collected together in the Scatter-Gather component MULE:COMPOSITE_ROUTING error, which is then processed in your configured error handler.


Example Project
In Anypoint Studio, you can download and open the example project Scatter-Gather Flow Control from Anypoint Exchange to learn more about how to use the Scatter-Gather component. This example shows the usage of the scatter-gather control flow to aggregate data in parallel and return the result in JSON.

The example uses prepared data as input for two resources that should be aggregated. The data represents information about two contacts and has the following structure:

Resource firstname surname phone email
contacts-1.csv John Doe 096548763 john.doe@texasComp.com
contacts-2.csv Jane Doe 091558780 jane.doe@texasComp.com

DataWeave is used to aggregate the data. The information about the contacts is aggregated to a JSON structure that represents data from both resources.

To download and open this example project while you are in Anypoint Studio, click the Exchange icon in the upper-left corner. Then, in the window that opens, log into Anypoint Exchange and search on the name of the project.


Scatter-Gather XML Reference
Element	Description
scatter-gather
	Sends a request message to multiple targets concurrently. It collects the responses from all routes, and aggregates them into a single message.

Attributes
timeout
	Sets the timeout for responses from sent messages, in milliseconds. A value of 0 or lower than 0 means no timeout.

maxConcurrency
	Determines the maximum amount of concurrent routes to process.
	By default all routes run in parallel.
	By setting this value to 1, scatter-gather processes the routes sequentially.

target
	The name of the target variable.

targetValue
	Value of the data to store in the target variable.
	If not set, the default value is #[payload].
	This field accepts any value that a variable accepts:
		Any supported data type.
		DataWeave expressions.
		The keywords payload, attributes, and message, but not the keyword vars.

Child Element	Description
route
	One of the routes in a the scatter-gather router.


Throws
MULE:ROUTING
MULE:COMPOSITE_ROUTING





https://docs.mulesoft.com/mule-runtime/4.3/set-payload-transformer-reference
Set Payload Transformer

The Set Payload (set-payload) component lets you update the payload of the message. The payload can be a literal string or a DataWeave expression. The set-payload component, however, is not recommended for complex expressions or transformations but rather, simple ones, such as selections. You should use Transform Message Component for complex scenarios.
。。更新message里的payload。
。这个组件 不推荐 复杂的表达式或转换。
。。复杂的 设想 应该使用 transform message组件。


Field	Usage	Description
Value (value)
	Required
		Accepts a literal string or DataWeave expression that defines how to set the payload, for example, "some string" or #[now()].

Mime Type (mimeType)
	Optional
		The mime type of the value assigned to the payload, for example, text/plain or application/json.

Encoding (encoding)
	Optional
		The encoding of the value assigned to the payload, for example, UTF-8.


The mimeType and encoding attributes will not affect a DataWeave expression used as value. They only affect the output Mule Message. If a transformation is required, the DataWeave expression must contain an explicit output directive.
。如果value是 DW表达式，mimeType encoding 无效。，这2个只影响输出的mule message，，如果需要转换，那么DW表达式必须包含一个 确定的 输出指令


This XML example sets a payload with static values:
Static Content Example:
	<set-payload value="{ 'name' : 'Ana', 'office' : 'BA' }" mimeType="application/json" encoding="UTF-8"/>


The next example sets the message payload to "Hello, World" appending today’s date using a DataWeave expression: #['Hello World!' ++ ' Today is ' ++ now()]

Expression Content Example:
	<set-payload value="#['Hello World!' ++ ' Today is ' ++ now()]"/>




https://docs.mulesoft.com/mule-runtime/4.3/variable-transformer-reference
Set Variable Transformer

The Set Variable (set-variable) component is for creating or updating a variable to store values for use within the flow of a Mule app. You can store simple literal values such as strings or messages, message payloads, or attribute objects. For example, you might store the original payload of a message (before it is processed) so you can use it later in the flow or within an error handler.

The set-variable component is not recommended for complex expressions or transformations. You should instead use it for simple ones, such as selections, and use the Transform Component for complex scenarios.
。不建议 复杂的表达式和转换。。。 复杂的，使用Transform组件（应该是Transform message组件）

zzzzz。。。variable，payload 有什么区别？。。估计variable是全局变量，payload 是 这个请求的变量。？ 是的，上面说 updating a variable to store values for use within the flow of a Mule app. ，flow里用的变量。
。。。感觉payload 也是一个 variable，只不过这个 payload 是 保留字。

Field	Usage	Description
Variable Name (variableName)
	Required
		Name of the variable. Names can only include numbers, characters, and underscores. For example, hyphens are not allowed in the name.

Value (value)
	Required
		Value for the variable, which can be a string or a DataWeave expression.

Mime Type (mimeType)
	Optional
		Sets the variable MIME type, such as text/plain or application/json.

Encoding (encoding)
	Optional
		Sets the variable encoding, such as ISO 10646/Unicode(UTF-8).

The mimeType and encoding attributes do not affect a DataWeave expression used as value. They only affect the output value stored. If a transformation is required, the DataWeave expression must contain an explicit output directive.


Examples
This example sets the variable to a string:
Name = myVar
Value = my first variable

This example sets the variable by using a DataWeave operation that results in a value of 5:
Name = myVar
Value = #[max([1,2,3] ++ [3,4,5])] in Anypoint Studio.

This example sets the variable to the message payload:
Name = myVar
Value = payload in Design Center, #[payload] in Anypoint Studio.

This example sets the variable to the message attributes:
Name = myVar
Value = attributes in Design Center, #[attributes] in Anypoint Studio.

This example sets the variable to the entire message:
Name = myVar
Value = message in Design Center, #[message] in Anypoint Studio.

This XML example sets a variable that takes a map as a value: 
<set-variable variableName="employee" value="{ 'name' : 'Ana', 'office' : 'BA' }" mimeType="application/json" encoding="UTF-8/>

This example sets the same variable using selectors in a DataWeave script. It assumes the name attribute is available as input to Set Variable: 
<set-variable variableName="employee" value="#[output application/java --- payload.name]"/>

These examples set the variable to a Boolean value, true:
Name = myVar
Value =
	true in Design Center, #[true] in Anypoint Studio
	true as Boolean in Design Center, #[true as Boolean] in Anypoint Studio
	(1 + 1 == 2) evaluates to true in Design Center, #[(1 + 1 == 2)] evaluates to true in Anypoint Studio

To display the value of a variable through the Logger component in Design Center, you might need to use the Anypoint Studio syntax to display the value of a variable, for example, #[vars.myVar] instead of vars.myVar.

。。vars，应该就是 variables 这个map的 名字。



Accessing Variables in Other Event Processors
Set Variable sets a variable in the current Mule event, and the variables then travel with the Mule event to downstream event processors. You can access any variable with DataWeave using *vars. So if you set a variable named lastMessage, you can access it as *vars.lastMessage. You can set variables in a Transform Message component, and also many connectors and event processors have a Target that can be set in the Advanced tab. These all set flow variables and they are accessed the same way, through the keyword vars..
。设置后，后续都能通过vars.名字 来访问。
。。上面有 remove variable transformer....





https://docs.mulesoft.com/mule-runtime/4.3/transform-component-about
Transform Message Component

The Transform (or Transform Message) component converts input data to a new output structure or format. You can use the component’s visual mapper to build your transformation implicitly by dragging and dropping elements of the incoming data structure, or you can explicitly write a transformation script in DataWeave expression language.
。。把输入转换为 新的输出结构或格式。，属性的值没有修改，只不过换了个名字，或者直接被删除了。，映射。

Graphical View (Drag-and-Drop editor): Two tree views show the expected metadata structures of the input and output. Mappings between these two trees are represented by lines that connect input to output fields. The lines can be created by dragging and dropping an element from the input to another element to the output.

Script View: The visual mapping can be represented as DataWeave code. Advanced transformations such as aggregation, normalization, grouping, joining, partitioning, pivoting and filtering can be coded here.
。。虚拟映射可以用DW来表示。


Preview
A preview of your output is built on sample data (such as a JSON snippet) and is updated in real time as you make changes to your transformation.
。。预展，样本数据+现在的修改。


Metadata
The Transform component accepts input and output metadata for the message payload, a variable or an attribute. You can provide format-specific resources and settings for supported mime types (formats):
。。组件接受 输入，输出的 payload的 元数据(感觉说的是MIME)，一个变量或一个属性。
。。你能提供 明确格式化 的 资源和设置 ，为了支持mime 类型。

	CSV example or RAML definition
	Copybook file or schema
	Excel column name and type definitions (including String, Number, Boolean, Date, and DateTime) for one more sheets
	Fixed Width column name, type (String, Integer, Decimal, Boolean, Date, and DateTime), width, and format settings
	Flat File schema
	JSON example or schema
	Object class or RAML definition
	Simple Type (String, Number, Integer, Boolean, Date, or DateTime)
	XML example or schema



https://docs.mulesoft.com/mule-runtime/4.3/transform-workflow-create-mapping-ui-studio
Workflow: Create a Mapping through the Anypoint Studio UI

Define Input and Output Structure of a Transformation Provide expected input and required output structures.

Graphically Construct a Mapping Drag elements of the input and drop them onto elements of the output. You can also include fixed values and functions to build out the output.

Preview the Output of a Transformation Verify that the transformation produces the expected output.




https://docs.mulesoft.com/mule-runtime/4.3/transform-input-output-structure-transformation-studio-task
Define Input and Output Structure of a Transformation (Anypoint Studio)

To use the graphical view and construct mappings via drag and drop, the Transform Message component first needs information about what fields are available for dragging and dropping onto.
。。使用图形试图，创建映射 通过拖拉，组件需要 哪些属性在拖拉中有用。

If you plan to create your transformation entirely by writing DataWeave code, you can skip defining input and output and reference elements directly.
。
。如果你计划 全部通过 DW 来创建你的转换器 ，你可以跳过 直接定义输入，输出，和 用到的元素。


Define Data Structure on Other Components
。。其他组件中定义 数据结构
	Make sure that all other elements in the flow are fully configured.
	When input and output of a component can be predicted based on its configuration, these are exposed to the rest of the flow. For example, when a Salesforce connector is configured to carry out a specific operation, its output structure is known and exposed.
。确定，flow中所有其他元素都已经 配置好了。
。。当一个组件的 输入输出 能被 它的配置 完全预告(推测)，那么它们就(自动)被暴露在 flow 的(后续)其他组件中。

	If input or output of certain components can’t be predicted, manually define the output and input of key components via their Metadata Tab.
	For example, an HTTP listener might receive requests containing any payload.
。。如果 某个组件的输入输出 不能被推测， 手工定义 输入输出 通过它们的metadata 标签。


Define Data Structure in Transform Message Component
。。transform message 组件中定义 数据结构。
	Right click on the root of the input or the output tree, then select Set Metadata.
	Manually define the data structure.
。右键 input，output 树的 根，然后选择 set metadata.


Define Data Structure Via XML
See DataWeave XML Reference.


Reader Configuration
In case your input needs to be parsed in a special way, you can set up certain properties for the reader object. Each input format has different configurable properties, or none. There are two ways to set these listed below.
。。输入需要 特殊的方法来转换，你能设置 确定(独特)的属性 为了 reader对象。，每个输入格式化有不同的可配置属性，有2个方法来设置。

	Configure other components on the flow, by accessing their corresponding Metadata tab.
。。通过metadata 来 配置其他组件，。。。。

	On the Transform Message component, right click on the root of the input pane and select Reader Configuration
。。在transform message组件，右键 输入的根，选择 Reader COnfiguration。。
。。没有啊，看图是 右键 input的 payload， 但是没有 Reader Configuration 这个，，，。。估计只有 下面3种 才有 reader configuration...

For a detailed reference of what properties can be set in the Reader Configuration of each format, see the corresponding reader properties section:
CSV
XML
Flat File
..上面是3个链接。


Writer Configuration
In case your output needs to be constructed in a special way, you can set up certain properties of the writer object. Each output format has different configurable properties, or none.
。。输出，需要一个 特殊/独特 的方法 来创建。你能设置 writer对象的 确定的属性，每个输出格式化 都有不同的 配置属性。
。。。这里的格式化，估计就是指 下面的 CSV，XML 这些 类型的 格式化器。

These properties must be written on the %output directive of your DataWeave code.

For a detailed reference of what properties can be set in the Writer Configuration of each format, see the corresponding reader properties section:
CSV
XML
JSON
Flat File
。。4个链接，链接到 https://docs.mulesoft.com/mule-runtime/4.3/dataweave-formats#json






https://docs.mulesoft.com/mule-runtime/4.3/transform-graphically-construct-mapping-studio-task
Graphically Construct a Mapping (Anypoint Studio)

Through the graphical UI of the Transform Message component you can do the tasks below. When doing each, the necessary lines are added to the DataWeave code to describe the action.


Map a Field
Drag an element on the input structure over to another on the output structure, this casts a line that joins them.
。。从input拖一个元素 到 output 对应的元素上，这样就会有一条线连接 输入的属性 和 输出的属性。

Map a Structure
Drag a high-level structure that contains lists of elements or inner fields onto another in the output. This creates a shaded region covering all the mapped structure. Fields that have identical names in these structures are automatically mapped too.
。拖一个高层的包含其他元素组成的结构(例子是List<Json>)，到output的 对应的 高层结构。。这会生成 一块区域 覆盖 映射的结构，，input的结构里的属性 会自动映射 同名的output的结构的属性。


Set a Fixed Value or Function
Double click on a field in the output tree. This adds a function icon using dataweave in studio (图 fx) next to it.
。。双击output里的一个属性，这会增加一个，使用DW

A new line is added to the DataWeave code that assigns a default null value to the field. This line is highlighted. Change its value to anything you want.
。。DW代码中会加入一个新行，关联的属性是null，自己改值。。是的，没有连系input，output的话， 最右侧的 DW代码里是空的， 双击 output的属性后，DW代码里 就有一行代码了，值是null。。可以直接手写代码，手写代码里 属性 区分大小写。

You can add:
A fixed string value by using " ".
A reference to an input field.
A DataWeave statement that includes functions.



https://docs.mulesoft.com/mule-runtime/4.3/transform-preview-transformation-output-studio-task
Preview the Output of a Transformation (Anypoint Studio)

You can provide sample input data to your Transform Message component to see how the transformation affects it. You don’t need to deploy your project to use this feature. A sample output is updated in real time as you make changes both to the sample input and the transformation.
。。你可以使用一些 样本 输入数据 给Transform Message，来看 它 怎么起作用。 不必发布，样本数据的output是实时更新的。

Click on the root of the input pane and select Edit Sample Data.
。右键input - edit sample data。

A new tab opens with an empty scaffolding of your input structure. Values are populated with the string ???? as a placeholder.
。。就是进入 payload 页。 ???? 是占位符 。

Enable the preview pane by clicking on the Preview button on the top-right of the editor.
As you make changes in the DataWeave code or your sample data, notice how the output data structure changes.
。最右面的Previous(ctrl + shift + p)

Test your transformation for any special characters or structures that might arrive, verify that the output is what you expected.




https://docs.mulesoft.com/mule-runtime/4.3/transform-tips-transform-message-ui-studio
Tips: Transform Message Component UI (Anypoint Studio)

Before creating your mapping, configure everything else on the flow so that the input and output is known.
。。创建mapping前，配置所有其他的组件，这样 input，output各自包含什么内容 就知道了。

To can filter what you see from the input or output structures, click the magnifying glass icon tips transform message ui studio (图片 放大镜) at the top of either section and then start typing the name of a field.
。。放大镜 来过滤出 你想要的 属性。。。不过这个 放大镜 是搜索框，不是 可点击按钮

Select an element in the output pane to have its corresponding line in the DataWeave code highlighted.
。。选择一个 output的 元素，能获得 它的 通信的 线，在DW代码 高亮。。。DW代码里好像没有高亮啊。线是高亮的。可能是 flow有错误的原因，下面的操作 是可以 在DW 代码中，将选择的映射 选中的。

If an input field is mapped to two or more output fields, you can right-click it and then select which of the multiple outputs you want to highlight in the DataWeave code.
。如果1个input属性映射了多个output属性，可以右键input的属性，选择 在DW代码中 高亮哪个。




https://docs.mulesoft.com/mule-runtime/4.3/transform-change-target-output-transformation-studio-task
Change the Target Output of a Transformation (Anypoint Studio)

By default, the Transform Message component outputs to the message payload. You can change this target, so that the result of your transformation populates another part of the output such as a property or variable.
。。默认时，transform message 组件输出 message 的payload。
。。可以修改，比如，让 你的转换的结果 做为一个 属性 或 变量 添加到 output。
。。property，variable 是不同的，，但是忘记了。。

Click Edit Current Target above the DataWeave code editor.
Specify where in the output Mule message to place the result of your DataWeave transform. In case you’re creating a new variable or property, you must also set a name for it.
。。DW code 编辑框 上面 有一个 笔 的图标，这个就是 edit current target。
。。这个弹出框 的 output 有3个选项，第一次进去的时候是Payload，
，，3个分别是 payload，attribute，variable。。。不是 property。。
。。这3个，只有 variable 需要 写 variable的名字。
。。本地，这3个选择下， DW code 都是一样的。。




https://docs.mulesoft.com/mule-runtime/4.3/transform-add-another-output-transform-studio-task
Add Another Output to the Transform Message Component (Anypoint Studio)

A single Transform Message component can give shape to several different parts of the output Mule event (the payload, variables, and attributes). Each different output part must be defined in a separate target XML element inside the <ee:transform> XML element as anther block of DataWeave code. In Anypoint Studio 7, you do this by writing the DataWeave code in a separate tab of the Transform pane. For example, if one tab defines the payload, and another attributes, these are both parts of the same output Mule event.
。。普通的transform message组件 就能 清晰地表达 mule output 事件的 不同部分(payload,variables,attributes).
。。每个不同的输出部分 必须定义在一个 单独的xml元素中，(这个xml元素属于<ee:transform>元素，来做为 另一个DW代码块)
。。在anypoint studio 7中，通过 在transform 的单独的tab中 写DW代码来完成上述动作。
。。例如，如果一个tab定义了 payload 和 另一个属性， 它们都是 一个output事件的 一部分。

To add a new output target:
Click Add new target.  (图片 类似 三+， 就是 上面的 笔 的 左边那个)
Specify where in the output Mule message to place the result of this new DataWeave transform. In case you’re creating a new variable or property, you must also set a name for it.
。。说明 这个新DW transform 的结果 放在 mule message的哪里。

。。。有什么用啊，这个放进去的是什么？好像还是payload啊。


How Targets of a Transform Message Component are Represented in the Configuration XML File

The payload is represented in an <ee:message> element, as a child element of the <ee:message> element.

<ee:transform doc:name="Set Transactions XML" doc:id="5c58d889-896d-495a-b2f6-fe1613ae8044" >
  <ee:message >
    <ee:set-payload ><![CDATA[%dw 2.0
                              output application/xml
                              ...]]>

Attributes are represented together inside the <ee:message> element, as a child of the <ee:message> element.

<ee:transform doc:name="Set Transactions XML">
  <ee:message >
    <ee:set-payload ><![CDATA[%dw 2.0
                              output application/xml
                              ...]]>
    </ee:set-payload/>

    <ee:set-attributes>
    <![CDATA[%dw 2.0
             output application/java
             ---
            { }]]>
    </ee:set-attributes>
  </ee:message>
  ...
</ee:transform>


Each variable is represented inside a separate <ee:variable> element, as a child the <ee:variables> parent element. The <ee:variables> element is a direct child of the <ee:transform> element, so it is not part of the <ee:message> element. This XML structure reflects that variables and the message are carried along together with the parent event object.

Here is an example of a variable target defined inside a Transform Message component:

<ee:transform doc:name="Set accounts var" doc:id="15e226ab-8204-4d84-ab4b-f4fcdd088656">
  <ee:message>
  ...
  </ee:message>
  <ee:variables>
    <ee:set-variable variableName="accounts">
      <![CDATA[%dw 2.0
               output application/json
               ---
               ...]]>
    </ee:variable>
  </ee:variables>
</ee:transform>





https://docs.mulesoft.com/mule-runtime/4.3/transform-move-transformations-separate-file-studio-task
Move Transformation to Separate Files (Anypoint Studio)

By default, the DataWeave code from a Transform Message component is expressed inline within your Mule XML file. You can instead keep each in a separate file ( .dwl ) and have your XML reference them. These files are packaged with your Mule application when built.
。。一般情况，在transform中DW代码 ，以内置的形式展现 在你的xml文件中。 你能把它保存到单独的文件，然后让你的xml配置
代替 它们，这些文件要

To export the DataWeave code to a .dwl file:
Click the Edit Current Target button.
Select the File radio button.
。。导出 DW 到dwl文件。 选择 edit current target(图标 笔), 下面选择file，输入名字.dwl，点击ok。在 src/main/resources 中创建了。

A file is created under the src/main/resources folder in your project containing your DataWeave code.



https://docs.mulesoft.com/mule-runtime/4.3/transform-to-change-target-output-design-center
Output a Transformation to an Attribute or Variable (Design Center)

。。。xxx。。。

Remove an Output Transformation
Click the title of the Output column.

Click the Trash Can icon for the transformation that you want to delete.
。。看懂了一些。
。。整个event 只能  一个payload，一个attributes，N个variable。
。。图标 笔 是 修改当前的 output，可以将当前output的类型修改成 其他类型。
。。图标 三+ 是增加 output，可以选择类型。
zzzzz。。这2个功能 收到 11N 的限制，所以 如果当前有payload，三+的弹出框 是没有 payload 这个类型的。
。所以一个叫 edit current target， 一个叫 add new target，， target就是指当前output框内显示的东西。
。。结合第一页的，
Logger message for an attribute: message.attributes
Logger message for the variable *myVar: *vars.myVar
。。message.attributes，所以 attribute 只能有一个。。
。。但是这里是 attribute!s! ...。。不是只有一个么。。

。。还是得看 event 这个东西的 java 定义。
。。org.mule.runtime.api.message.Message   只看到一个接口。



https://docs.mulesoft.com/mule-runtime/4.3/transform-dataweave-xml-reference

Transform Message Component XML Reference

Instead of using the Studio UI to include DataWeave language in your Mule projects, you can also create and configure a 'Transform Message' component entirely through XML, either from the XML tab in Studio or from an external editor.
。不用 GUI，也可以完全用xml来 创建和配置 transform message 组件。
。使用GUI的时候，相应的xml 是会自动创建的。


Namespace and Schema Location

At the very start of your Mule Project, you must include the following dependencies:

<mule xmlns:ee="http://www.mulesoft.org/schema/mule/ee/core"
      ...
      xsi:schemaLocation="
      ...
      http://www.mulesoft.org/schema/mule/ee/core
      http://www.mulesoft.org/schema/mule/ee/core/current/mule-ee.xsd">
</mule>


<ee:transform/>
The <ee:transform> element is the top-level XML tag for the Transform component.

Attributes	Description
doc:name
	Defines the name of the element as it appears in the Studio canvas.

mode
	Refers to Deferred Execution. Accepted values: immediate or deferred. See DataWeave Memory Management.
。。mode 的链接是：https://docs.mulesoft.com/mule-runtime/4.3/dataweave-memory-management
。。但是里面根本没有提高 immediate deferred 这2个。。立刻，推迟。
..而且，看gui生成的xml，没有mode， 有doc:id


Adding DataWeave Scripts to the Transform Component
You can either type your DataWeave code into your XML using CDATA within a Transformation element, or you can reference an external .dwl file.
。。直接写到 xml中，或者 ref到dwl文件

This example that writes a DataWeave script inline within a <dw:set-payload> transformation element:
<ee:transform doc:id="747f74d4-cb66-4f8e-8222-24784e6863ae"
  doc:name="Transform" doc:timestamp="1510861248434">
    <ee:message>
        <ee:set-payload>
          <![CDATA[%dw 2.0
            output application/json
            ---
            {
              // YOUR DW SCRIPT
            }
          ]]>
       </ee:set-payload>
    </ee:message>
</ee:transform>

Here is same example, calling an external .dwl file:
<ee:transform doc:id="747f74d4-cb66-4f8e-8222-24784e6863ae"
  doc:name="Transform" doc:timestamp="1510861248434" doc:mode="immediate">
    <ee:message>
      <ee:set-payload resource="transform.dwl" />
    </ee:message>
</ee:transform>

The default location for these files is in the src/main/resources folder in your project.



Transformations

A single Transform component element (<ee:transform/>) can specify content for all parts of the Mule Event:
	<ee:message/> can contain <ee:set-payload/> and <ee:set-attributes/>.

	<ee:variables/> can contain one or more <ee:set-variable/> elements.

<ee:transform doc:id="747f74d4-cb66-4f8e-8222-24784e6863ae"
  doc:name="Transform" doc:timestamp="1510861248434">
    <ee:message>
        <ee:set-payload resource="transform.dwl" />
        <ee:set-attributes resource="myattributes.dwl" />
    </ee:message>
    <ee:variables>
        <ee:set-variable variableName="myVar" resource="myvar.dwl"/>
    </ee:variables>
</ee:transform>


Attributes	Description
resource
	Points to a .dwl file containing a DataWeave transformation script. These values are solved statically. For example, using <ee:set-payload resource="myscript-${env}.dwl"/> generates this error in the Transform Message UI: Unexpected end of input, expected header or content.

variableName
	For output variables only, defines the name of the variable.


Full XML Sample
<ee:transform doc:id="747f74d4-cb66-4f8e-8222-24784e6863ae"
  doc:name="Transform" doc:timestamp="1510861248434">
    <ee:message>
        <ee:set-payload><![CDATA[%dw 2.0

output application/json
---
(payload map (value0, index0) -> {
id: value0.id,
username: value0.username,
address: {
street: value0.address.street
},
website: value0.website
})]]></ee:set-payload>
        <ee:set-attributes><![CDATA[%dw 2.0

output application/json
---
{
reasonPhrase: attributes.reasonPhrase
}]]></ee:set-attributes>
    </ee:message>
    <ee:variables>
        <ee:set-variable variableName="myVar"><![CDATA[%dw 2.0

output application/json
---
{
a: payload[0].phone
}]]></ee:set-variable>
    </ee:variables>
</ee:transform>







https://docs.mulesoft.com/mule-runtime/4.3/try-scope-concept
Try Scope

The Try scope enables you to handle errors that may occur when attempting to execute any of the components inside the Try scope. It also supports transactions. A Try scope wraps one or more operations, then catches and handles any exceptions that might be thrown by any of these enclosed operations. The behavior is as if you extracted those enclosed event components into a separate flow with its own error handling strategy, but inline, without having to actually define a new flow.
。。也支持 事务。。。
。try 就像 你 内置了一个flow，


Error Handling with the Try Scope
When designing your flow, try to group those operations that are likely to experience errors inside a Try scope. The Try scope enables you to isolate potentially troublesome operations in your flow and assign them an error-handling method. You can also configure the operations inside the Try scope to be processed as a transaction.
。。设计时，尽量把 可能出错的 操作 集合起来，放到try里。
。。try允许你隔离 flow中 潜在的 讨厌的 操作， 指定它们 一个错误处理方法。
。配置 try里的操作，来让它们能像 事务 一样处理。

The Try scope has an error handling strategy that you configure in the same way you configure error handling for a flow.
。。就像 flow 配置 错误处理 一样 配置 try的错误处理。

The Try scope can distinguish among various error type conditions and apply different behaviors. If an error is raised by a component inside a Try scope, then the Try scope’s error handler is executed. At this point, the error is available for inspection, so the handlers can execute and act accordingly:
。try能 在各种错误中 区分，然后应用不同的错误处理。

On Error Continue
	Executes and sends the result of the execution to its container Try scope, which uses that result to complete the execution successfully. Any transactions at this point are also committed.

On Error Propagate
	Rolls back any transactions, then executes and uses that result to re-throw the existing error, causing its container Try scope’s execution to fail.



If the Try scope has several components, then after a component raises an exception, subsequent components in the Try scope are not executed, regardless of the type of error handler that catches the exception. In the case of On Error Propagate, the error is propagated to the flow’s error handler, as if the Try scope did not exist. In the case of On Error Continue, processing continues outside the Try scope at the next flow component, as if the Try scope never threw an exception.



Handling Transactions
A transaction is a series of actions that should never be partially executed. Configure a Try scope so that it is a set of operations that are considered one unit that either succeeds or fails, depending on whether errors are propagated and the transaction rolled back, or handled and the transaction committed. In either case, the process flow in which the Try scope resides continues.
。。事务，一系列不能被分割的操作。
。配置try，是的 一系列操作被视为 1个单位，要么全部成功，要么全部失败。

The Try scope treats child operations as a transaction when the Transactional Action (transactionalAction) is set to ALWAYS_BEGIN or BEGIN_OR_JOIN. It can be configured in the following ways:

Ignore (INDIFFERENT)
	Default. If a transaction is active, the scope joins it. If not, the scope does not create a transaction.
。。supports。

Always Begin (ALWAYS_BEGIN)
	A new transaction is started every time the scope is executed.
。。requires_new，，，下面说 如果当前有事务，就报错，，这个没有匹配的spring propagation的。

Begin or Join (BEGIN_OR_JOIN)
	Relevant only when execution order might vary (for example, due to asynchronous actions occurring outside the flow). If current flow processing has already started a transaction, the scope joins it. If not, the scope initiates a new transaction.
..required


Analyzing Different Transactional Actions in Try Scope
The following example shows a jms:listener operation configured to initiate a transaction at flow level, a Try scope that tries to initiate or join the transaction (depending on its configuration), and a jms:publish operation configured to run outside of the transaction:

<flow name="someFlow">
	<jms:listener config-ref="JMS_Config" destination="test.in" transactionalAction="ALWAYS_BEGIN"/>
	<!-- Processors -->
	<try transactionalAction="${action}">
		<!-- Processors -->
		<!-- Join if possible is the default value for jms:publish operation -->
		<jms:publish config-ref="JMS_Config" destination="test.out" transactionalAction="NOT_SUPPORTED"/>
		<raise-error type="APP:SOME"/>
		<!-- More Processors -->
	</try>
	<!-- Processors -->
</flow>

。。jms:public 的 transactionalAction 是GUI的哪个？ （jms public, Advanced里的，不是 general）

If the operations within the Try scope do not produce an error, the scope finishes the execution and commits the transaction, independently(独立) of the configured transactionalAction value.

The transaction and the messages are handled differently when a <raise-error/> component is added, depending on the transactionalAction of the Try scope:
Ignore (INDIFFERENT)
	In this case, the transaction continues while executing the operations inside the Try scope. When the error is raised, it is propagated to the source (which does not handle it with an <on-error-continue> error handler). The transaction is rolled back, and the message is available again in the JMS queue. This rollback does not affect the completed jms:publish operation, which was outside of the transaction scope because transactionAction was set to its default, NOT_SUPPORTED.
	Had transactionAction been set to JOIN_IF_POSSIBLE, the jms:publish operation would have been rolled back.
。这里没有错误处理，所以这个错会抛出到flow上，，try的事务回滚，不会影响publish的事务。

Always Begin (ALWAYS_BEGIN)
	This raises an error because an active transaction already exists.
。。。。。。当前有活动的事务，所以报错。。。

Begin or Join (BEGIN_OR_JOIN)
	In this case, a transaction was already initiated so the scope joins the active transaction. The result is the same as in INDIFFERENT.
。当前已经有事务了，所以加入当前的事务，结果和 indifferent 一样。



Error Handler at Flow Level
In the following example, an error handler is added at flow level:

<flow name="someFlow">
	<jms:listener config-ref="JMS_Config" destination="test.in" transactionalAction="ALWAYS_BEGIN"/>
	<!-- Processors -->
	<try transactionalAction="${action}">
		<!-- Processors -->
		<!-- Join if possible is the default value for jms:publish operation -->
		<jms:publish config-ref="JMS_Config" destination="test.out"/>
		<raise-error type="APP:SOME"/>
		<!-- More Processors -->
	</try>
	<!-- Processors -->
	<error-handler>
		<on-error-continue/>
	</error-handler>
</flow>

The behavior in this example is:

Ignore (INDIFFERENT)
	The transaction continues. Because the error is handled by an on-error-continue error handler, the transaction is committed. The message read from the jms:listener source is consumed, and the message processed by the jms:publish operation is actually sent.
zzzzz。事务继续，因为错误被on-error-continue处理了，事务会commit，。。。。。，， 这个error-handler 是 flow的。。。怎么觉得 这里不应该commit，而是rollback啊。。try没有抓到异常啊。。下面的例子是 被try抓到异常。

Always Begin (ALWAYS_BEGIN)
	Raises an error because an active transaction already exists.

Begin or Join (BEGIN_OR_JOIN)
	Displays the same behavior as INDIFFERENT.


Error Handler Inside the Try Scope
In this case, the error handler is inside the Try scope and the error occurs after the execution of the scope:
<flow name="someFlow">
	<jms:listener config-ref="JMS_Config" destination="test.in" transactionalAction="ALWAYS_BEGIN"/>
	<!-- Processors -->
	<try transactionalAction="${action}">
		<!-- Processors -->
		<!-- Join if possible is the default value for jms:publish operation -->
		<jms:publish config-ref="JMS_Config" destination="test.out"/>
		<!-- More Processors -->
		<!-- There could be a component that raises an error, it will be handled by the error handler -->
		<error-handler>
			<on-error-continue/>
		</error-handler>
	</try>
	<!-- Processors -->
	<raise-error type="APP:SOME"/>
</flow>

Depending on the configured transactionalAction, the behavior in the Try scope is one of the following:
Ignore (INDIFFERENT)
	The transaction continues but the error is not handled by an on-error-continue at the flow level, causing the transaction to be rolled back, and the message to not be sent.
。。这个 error-handler 是 try的。。。

Always Begin (ALWAYS_BEGIN)
	Raises an error because an active transaction already exists.

Begin or Join (BEGIN_OR_JOIN)
	Displays the same behavior as INDIFFERENT.


Configuring Local or XA Transactions
In addition to configuring the Transactional Action, you can also configure the Transaction Type to be Local (single Resource) or XA Transaction.
。。XA是由X/Open组织提出的分布式事务的规范，，，不过好像性能不够。

When setting the Transaction Type you must consider that a transaction cannot change it’s type in the middle of its execution. This means that the Try scope only uses the Transaction Type when a new transaction is created.

For each Transactional Action, the behavior changes:
Ignore (INDIFFERENT)
	The Transaction used is the one already created (if there is one). This means that setting the Transactional Type makes no difference for this Transactional Action.

Always Begin (ALWAYS_BEGIN)
	The Transaction created is of the type set in the Transaction Type configuration. Remember that ALWAYS_BEGIN is an invalid configuration when already running within a Single Resource transaction.

Begin or Join (BEGIN_OR_JOIN)
	If there is a Transaction already created, then the Transactional Type makes no difference, as in the case of INDIFFERENT. If there is no Transaction, it creates one of the Type configured in Transaction Type, as in the case of ALWAYS_BEGIN.




https://docs.mulesoft.com/mule-runtime/4.3/try-scope-xml-reference
Try Scope XML Reference

A Try scope follows the structure described below.
	A single root element <try>

	Components that are executed under the error-handling rules defined by the Try scope are defined as child elements of the try element. You can place one or many here.

	A single <error-handler> element holds all error handling strategies for the scope.

	In the error handler, one or several on-error-continue and on-error-propagate define the various strategies. At least one of these must be present.

	Components that are executed when a matching error occurs are defined as child elements of the on-error element. You can place one or many here.

<try>
  <!-- COMPONENTS TO TRY TO USE -->
  <error-handler>
    <on-error-continue>
      <!-- COMPONENTS TO USE IN CASE OF ERROR -->
    </on-error-continue>
    <on-error-propagate>
      <!-- COMPONENTS TO USE IN CASE OF ERROR -->
    </on-error-propagate>
  </error-handler>
</try>

Each error handling strategy in a Try scope (on-error-*) follows a condition. This condition is typically an error type (or a list of several) which must match the current error. You can also define this condition as a freely written expression, such as error.cause.message.contains("fatal").

Below is an example that includes two error handling strategies, each executing a logger component:
<try>
  <http:request config-ref="HTTP-config" method="GET" path="/" />
  <error-handler>
    <on-error-continue enableNotifications="true" logException="true" type="CONNECTIVITY">
      <logger level="INFO" doc:name="Logger" message="Connectivity Error"/>
    </on-error-continue>
    <on-error-propagate enableNotifications="true" logException="true" doc:name="On Error Propagate" type="EXPRESSION">
      <logger level="INFO" doc:name="Logger" message="Expression error" />
    </on-error-propagate>
  </error-handler>
</try>


Properties of Try
transactionalAction
	INDIFFERENT
		Defines if what’s in the scope is treated as a transaction. Possible values:
		INDIFFERENT : What’s in the scope isn’t treated as a transaction.
		ALWAYS_BEGIN: A new transaction is sarted every time the scope is executed.
		BEGIN_OR_JOIN: If the current processing of the flow has already begun a transaction, join it. Otherwise, begin a new one. (Only relevant when execution order may vary).

transactionalType
	LOCAL
		Defines the type of transaction to use. Possible values:
		"LOCAL"
		"XA"



https://docs.mulesoft.com/mule-runtime/4.3/until-successful-scope
Until Successful Scope

The Until Successful scope processes the components within it, in order, until they succeed or exhaust the maximum number of retries. Like all Core components other than Async, Until Successful runs synchronously. If a component within the scope fails to connect or produce a successful result, Until Successful retries the failed task until all configured retries are exhausted. If a retry succeeds, the scope proceeds to the next component. If the final retry does not succeed, Until Successful produces an error.
。。有序 处理内部定义的组件，直到 (全部?)成功 或 尝试次数耗尽。
。。就像其他的core组件(除了async)一样，until successful 线性执行


Common processes that use Until Successful include:
	Dispatching to outbound endpoints, for example, when calling a remote web service that might have availability issues.
。。转发给外部端点
	Executing a component method, for example, when executing on a Spring bean that may depend on unreliable resources.
。。执行一个 组件方法。如 读取 不可靠 的资源。
。。感觉好像没有用啊，没有就是没有，再读N遍也是没有啊，而且 这里是 处理完一个再处理下一个的，直到全部处理完。
。。不是，按个执行，失败就下一个，不retry，直到有一个成功。
	Using a sub-flow to re-execute several actions until they all succeed.
。。使用sub flwo来重新执行 数个动作，直到全部成功。。还是上面的问题，执行失败，你再重试，也是失败啊。

The Until Successful scope provides fields described in this table:
Max Retries (maxRetries)
	Specifies the maximum number of retries that are allowed. This attribute can be either a number or an expression that resolves to a number. An error message looks like this: Message: 'until-successful' retries exhausted. The Mule error type is MULE:RETRY_EXHAUSTED.

Milliseconds Between Retries (millisBetweenRetries)
	Specifies, in milliseconds, the minimum interval between two retries. The actual interval depends on the previous execution, but it should not exceed twice this number. The default value is 60000 milliseconds (one minute). This attribute can be either a number or an expression that resolves to a number.

。。看xml，还有个 maxRetriesBefore430="5" ，这个无法通过GUI修改。
..maxRetries 默认应该是5. 。。因为GUI里 清空，切到XML，再切回去，就是5了。而且 5的话，XML里没有 maxRetries 这个属性。

This XML example allows for up to five retries each second, the default. Note that doc:id attributes are removed to make the example more readable.

<flow name="untilsuccessfulFlow">
  <scheduler doc:name="Scheduler" >
    <scheduling-strategy >
      <fixed-frequency frequency="30" timeUnit="SECONDS"/>
    </scheduling-strategy>
  </scheduler>
  <until-successful maxRetries="5" doc:name="Until Successful">
    <!-- A single processor here -->
    <http:request method="GET" doc:name="Request"
      config-ref="HTTP_Request_configuration"
      url="https://jsonplaceholder.typicode.com/users"/>
  </until-successful>
  <logger level="INFO" doc:name="Logger" />
</flow>


Variable Propagation
Every execution of the Until Successful scope starts with the same variables and values present before the execution of the block. New variables or modifications to already-existing variables while processing one element are not visible in the next execution (in case there is an error). If the execution finishes correctly, the variables (and payload) are propagated to the rest of the flow.
















====================================
====================================
====================================
====================================
====================================
core end
====================================
====================================
====================================
====================================









https://docs.mulesoft.com/mule-runtime/4.3/build-application-from-api
Build an Application from an API Specification

During your app development lifecycle, you can build Mule REST or SOAP APIs from an API specification using APIkit. You develop APIs based on the following modeling languages:
	RAML (RESTful API Modeling Language)
	WSDL (Web Service Description Language)
	OData (OData-Enabled APIs using APIkit OData Extension)
	OAS (Open API Specification)
。。模型语言

By using APIkit in Anypoint Studio, you can simplify the design and implementation of APIs by automating Mule flow creation based on importing either a RAML or a WSDL specification file. After generating the API, you can perform calls to test the API and then finalize its implementation.

Additionally, APIkit helps you build applications using Apache Maven, and also provides a CLI tool to generate flows for an already existing application. However, before you perform any of these tasks, you must learn the basic APIkit anatomy and understand the scaffolding technique that generates Mule flows when you start APIkit projects.
。。先要理解基础的apikit，
。。apikit 是什么？。 设计思维？

APIkit Anatomy
A REST or SOAP Studio project that uses APIkit has the following parts:
	A REST or SOAP interface
	A main flow
	Backend flows
。。一个工程，3部分，一个rest/soap接口，一个主要的flow，多个后台flow


REST Interface
The relationship between REST resources and actions is called resource-action pairing. The interface exposes internal data assets requested by the REST service. Also, the interface defines the API, designating resources that contain or access data and specify actions, such as GET and PUT, on the example data.


SOAP Interface
The interface defined by a WSDL file exposes a number of services. Each service has a set of operations. The underlying type of data is XML defined by schemas within the WSDL file or by an imported XSD file.


Main Flow
The main flow for the API manages these functions:
	Exposes the API using Anypoint Connector for HTTP (HTTP Connector).
	Routes requests between the interface and the backend flows based on the HTTP request.
	Accepts requests, sends responses, and facilitates communication between the interface and API processes.
。暴露api，路由request，接受request，发送response，促进接口和api处理器的交互。

The main flow consists of HTTP Listener and APIkit (REST or SOAP) Router components. The APIkit Router is a key message processor that validates requests against the provided definition, enriches messages (for example by adding default values to the messages) and routes requests to a particular flow. Also, the Router raises errors messages if errors occurs while routing, validating or processing the user request.


Backend Flows
APIkit for REST generates a backend flow for each resource-action pairing in a RAML file.
APIkit for SOAP generates a backend flow for each operation in a WSDL.
In the following example, the RAML interface receives the GET request users. The backend flow accesses a database to look up users by ID.


Perform Calls
For REST APIs, use API Console to perform calls to the API.
For SOAP APIs, use a SOAP testing tool such as SoapUI to perform calls to the API.



Start APIkit Project and Generate Mule Flows
When you start a new APIkit project in Studio, you have the option to whether import or not an API definition file and and when you do so, the APIkit scaffolding technique generates different flows for the API. There are three ways to start an APIkit project in Anypoint Studio:
。。3条路 开启apikit。

1. Start a New APIkit Project Using an API Definition File
Use this method if you want to start a project by either importing an existing RAML or WSDL file or by referencing the file URL:
Select File > New > Mule Project.
In Project Name, enter a name for the project, for example myproject.
Select or accept the default EE Runtime, for example Mule Server 4.2.0 EE.
In API Implementation, select Specify API Definition File Location or URL.
In Location, do one of the following:
	If you created an api.raml file in Design Center, select Design Center . Login to Anypoint Platform if necessary, and select api.raml.
	If you didn’t create a RAML file in Design Center, select Browse Files and select the RAML or WSDL file that you created in a text editor. For a WSDL file, select a service and port from the drop-down menus or accept the defaults.
Accept the Location default options, and click Finish.

。。4.3.0 EE 中，新建的时候API Implementation 是3个标签，分别是：import a published api, import raml from local file, download raml from design center.


Using this method the APIkit scaffolding technique generates:
	An archetype that contains basic configuration files plus a Mule XML configuration file
	An implementation of the API
。。脚手架技术。
。。一个典型，包含基本的配置文件 和 mule xml 配置文件。
。。一个api的实现。

For RAML-based APIs, the RAML file appears under the src/main/resources/api folder. The implementation of the API contains:
	A main flow with an HTTP Listener
	APIkit Router component
	Error handlers
	RAML dependencies (push or pull changes from one location, such as Studio, to the other, such as Design Center)
	Backend flows containing REST resource-action mappings
。。一个监听端口的 主要的flow
。。apikit 的 路由分发 组件
。。错误处理
。。restfule api modeling language，的依赖 (。。。？)
。。后台flow，包含 rest 资源行为 的映射。


For WSDL-based APIs, the WSDL file appears under the src/main/resources/api folder. The implementation of the API contains:
	A SOAP Router
	A SOAP fault response template
	Backend flows containing SOAP message templates
。。一个soap 路由，
。。soap 故障 应答 模板
。。后台flow，包含soap 消息 模板

zzzzz。。真有 apiKit router 这个组件。。
。。这页有很多图 和描述，主要是 工程的 简单内容。(几个flow，会告诉你，这个是main flow，这个是soap fault response，这些是backend flow)
。。根据下面的，，，估计这页的图 是 使用 api 规范后，自动生成的 一些flow。。。，但是 规范的文件 应该是 自定义的吧。，可能这里是 最简单的一个 api规范。


2. Start a New APIkit Project
Use this method if you start an APIkit project without an API specification file and you can later add it into the project. APIkit generates a skeletal RAML file and XML configuration file.
。。开始apikit工程，不使用api 规范。，，你可以后续添加 api规范 到工程。
。apikit 生成一个 raml配置文件 和 xml配置文件 的 骨架。





































































































































































































































































































































































































































































































































































































https://docs.mulesoft.com/mule-runtime/4.3/dataweave
DataWeave Language

https://docs.mulesoft.com/mule-runtime/4.3/dataweave-quickstart
DataWeave Quickstart

You also load content from files so that your DataWeave script can act on it.

DW：
%dw 2.0
var myJson = {"hello" : "world"}
output application/java
---
myJson

preview：
{
  hello: "world" as String {class: "java.lang.String"}
} as Object {encoding: "UTF-8", mediaType: "*/*", mimeType: "*/*", class: "java.util.LinkedHashMap"}

。页面上的preview 是：
{
	"hello": "world"
}
。。我这个 信息太多了。。



Set Up a Project in Studio
Set up a Mule project that serves as a DataWeave playground:
	In Studio, click File → New → Mule Project to create a Mule project.
	Provide the name testscript for the project, and click Finish.
	From the Mule Palette tab of your new project, click Core, and then drag the Transform Message component into the Studio canvas.
	In the Transform Message tab, click Preview (on the far right) to open the Preview pane, and click the empty rectangle next to Preview to expand the source code area.
。。就是新建工程，输入工程名字，拖拉一个Transform Message组件。点击preview，点击source only(就是preview旁边的单个矩形)



Start Scripting
Concatenate Two Strings into a Single String
Transform JSON Input to XML Output
Learn About Supported Data Types
Define and Use a DataWeave Variable as Input
Use a DataWeave Function in a DataWeave Variable
Read, Transform, and Select Content from an Input
Read File Contents with a DataWeave Function
Map Elements from an Array into an Object
Pluck Values from an Object into an Array
Map and Merge Fields
。。上面这些 是本页 的锚点。。
zzzzz


Concatenate Two Strings into a Single String
%dw 2.0
output application/java
---
{mystr:("hi"++"world")}
===============
{
  mystr: "hiworld" as String {class: "java.lang.String"}
} as Object {encoding: "UTF-8", mediaType: "*/*", mimeType: "*/*", class: "java.util.LinkedHashMap"}

。。空格无所谓的。就是++前后的空格，{后。

The body of the DataWeave script contains a key-value pair ({ myString: ("hello" ++ "World") }). The value of this input object is a DataWeave expression (("hello" ++ "World")) for concatenating the strings "hello" and "World" into a single string, "helloWorld".
。。键值对，这个输入对象的值 是一个DW表达式，来连接hello 和 world，变成helloworld。

The header of the script is all the content above the three dashes, ---. It includes important directives, including one for specifying the output format application/json. You can learn more about DataWeave Scripts when you are ready.
。。脚本的头部是 --- 之上的全部，包含 重要指令，包含了 指定输出格式的 application/json.
。。估计% 是注释吧，所以不算 --- 之上。 不是注释。


Transform JSON Input to XML Output
Many integrations require transformations from one format to another. This procedure uses the *output directive to produce XML output from JSON input.
。。许多集成 要求 将一种格式 转换为另一种。这一步骤 使用 output 指令 来 生成xml的输出 从 json的输入中。

Replace the body of the current script in the source code area with JSON output from Concatenate Two Strings into a Single String, and change the output application/json directive to output application/xml:
%dw 2.0
output application/xml
---
{ "myString" : ("helloWorld") }

。。用这段 来代替 上面的 ++ 的例子，
。。。本地不行啊。
。Error while executing dataweave script for a temporary application
org.mule.tooling.agent.rest.client.exceptions.ToolingServiceAPIException: HTTP 500 Internal Server Error. 
zzzzz。。这个还要连外网的？搞笑呢。。
。。restart 就可以了。！！。。

See the XML output in the Preview pane:
<?xml version='1.0' encoding='UTF-8'?>
<myString>helloWorld</myString>

Notice that the "myString" key of the input JSON object { "myString" : ("helloWorld") } is converted to the root element of the XML output and that the concatenated string becomes the value of that XML object. So the XML output is <myString>helloWorld</myString>. That output is preceded by a standard XML declaration that specifies the XML version and encoding.

%dw 2.0
output application/xml
---
{
	aaasd : "asd1"
}
========
<?xml version='1.0' encoding='UTF-8'?>
<aaasd>asd1</aaasd>


%dw 2.0
output application/xml
---
{
	aaasd : ("asd1")
}
===========
<?xml version='1.0' encoding='UTF-8'?>
<aaasd>asd1</aaasd>


%dw 2.0
output application/xml
---
{
	"aaasd" : ("asd1")
}
========
<?xml version='1.0' encoding='UTF-8'?>
<aaasd>asd1</aaasd>

zzzzz。。key 加不加"" 有什么区别，  value必须加""，不加""，会说 不能解析 这个 reference，ok,value不加""，就是一个 变量名。
。。但是key不加""，怎么没有认为是变量名？
。。加() 就是 一个DW表达式。


%dw 2.0
output application/xml
---
{
	(aaaswd) : ("asd1")
}
。。这个报错，aaaswd 不能解析 它的引用。
。。aaaswd 加上"" 后就可以了，，
。。看来 key 必须是一个 编译时的常量。。也不是。

%dw 2.0
output application/xml
var aaaswd = "asd"
---
{
	(aaaswd) : ("asd1")
}
=====
<?xml version='1.0' encoding='UTF-8'?>
<asd>asd1</asd>

zzzzz。这个是可以的。aaaswd肯定是可变的吧？？。。。如果是可变的，那么就不是常量了。。

。。又有问题了。。。
。。最开始的时候，(aaaswd)，有错误修复的， 自动在---之上增加了一行 aaaswd = ??? , 然后可以了，输出是<aaaswd>asd1</aaaswd>
。。现在手动 改成= ???  就不行了。。说是 没有初始化。。。我。。。
。。没有办法重现了，，估计是 先用 aaa:"aca" 来生成一个正确的 preview，然后 (aaa):"aca"，自动修复，增加一个var aaa = ??? 这个不会刷新 preview，导致以为 可以吧。。

"Not implemented yet.

3| var aaaswd = ???
                ^^^
Trace:

    at main (line: 3, column: 14)" evaluating expression: "%dw
   2.0
output application/xml
var aaaswd = ???
---
{
	(aaaswd)
   : ("asd1")
}
".


Learn About Supported Data Types

Now provide a DataWeave script that simply introduces you to a variety of supported data types and shows how to add comments to a script.
。。DW脚本 来简单介绍 各种 支持的数据类型，展现如果增加 注释。。
。。这里的数据类型是指 string，int，， 不是 MIME类型

复制下面的到
%dw 2.0
output application/json
---
{
  /*
   * A multi-line
   * comment here.
   */
  myString: "hello world",
  myNumber: 123,
  myFloatingPointNumber: 123.456,
  myVeryBigNumber: 12341234134123412341234123,
  myDate: |2018-12-07|,
  myTime: |11:55:56|,
  myDateTime: |2018-10-01T23:57:59-03:00|,
  myBoolean: true,
  myArray: [ 1, 2, 3, 5, 8],
  myMixedArray: [ 1, 2, "blah", { hello: "there" } ],
  myObjectKeyValuePair: { innerKey: "innerValue" },
  myObjectWithConditionalField: { a : { b : 1, ( c : 2 ) if true, (d : 4) if false } },
  myNull: null,
  myBinary: "abcd1234123" as Binary
  //A one-line comment here.
}

====本地的 输出：
{
  "myString": "hello world",
  "myNumber": 123,
  "myFloatingPointNumber": 123.456,
  "myVeryBigNumber": 12341234134123412341234123,
  "myDate": "2018-12-07",
  "myTime": "11:55:56",
  "myDateTime": "2018-10-01T23:57:59-03:00",
  "myBoolean": true,
  "myArray": [
    1,
    2,
    3,
    5,
    8
  ],
  "myMixedArray": [
    1,
    2,
    "blah",
    {
      "hello": "there"
    }
  ],
  "myObjectKeyValuePair": {
    "innerKey": "innerValue"
  },
  "myObjectWithConditionalField": {
    "a": {
      "b": 1,
      "c": 2
    }
  },
  "myNull": null,
  "myBinary": "abcd1234123"
}
。。这个就没有 那些 额外的 类型说明了。。。

DataWeave supports multi-line comments within /* */ markup and single-line comments after forward slashes (//). It also supports many data types, shown after the colon (:) in key-value pairs, such as myString: "hello world" and myNumber: 123. These types include strings (surrounded by quotation marks, ""), numbers, date and time structures (within pipes, ||), Booleans (true and false), arrays (within square brackets, []), JSON-like objects (key-value structures within curly braces, {}), null, and binaries. When you are ready for more on this topic, you can review DataWeave types.
。。DW支持/* */多行注释， // 单行注释。
。支持许多数据类型，在:后展现。
这里包含了，string(被""包围)，number，date和time(在| |中间)，Boolean(true 或 false)，array([] 中)，JSON-like objects({}中，键值对)，null, binary

=== 页面的输出。
{
  "myString": "hello world",
  "myNumber": 123,
  "myFloatingPointNumber": 123.456,
  "myVeryBigNumber": 12341234134123412341234123,
  "myDate": "2018-12-07",
  "myTime": "11:55:56",
  "myDateTime": "2018-10-01T23:57:59-03:00",
  "myBoolean": true,
  "myArray": [ 1, 2, 3, 5, 8 ],
  "myMixedArray": [ 1, 2, "blah", { "hello": "there" } ],
  "myObjectKeyValuePair": { "innerKey": "innerValue" },
  "myObjectWithConditionalField": { "a": { "b": 1, "c": 2 } },
  "myNull": null,
  "myBinary": "abcd1234123"
}


Define and Use a DataWeave Variable as Input

Now try a simple DataWeave script that outputs the value of the DataWeave variable myJson. You set the variable using the var directive in the script’s header.
。。DW脚本，输出DW变量myJson的值。 设置变量通过var指令，在脚本的 头部。

Replace the current script in the source code area of the Transform Message tab with this one:

%dw 2.0
var myJson = { "hello" : "world" }
output application/json
---
myJson

A JSON object ({ "hello" : "world" }) is defined as the myJson variable in the script’s header. You can learn more about DataWeave Variables when you are ready.

See the output in the Preview pane:
{
  "hello": "world"
}


。。又开始500了。又得重启下。
Error while executing dataweave script for a temporary application
org.mule.tooling.agent.rest.client.exceptions.ToolingServiceAPIException:
   HTTP 500 Internal Server Error. org.mule.tooling.agent.rest.client.exceptions.model.ErrorEntity@46ab2202[errorType=class
   com.mulesoft.agent.exception.NoSuchApplicationException,errorMessage=No
   application deployed for artifactId: 3acec1f8-d2ee-4b25-bda9-e60b199d1a76,errorDetail=com.mulesoft.agent.exception.NoSuchApplicationException:
   No application deployed for artifactId: 3acec1f8-d2ee-4b25-bda9-e60b199d1a76
	at
   com.mulesoft.agent.services.tooling.MuleAgentToolingService.getDeployedApplication(MuleAgentToolingService.java:438)
	at
   com.mulesoft.agent.external.handlers.dataweave.DataWeaveRequestHandler.lambda$executeOnApplication$0(DataWeaveRequestHandler.java:82)

。。不用重启，，关了这个 组件的 显示，然后 再重新点击这个组件 就可以了。

%dw 2.0
var myJson = { "hello" : "world" }
output application/json
---
myJson
====
{
  "hello": "world"
}
。。没有类型。。


Use a DataWeave Function in a DataWeave Variable
Now try a script that uses the DataWeave *avg function in a DataWeave variable (myJson) to get averages of two sets of numbers.
。。在DW 变量 myJson 中使用 avg方法 来获得 2个集合的数的平均值


%dw 2.0
var myJson = {
  a: avg([1, 1000]),
  b: avg([1, 2, 3])
}
output application/json
---
myJson
===================
{
  "a": 500.5,
  "b": 2.0
}

The avg functions here get invoked when you add myJson to the body of the script, producing the calculated averages within the JSON object you can see in the Preview pane. The structures [1, 1000] and [1, 2, 3] are arrays. You can learn more about avg when you are ready.


Read, Transform, and Select Content from an Input
Now try a more complicated script that reads XML input, transforms it to JSON, and only selects the contents of the car element.
。。读xml，转成json，选择其中的car元素。

%dw 2.0
var myRead = read("<car><color>red</color></car>",
                "application/xml")
output application/json
---
{
  mySelection : myRead.car
}
================
{
  "mySelection": {
    "color": "red"
  }
}

If you encounter an issue previewing this example, try changing myRead.car to myRead."car"
。如果遇到问题，就 car 变成 "car"

。。这个read后面的xml，只能 一个 根元素。并且 如果 根元素下有多个相同元素，无法选择的。。。。现在又不是了，，多个相同元素，选择第一个。而且可以用[]。。。

%dw 2.0
var myRead = read("<aaa><car><color>red</color></car></aaa>",
                "application/xml")
output application/json
---
{
  mySelection : myRead.aaa.car
}
==================
{
  "mySelection": {
    "color": "red"
  }
}


%dw 2.0
var myRead = read("<aaa><car><color>red</color></car><car>a123aa</car></aaa>",
                "application/xml")
output application/json
---
{
  mySelection : myRead.aaa[1]
}
====================
{
  "mySelection": "a123aa"
}

。。从0开始的。


%dw 2.0
var myRead = read("<aaa><car><color>red</color></car><car>a123aa</car></aaa>",
                "application/xml")
output application/json
---
{
  mySelection : myRead[0][0]
}
=============
{
  "mySelection": {
    "color": "red"
  }
}
。。这个有N多的 警告。。
Auto-Coercing type from: `Any` to: `Object`.
Auto-Coercing type from: `0` to: `Name`.
Auto-Coercing type from: `Any` to: `Range`.
。。。一共30个，，其中4个是 0 to，， 其他都是 any to


Read File Contents with a DataWeave Function

Now use *readUrl to read the contents of a file in the Studio src/main/resources folder so you can use that content as sample data for a DataWeave script.
。。使用 readUrl 读取文件的内容。

Add a file by right-clicking the src/main/resources folder in the Package Explorer tab, then navigating to New → File, providing the file name myJson.json for that file, and clicking Finish.
From src/main/resources, provide and save the following sample content in the myJson.json tab (or within the Source sub-tab of the myJson.json tab, if it is present):
{
  "hello": "world"
}
。。右键xxx 文件夹，new-file，输入 名字，，，复制内容到这个文件。

Returning to the Transform Message component within the testscript tab, replace the current script with one that uses readUrl to read the JSON contents from your file:
%dw 2.0
output application/json
---
readUrl("classpath://myJson.json", "application/json")

View the matching output in the Preview pane.
{
  "hello": "world"
}

Note that you can also load the contents of a file through a metadata type in the Transform Message component. That procedure is covered later, in Run Examples with Longer Payloads. It uses the myJson.json file you just created.

。。这个IDE 没有 F2，修改名字，。。


Map Elements from an Array into an Object

Almost all integrations require data mappings. Here, you map elements within an array to keys and values of a JSON object:
。。所有的集成都要求 数据映射。 在这里，你映射 数组里的元素 到 JSon 对象

%dw 2.0
output application/json
---
{
  (
    ["a", "b", "c"] map ((value, index) -> {
        (index): value
    })
  )
}
==========
{
  "0": "a",
  "1": "b",
  "2": "c"
}

The *map function iterates over the array on the left to apply the lambda (anonymous function) on the right (((value, index) → { (index): value })) to elements in that array. The lambda uses named parameters (value and index) to select the values and indices from the array and populate a JSON object with key-value pairs. Learn about map, and when you are ready, compare map with *mapObject, which takes an object as input.
。。map方法，遍历数组，从左到右，应用一个lambda。
。lambda 使用 命名/具名的参数名(value，index)，来选择 值 和 下标 从 数组中，  添加一个json对象。
。。mapObject也是一个方法。。。很强的方法。。。



Pluck Values from an Object into an Array
Now use the DataWeave *pluck function to iterate over values in a JSON object and output those values into an array.
。。使用pluck 方法来 遍历json的值，然后输出到 数组。

%dw 2.0
output application/json
---
{
  "0": "a",
  "1": "b",
  "2": "c"
} pluck ((value) -> value)
=======
[
  "a",
  "b",
  "c"
]

。。 0,1,2 是没用的， pluck 中 只有 value。

Notice that the input object matches the output of the map example and that the output array matches the input from the map example. Learn more about pluck when you are ready.
。。pluck  和 map 正好 相反。


Map and Merge Fields
Now try a more complex example that maps and merges fields from items in separate arrays. The point here is simply to provide a taste of DataWeave’s ability to handle more complex mappings and transformations needed for some integrations.
。。映射和合并 域 从 项目 在 不同(不是独立。。) 的数组中。
。这里的重点是简单地介绍DataWeave处理某些集成所需的更复杂映射和转换的能力。

%dw 2.0
var myVar = [
  { bookId: 101,
    title: "world history",
    price: "19.99"
  },
  {
    bookId: 202,
    title: 'the great outdoors',
    price: "15.99"
  }
]
var myVar2 = [
  {
    bookId: 101,
    author: "john doe"
  },
  {
    bookId: 202,
    author: "jane doe"
  }
]
output application/json
---
myVar map (item, index) -> using (id = item.bookId) {
	"id" : id,
	"topic" : item.title,
	"cost" : item.price as Number,
	(myVar2 filter ($.*bookId contains id) map (item) -> {
		author : item.author
	})
}
。。映射myVar这个数组分解成 item，index，
zzzzz。。map()括号里的随意取名的？ 上上面 是 map(value, index).. 是的，随意。.
。。using 别名？
。。as Number 做一个转换。
。。最后是 遍历 myVar2，如果 myVar2中某个元素的 bookId 包含 id，那么就 增加一个 键值对 author : item.author
。。是增加，吧bookId改了，就没有 author 键值对了。
。。还有 contains id ，这个 id是 using 里的？是的。 可以直接 item.bookId 。
。。而且，如果"id3":,, 然后 contains id3，，是报错了。 id3 不能解析。
===========================
[
  {
    "id": 101,
    "topic": "world history",
    "cost": 19.99,
    "author": "john doe"
  },
  {
    "id": 202,
    "topic": "the great outdoors",
    "cost": 15.99,
    "author": "jane doe"
  }
]

When you are ready to explore the language further, you can learn how the *filter function used near the end returns author values from the array in the myVar2 variable. You can read about -type coercion with DataWeave- to see how as works with the data type in the line "cost" : item.price as Number, to coerce input strings like "19.99" into numbers like 19.99. You can see how using (shown in using (id = item.bookId)) enables you to create local DataWeave variables in a script.
。当你准备进一步探索语言时，你能学到 filter方法 如何起效，返回author 从 myVar2 变量的数组中。
。。你能读 type coercion with DataWeave 来知道 如何处理 cost 这行的 数据类型： item.price做为number，来 强转 "19.99" 到19.99.
。。你可以看到 如何在 脚本中使用 本地DW变量(如 using (id=item.bookId))
..using 估计是 别名。




Try Out More DataWeave Examples
Now you are ready to run DataWeave examples on your own. In your DataWeave playground, you can run examples from the docs whenever you want to discover more about the DataWeave language. You can play with the examples and use them to start your own scripts.

Find many more examples to try out in Studio here:
DataWeave Operators
DataWeave Reference: Docs on the DataWeave function modules provide many examples that use DataWeave functions.
Flow Control in DataWeave
Pattern Matching in DataWeave
DataWeave Cookbook
Define DataWeave Functions
。。都是链接，是很后面才会碰到的其他页面。


Run Examples that Act on a Payload
In DataWeave, *payload is a built-in Mule Runtime variable that holds the contents of a Mule message. It enables you to retrieve the body of a message simply by typing payload into the DataWeave script. The docs often refer to this content as "the payload."
。。payload 是内置的 mule 运行时变量，保存了Mule message。
。。它可以让你 找回 message的主体 通过 把payload 定型/输入 到 DW 脚本。

To try out DataWeave examples that use payload to get the contents of a Mule message, you have some options:
。。为了测验那么 使用payload来获得mule message的内容的 DW例子，下面是一些选项
	Run Examples with Short Payloads: You can use a DataWeave variable.
	Run Examples with Longer Payloads: Add the payload content through a file. This technique also works for short payloads.
。。2个url。是本页的，就在下面。
Alternatives that require a running Mule app (such as using Set Payload) are not covered here but are introduced in Next Steps.
。。替代方案 需要 一个 运行的mule应用 没有被这里覆盖(应该是指 这里没有提到)，但 在Next steps(url)中被介绍了。


Run Examples with Short Payloads
For short examples with a few lines of sample data, you can convert the input payload to a variable. You simply copy the payload’s content into a DataWeave variable (var) in the header of a DataWeave script. Then, in the body of the script, you replace payload with the name of the new variable to reference that variable.
。。为了这些有样本数据的简短例子，你能转换输入的payload 到一个变量。。。。不是把整个payload变成一个变量，而是选取payload的一个变量。
。。你仅仅复制payload的内容 到 DW 变量，在DW脚本的头部。
。。在DW脚本的 主体，你替换payload，用 指向那个变量的 新变量的名字。

%dw 2.0
output application/json
---
ContainsRequestedItem: payload.root.*order.*items contains "3"

Notice that you cannot preview it yet:

Now modify this example so that it uses some sample input instead of attempting to use payload to input content of a Mule message that does not exist:

Add a myInput DataWeave variable supplying some input data the script can read, and change the Mule payload variable in the body of the script to the DataWeave myInput variable.

%dw 2.0
var myInput = read("<root>
    <order>
      <items>1</items>
      <items>3</items>
    </order>
    <order>
      <items>2</items>
    </order>
</root>",
"application/xml")
output application/json
---
ContainsRequestedItem: myInput.root.*order.*items contains "3"
============================
{
  "ContainsRequestedItem": true
}

。。。前面带*，应该是指 任意的标签。
。。不带的话，是第一个。


Run Examples with Longer Payloads
For DataWeave examples with many lines of sample data, consider creating a metadata type through Transform Message. Metadata types can accept a local file that contains sample data.
。。之前是payload短，可以直接写在DW code中。
。。现在是payload 长，无法直接写到DW code中。

。。创建一个metadata 类型 通过 transform message， metadata 类型能接受一个 包含样例数据的 本地文件。

Use the readUrl example to create src/main/resources/myJson.json in Studio.
You can skip this step if you still have the file in Studio. The next steps show how to use the contents of this file as your payload.
。。使用 readUrl 例子来创建 文件。 如果有了，就可以跳过。

Now replace the current script from the source code area of the Transform Message with one that selects the payload of the input:
。替换当前脚本

%dw 2.0
output application/json
---
payload

不会有preview。。。。恩，因为payload是空的。。

Now provide a simple JSON object as the payload of the Transform Message:
In the Transform Message tab, click the left-most rectangle from the Preview button to open the columned, graphical view next to the source code area:
。。提供一个json做为payload。
。。在transform message ，选择 preview 左侧3个 长方形中 做左侧的矩形， 打开xxxView。

In the left-most column of the Transform Message tab, find Payload: Unknown, and click Define metadata.
If you do not see the Define metadata link, simply right-click the Payload: entry in the left-most column, and then click Set Metadata to open the Select metadata type dialog.
。。在最左侧的列中，找到 payload:Unknown. 点击 define metadata。
。。如果你没有看到 define metadata, 就右键 payload: (最左栏没有意义啊，直接右键payload就有setmedatada了。。)，选择 set metadata

Now load the contents of myJson.json (created in the readUrl example) into the Transform Message component:
Click +Add to open the Create new type dialog.
In the dialog, provide the Type id myJsonType, and click Create type.
。。开始加载 文件 到 transform message 组件
。。1. 点击+add号，2，输入myJsonType。点击创建。
Back in the Select metadata type dialog that opens, select JSON from the Type drop-down menu.
。。3 type(上面那个，默认是 simple type) 下拉框里选择 JSON

Below the new type, change Schema to Example (as shown above).
。。把 schema 换成 example

Use the navigation button with the ellipsis (…​) to find src/main/resources/myJson.json, and click Open, which displays the structure of the file contents (hello: String) in the Select metadata type window.
。。...按钮 选择 到 myJson.json.,,会展现 文件的内容。

Now click Select to load the contents of the file into the message payload.
。。最下面的select。，把这个内容导入到 payload

Notice that the JSON object from myJson.json is now in the Preview pane.
If necessary, you can click Preview to open the Preview pane.
。。现在preview 里已经有:
{
  "hello": "world"
}
。。。。。这个有时不显示，得 换个组件，然后再点击 transform message，

Now click the empty rectangle next to the Preview button to open the source code area, and change the body of the script to payload.hello, retaining the existing DataWeave header content.
Notice that the Preview pane now contains only the value of the payload: "world".
。。现在可以DW code里 写 payload.hello。  preview 里展现 "world"
。。。恩，不带{}的 "world"

%dw 2.0
output application/json
---
payload.hello
=========================
"world"



Next Steps
。。这里是一些介绍，一些url，来进一步使用。

To try out sample data in a running Mule app, without relying on external data sources, you can use these Core components with or without Transform Message:
	Set Payload to provide content for the payload of a Mule event.
	Set Variable to create content in a Mule event variable.
	Scheduler to trigger the regular generation of Mule events.
	Logger to view output and issues logged in the Studio console.
。。一个flow， Schedulaer->Set Payload->Logger

The fx value of Set Payload is set to output application/json --- { hello : "world"}.
The fx value of the Logger is set to payload, which looks like #[payload] in the UI and the XML configuration file for the project.
The Console tab of the running Mule app (testscript) displays the payload from Set Payload.

。。日志会输出 "hello":"world"



https://docs.mulesoft.com/mule-runtime/4.3/dataweave-language-guide
DataWeave Language













































































































































































































